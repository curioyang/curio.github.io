[{"path":"/undefined/tech/Mac制作U盘启动镜像/","content":"1. 插入U盘执行diskutil list，可以看到当前U盘信息 12345678 &gt; diskutil list /dev/disk4 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *31.9 GB disk4 1: Linux 512 B disk4s3 2: Windows_FAT_32 K230_APP 268.4 MB disk4s4 (free space) 31.5 GB - 2. 取消挂载1diskutil unmountDisk /dev/disk4 3. 使用dd进行烧录12345&gt; sudo dd if=../ubuntu-24.04.2-desktop-amd64.iso of=/dev/rdisk4 bs=1m6049+1 records in6049+1 records out6343219200 bytes transferred in 302.613571 secs (20961450 bytes/sec) bs&#x3D;n 代表同时设置输入输出的块大小，n 代表字节数，默认为 512，可以使用 b&#x2F;k&#x2F;m&#x2F;g 等字母后缀代表不同的单位 可以看到of 使用的是&#x2F;dev&#x2F;rdisk4 而不是&#x2F;dev&#x2F;disk4，区别如下: 特性 /dev/disk /dev/rdisk 访问方式 缓冲访问 原始访问，绕过文件系统缓存 性能 较低 较高 适用场景 日常使用，如文件系统操作 低级操作，如磁盘映像写入 数据处理 数据经过文件系统缓存机制 数据直接与物理磁盘交互 使用频率 更常用于挂载文件系统 在需要直接操作磁盘时使用 4. ejected U盘12&gt; diskutil eject /dev/disk4 Disk /dev/disk4 ejected 用去吧你就，非常简单"},{"title":"音频相关的处理函数","path":"/undefined/tech/audio function/","content":"hann_windows12345678910111213141516171819std::vector&lt;double&gt; hann_windows(int window_length = N_FFT,bool periodic = true) &#123; int N = window_length; if (periodic) &#123; N = window_length + 1; // 如果是周期性窗口，窗口长度加1 &#125; auto M = N - 1; auto double_PI = 2 * M_PI; std::vector&lt;double&gt; window(window_length); for (int n = 0; n &lt; window_length; ++n) &#123; double normalized_n = static_cast&lt;double&gt;(n) / M; window[n] = 0.5 - 0.5 * cos( double_PI * normalized_n); // 或者使用等效的公式： // window[n] = pow(sin(M_PI * normalized_n), 2); &#125; return window; &#125; $$X[\\omega, m] &#x3D; \\sum_{k &#x3D; 0}^{\\text{win_length-1}}% \\text{window}[k]\\ \\text{input}[m \\times \\text{hop_length} + k]\\ % \\exp\\left(- j \\frac{2 \\pi \\cdot \\omega k}{\\text{n_fft}}\\right)$$"},{"path":"/undefined/tech/LLM/TODO/","content":"block quantize [ ]"},{"path":"/undefined/paper/VTensor/","content":"VTensor: Using Virtual Tensors to Build a Layout-Oblivious AI Programming Framework AbstractTensor是开发AI算法中一个常见的编程接口。layout涉及了tensor数据在内存中的顺序并且会通过影响数据局部性而影响性能，因此AI库中layout是有默认的使用规范。由于AI应用中可以使用任意的layout，现有的AI系统并不会提供编程抽象来保护AI库约定的layout，开发人员需要编写很多于layout相关的代码，这减低了开发新的库或者开发新算子的效率。此外，开发者要指定layout转换的操作给内部算子来解决不确定的输入layout，从而失去了layout优化的机会。基于多态性的思想，我们提出一个与layout无关的虚拟tensor编程接口，称为VTensor框架，这允许开发人员在写一个新算子的时候不需要考虑tensor的物理layout。另外，VTensor框架在运行时会执行全局layout推断，以一种简单、清晰的方式解决虚拟tensor中对layout的需求，并且执行运行时layout无关优化来全局减少layout转换操作的数量。实验证明，开发者可以避免写layout相关的代码。和TF相比可以平均减少写一个新算子时47.82%的代码行数，平均提升18.65%的性能。 2. 动机和背景2.1中介绍基于数据流图的layout-aware编程模型之后通过示例来介绍layout-aware编程模型中的挑战2.2专注于可维护性2.3专注于layout优化 2.1 TF的设计Tensor贯穿TF模型，并且与layout强相关(这一点做过AI部署工作的同学应该非常熟悉，万恶的NHWC，以及ONNX的NCHW等等)。VTensor中将tensor的语义从计算图和库中抽离，将他们重新表示为虚拟tensor和物理tensor。 计算图Tensor 库tensor ↓ ↓ 物理tensor 虚拟tensor 更直白地讲，常量视为物理Tensor，数据流视为虚拟Tensor 2.2 TF等维护性差(VTensor优势1)在实现算子库的时候，通常需要将所有输入layout的情况都考虑到，否则一旦用户的layout比较独特，那么这个算子执行大概率会失败。使用Vtensor时,只需要专注于算子的计算逻辑即可,访问tensor的信息不需要知道物理layout。很多逻辑无关的代码都是自动生成的，当需要分配物理tensor时，直接调用相应的接口，就可以通过VTensor来声明。 2.3 未经优化的layout转换将layout的转换从算子中提出，创建单独的layout转换节点 3. VTensor框架预览提供四个编程接口来定义算子、描述lib、说明如何内部调用库 VTensor API：可以通过编写Compute函数访问VTensor来实现算子的计算逻辑，Dispatch API作为lib和设备的调用入口。 PTensor API：开发者可以声明和VTensor相对应的PTensor。PTensor通常是在库调用函数中使用，其中包含有具体的layout，可以为确定状态NCHW，也可以为非确定状态ANY，同时提供require函数来指定layout。 library描述文件：促进多library支持。每个library都需要一个这样的文件来描述PTensor的物理layout和library之间layout名称的映射。同时在描述文件中会指定layout转换程序，并从多个备选方案中选择layout的指导方针(人话就是指定library名称来选择对应的转换pass) &lt;硬件不同时可以根据这个描述文件来自动做layout的转换&gt; 框架API：向VTensor框架中注册处理程序。运行时layout优化期间会调用这些处理程序。每个libraryInvoker函数都必须被注册为调用的入口函数，这个函数中会包含如何从抽象参数列表中创建实际的参数。同时会插入一个tag，用于标记eval和infer时执行不同长度的操作，eval只会执行tag之前，infer全部同时提出DLR(dynamic layout resolver)在运行时动态优化数据布局。通过分析访问模式，该框架可以自动调整内存布局以提高性能。"},{"path":"/undefined/paper/oneflow/","content":"introduction 介绍传统框架的缺点，没有考虑并行，引出后面要讨论并行的问题 深度学习模型变得越来越复杂，越来越大。现存的深度学习框架面对训练大型的深度学习模型也出现了一些挑战，早期的设计没有预料到新出现的需求，比如大模型的PP。 介绍并行，以及其他工作的做法，说明他们的缺点，引出要解决的问题。 取决于神经网络结构和硬件配置，各种并行策略找到了他们的最佳途径。数据并行尤其适合少量参数的深度学习模型（少于1千万参数），一旦梯度&#x2F;参数反向传播和通信相互掩盖，就可以实现近似线性加速。模型并行和pipeline并行适用于参数明显更多的模型，可能在单设备上无法容纳，或者数据并行的通信代价太高。stanza和DLPlacer训练卷积层采用数据并行，其他层使用模型并行。OptCNN在多个相同设备上通过沿着batch和channel维度切分算子来并行模型训练。TOFU利用partition-n-reduce方法来切分一个单独的算子为子算子，并将子算子部署在多个GPU上，FlexFlow搜索 SOAP空间来利用算子内外的并行性。一个分布式DL框架应该可以为所有选择的并行策略自动的生成物理执行计划，最小化用户编程影响。更高级别的要求是框架可以自动为所有的NN结构和硬件配置组合找到最好的并行策略。然而，现存的DLC框架甚至无法达到第一个目标，例如灵活地支持多种并行策略。这是我们这篇文章要解决的主要问题，重新设计一个新颖的分布式训练框架。 阐述其他工作都是基于已有的框架进行二次开发，存在局限性 一些新兴的开源项目开发专用的系统或者自定义的库来更好地支持模型或者PP。例如HugeCTR为最流行的模型支持模型并行。Megatron和DeepSpeed支持大型的NLP预训练模型并行。InsightFace训练大尺寸人脸识别模型时使用模型并行。然而这些系统都是为了特定应用而设计的，并不能通用。Wrappers or plugins在一些主流的DL框架中被使用为了更好地支持更多复杂的并行策略。Mesh——TF在TF的基础上提供一些API给开发者去表示广泛的并行计算pattern。GPip在TF和TORCH上训练大型DNN使用跨分布式设备的pipeline来解决单设备上内存容量有限的问题。FairScale集成了Megatron-LM and DeepSpeed的技术在pytorch上进行模型并行或者流水线并行。这些框架都是最初没有预见这些复杂的并行性需求，做这些增量的改进通常会产生不可忽略的系统开销，并且需要用户进行大量的工程性优化。 介绍论文主要内容 如果能够提前与预知大型AI模型的快速发展，那么分布式编译器的通用设计和有效实现将是什么？系统可以更简单么？在这篇文章中，我们探索了这种可能性，并提出OneFlow，一个从0开始构建的新颖的DNN训练框架。OneFLow包含从编译器到基于actor模型的运行时。采用SBP抽象，允许各种数据并行和模型并行的混合，比现在的框架更加简单的方式。actor模型提供了一种简洁的运行时机制来管理复杂的依赖关系(资源受限，数据移动，分布式训练中的计算)通过大量的实验证明了oneflow在训练各种大型的DNN模型方面都是通用且高效的，相比于目前最好的系统。结果表明，相比于在现有框架上进行二次开发的库，oneflow这种简单且通用的实现可以与之持平或者更好一点。 background and motivation 主要介绍技术基础 DNN通常被表示为逻辑计算图，然后被手工编程或者编译器自动转换为一个物理计算图，在runtime阶段由一些优化kernels组成。分布式训练引入了一些必须的通信算子来进行设备之前的数据交换。设备间的带宽仍然比设备内的小1-2数量级。然而一个分布式DLC框架应该将数据移动和计算同等重视。 Distributing the Workload in Spatial DomainSpatial Scheduling 详细说明了如何将ops跨多设备展开，图1介绍了在一个混合并行的例子中一个一个手工放置通信算子是一个工作量很大的事情，当一个新的模型出现时这种复杂的并行性策略制定又是很麻烦的 Distributing the Workload in Temporal DomainTemporal Scheduling ，DL中的数据流时间策略涉及到安排算子的执行，尤其是为了最大化硬件利用率和系统吞吐。最好的性能提升的机会通常出现在将计算和通信相互掩盖的情况下。 Managing the Complex DependenciesDependencies caused by resource sharingDependencies caused by data movementsummary介绍oneflow 编译期和runtime的内容"},{"title":"VPS 设置","path":"/undefined/vps/","content":"搭建1. 购买VPS服务https://my.racknerd.com/ ，主打便宜，能用。 2. 登录服务器 ssh 网站提供的vnc 3. 开始安装基础服务 Ubuntu&#x2F;Debian 123apt update &amp;&amp; apt upgrade -yapt install -y curl wget vim ufwapt install -y shadowsocks-libev CentOS&#x2F;AlmaLinux 1234yum update -yyum install -y curl wget vim firewalldyum install -y epel-releaseyum install -y shadowsocks-libev 4. 配置文件vim /etc/shadowsocks-libev/config.json 12345678&#123; &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: 8388, &quot;local_port&quot;: 1080, &quot;password&quot;: &quot;your_password&quot;, //修改密码即可 &quot;timeout&quot;: 300, &quot;method&quot;: &quot;aes-256-gcm&quot;&#125; 5. 启动服务12systemctl start shadowsocks-libevsystemctl enable shadowsocks-libev 6. 配置防火墙 Debian&#x2F;Ubuntu：使用UFW开放端口：12ufw allow 8388ufw enable CentOS&#x2F;AlmaLinux：使用Firewalld：123firewall-cmd --permanent --add-port=8388/tcpfirewall-cmd --permanent --add-port=8388/udpfirewall-cmd --reload 访问加速1 开启TCP Fast OpenTCP Fast Open (TFO) 是一种 TCP 优化技术,可以显著提高 Shadowsocks 的连接速度和传输效率。在服务端和客户端都开启 TFO 后,可以减少握手次数,降低延迟。 CentOS&#x2F;RHELecho “net.ipv4.tcp_fastopen = 3” &gt;&gt; /etc/sysctl.conf sysctl -p Ubuntu&#x2F;Debianecho “net.ipv4.tcp_fastopen = 3” &gt;&gt; /etc/sysctl.d/10-tcp-fastopen.conf sysctl -p /etc/sysctl.d/10-tcp-fastopen.conf 2 使用BBR加速BBR (Bottleneck Bandwidth and Round-trip propagation time) 是 Google 开发的一种 TCP 拥塞控制算法,可以显著提高网络吞吐量和减少延迟。在 Shadowsocks 服务器上启用 BBR 可以大幅提升网络性能。 CentOS&#x2F;RHELecho “net.core.default_qdisc=fq” &gt;&gt; /etc/sysctl.conf echo “net.ipv4.tcp_congestion_control=bbr” &gt;&gt; /etc/sysctl.conf sysctl -p Ubuntu&#x2F;Debianecho “net.core.default_qdisc=fq” &gt;&gt; /etc/sysctl.d/bbr.conf echo “net.ipv4.tcp_congestion_control=bbr” &gt;&gt; /etc/sysctl.d/bbr.conf sysctl -p /etc/sysctl.d/bbr.conf 服务器开机自启动设置开机自动启动要设置Shadowsocks-libev在系统启动时自动启动，您可以按照以下步骤进行操作： 创建一个Systemd服务单元配置文件： 1sudo nano /etc/systemd/system/shadowsocks-libev.service 在打开的文件中，输入以下内容： 12345678910111213[Unit]Description=Shadowsocks-libev ServerAfter=network.target[Service]ExecStart=/usr/bin/ss-server -c /etc/shadowsocks-libev/config.jsonRestart=on-failureUser=nobodyCapabilityBoundingSet=CAP_NET_BIND_SERVICEAmbientCapabilities=CAP_NET_BIND_SERVICE[Install]WantedBy=multi-user.target 这里假设Shadowsocks-libev的可执行文件路径是 /usr/bin/ss-server，配置文件路径是 /etc/shadowsocks-libev/config.json。如果实际路径不同，请相应地修改上述命令。 保存并关闭文件 (Ctrl + X，然后输入 Y 并按下 Enter)。 启用并启动Shadowsocks-libev服务： 12sudo systemctl enable shadowsocks-libevsudo systemctl start shadowsocks-libev 现在，Shadowsocks-libev将在系统启动时自动启动，并以指定的配置运行。您可以使用以下命令验证服务是否正在运行： 1sudo systemctl status shadowsocks-libev 该命令将显示服务的状态信息，包括是否处于活动状态。 参考连接 https://liujie.io/archives/shi-yong-shadowsockets-libev-jin-xing-mo-fa-fang-wen"},{"title":"「LLM」MOE编译器优化","path":"/undefined/tech/LLM/03_MOE/","content":"HF的Qwen3实现 Qwen3的MOE默认版本 123456789101112131415161718expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)# Loop over all available experts in the model and perform the computation on each expertfor expert_idx in range(self.num_experts):\texpert_layer = self.experts[expert_idx]\tidx, top_x = torch.where(expert_mask[expert_idx])\t# Index the correct hidden states and compute the expert hidden state for\t# the current expert. We need to make sure to multiply the output hidden\t# states by `routing_weights` on the corresponding tokens (top-1 and top-2)\tcurrent_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\tcurrent_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\t# However `index_add_` only support torch tensors for indexing so we&#x27;ll use\t# the `top_x` tensor here.\tfinal_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)return final_hidden_states, router_logits 这个版本中会让所有的专家都进行计算，但是expert_mask中可能当前专家并没有被分配token，所以torch.where这里会返回空的两个tensor，后续计算中都是空的tensor在进行，虽然没有实际的有效计算，但是仍然会调用expertLayer，存在大量开销(为什么是大量开销？topk不会和专家数一样大么？MOE的核心概念就是稀疏激活专家！所以不存在topK和num_expert 接近的情况) Qwen3的MOE优化实现版本 1234567891011121314151617181920expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)# Loop over all available experts in the model and perform the computation on each expertexpert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()for expert_idx in expert_hitted:\texpert_layer = self.experts[expert_idx]\tidx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\t# Index the correct hidden states and compute the expert hidden state for\t# the current expert. We need to make sure to multiply the output hidden\t# states by `routing_weights` on the corresponding tokens (top-1 and top-2)\tcurrent_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\tcurrent_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\t# However `index_add_` only support torch tensors for indexing so we&#x27;ll use\t# the `top_x` tensor here.\tfinal_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)return final_hidden_states, router_logits 优化版本中充分利用了MoE的稀疏特性，只调用活跃的专家来参与后续流程的计算，大大减少了空调用Layer的情况。 但是，这种通过Python高级索引来取数进行计算的方式，在实际部署的时候存在一个很大的问题。编译器无法优化！或者说很难优化！ 这里就需要一种balance，计算量和非连续访存之间的平衡。目前还没有什么特别的想法 MOE的具体实现与MLP的区别总体来说MOE中是包含了大量的专家的，而每个专家进行计算都是使用MLP来完成，所以可以说MOE是对MLP的封装(当然，部分模型中是既有MOE部分也有MLP部分)。为什么会进行这样的封装呢？ MLP可以理解为一个专家对所有的tokens进行计算，得到最终的结果，相对来说模型的泛化性相比于MOE来说较低(可以理解为MOE每个专家负责不同领域的内容，而MLP是一个专家精通各个领域，当然这个行为并不可解释，主观上的臆想)。MOE相较于MLP的一个突出的特点就在这里，与此而来的一个特点就是专家的稀疏性，每次进行decode都只有部分专家被激活来进行计算。 MOE中相比于MLP就会多一个权重来进行专家任务划分gate ，整体的计算流程如下： 通过gate来计算一个路由权重 router_weights，后续通过对router_weights进行概率化得到哪些专家需要对哪些token进行计算。 将专家与他需要处理的token进行计算，然后根据专家对每个Token最终结果的权重(也就是这个专家对这个token的话语权)再次计算，得到这个专家对于这些token的结果。 汇总所有专家对他负责的Token的结果，得到MOE的最终结果。"},{"title":"健身记录","path":"/undefined/FIT/","content":"讲在前面健身不是为了把肌肉线条锻炼的多么的好看，帅气，而是主打一个为了身体健康。今年已经30岁了，逐渐感觉身体机能在下降，并不喜欢这种感觉。同时我也不喜欢大块的肌肉，更新换灵活的肌肉，不至于背上痒痒抓不着，那也太难受了。 目前遇到的缺陷 肱二头肌无力5公斤的哑铃做弯举都很吃力，或者说耐力很差，第三组的时候动作变形，会依靠肩膀的力量来带动小臂，目前还不知道具体原因，待后续补充。 腹部（以前仰卧起坐100+的 T_T）举腿时明显感觉手抓不住器械，同时身体大幅度会晃动（这个应该是身体核心力量变差了）。 好处 每天早起，6点，去小区对面健身房练，一周的时候明显能够感觉到脸瘦了一圈，不是以前那种圆嘟嘟的感觉了，开心！ 目前没有针对单独的身体部位开始锻炼，每次基本都会练到全身，称之为“一分化”，但是明显感觉上半身精神了不少，开心！"},{"title":"「LLM」HuggingFace仓库下载及指定缓存路径","path":"/undefined/tech/LLM/02_HuggingFace_download/","content":"首先hf上是存在对国家歧视的，部分需要申请访问资格的repo会自动拒绝国内的账户（说的就是你LLaMA），这种情况有两种解决办法，一是魔塔上下载模型，另一种是从hf上其他用户手里下载（但是这种情况可能需要修改你下载好的仓库中的json文件才能用transformers的api使用）。同时国内下载模型的速度感人，可以切换国内的镜像来下载。下载前最好指定cache路径，否则你想找模型很难得啦，当然下载好的模型路径其实是有点奇怪的，最好的办法是通过transformer加载，然后保存一份，这样的目录下是很干净的模型。Let’s see see! 国内镜像1export HF_ENDPOINT=https://hf-mirror.com Cache位置1export HF_HOME=/path/to/your/local_dir python代码下载完整仓库12345678910111213import transformersfrom transformers import AutoModelForCausalLM, AutoTokenizerfrom huggingface_hub import snapshot_downloaddef download_from_huggingface(model_name):\tprint(f&quot; Downloading \\033[32m\\033[1m &#123;model_name&#125; \\033[0m from huggingface ... &quot;) model_path = snapshot_download(repo_id=model_name) # revision=transformers.__version__ 可以指定下载版本， 但是transformers版本并不能作为仓库版本来用 print(f&quot;\\033[32m\\033[1m &#123;model_name&#125; \\033[0m has been downloaded into \\033[34m\\033[5m &#123;model_path&#125; \\033[0m&quot;)return model_path 之后你会在Cache目录下看到一些hf相关的文件，而模型会在上面打印的路径下。当然，打开后你会发现这里的文件全是软链接，并且不见得是你当前transformers对应的版本的内容。而总所周知，transformers不同版本中的同一个模型的实现是会变的，例如GLM4(在4.49版本下是继承llama的搭框架以及Phi的MLP func， 但是4.46中是其他版本，当然目前我也没法确定他们最终的推理效果是不是一致)。 12345678910111213141516171819202122232425models--Qwen--Qwen2.5-0.5B-Instruct/├── blobs│ ├── 07bfe0640cb5a0037f9322287fbfc682806cf672│ ├── 0dbb161213629a23f0fc00ef286e6b1e366d180f│ ├── 20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0│ ├── 443909a61d429dff23010e5bddd28ff530edda00│ ├── 4783fe10ac3adce15ac8f358ef5462739852c569│ ├── 4b8373851d093eb9f3017443f27781c6971eff24│ ├── 6634c8cc3133b3848ec74b9f275acaaa1ea618ab│ ├── a6344aac8c09253b3b630fb776ae94478aa0275b│ ├── dfc11073787daf1b0f9c0f1499487ab5f4c93738│ └── fdf756fa7fcbe7404d5c60e26bff1a0c8b8aa1f72ced49e7dd0210fe288fb7fe├── refs│ └── main└── snapshots └── 7ae557604adf67be50417f59c2c2f167def9a775 ├── config.json -&gt; ../../blobs/0dbb161213629a23f0fc00ef286e6b1e366d180f ├── generation_config.json -&gt; ../../blobs/dfc11073787daf1b0f9c0f1499487ab5f4c93738 ├── LICENSE -&gt; ../../blobs/6634c8cc3133b3848ec74b9f275acaaa1ea618ab ├── merges.txt -&gt; ../../blobs/20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0 ├── model.safetensors -&gt; ../../blobs/fdf756fa7fcbe7404d5c60e26bff1a0c8b8aa1f72ced49e7dd0210fe288fb7fe ├── README.md -&gt; ../../blobs/4b8373851d093eb9f3017443f27781c6971eff24 ├── tokenizer_config.json -&gt; ../../blobs/07bfe0640cb5a0037f9322287fbfc682806cf672 ├── tokenizer.json -&gt; ../../blobs/443909a61d429dff23010e5bddd28ff530edda00 └── vocab.json -&gt; ../../blobs/4783fe10ac3adce15ac8f358ef5462739852c569 ${HF_HOME}&#x2F;hub&#x2F;models–Qwen–Qwen2.5-0.5B-Instruct&#x2F;snapshots&#x2F;7ae557604adf67be50417f59c2c2f167def9a775这个目录就是你当前下载的repo了，可以看下config.json 中的参数 123456789101112131415161718192021222324252627&#123; &quot;architectures&quot;: [ &quot;Qwen2ForCausalLM&quot; ], &quot;attention_dropout&quot;: 0.0, &quot;bos_token_id&quot;: 151643, &quot;eos_token_id&quot;: 151645, &quot;hidden_act&quot;: &quot;silu&quot;, &quot;hidden_size&quot;: 896, &quot;initializer_range&quot;: 0.02, &quot;intermediate_size&quot;: 4864, &quot;max_position_embeddings&quot;: 32768, &quot;max_window_layers&quot;: 21, &quot;model_type&quot;: &quot;qwen2&quot;, &quot;num_attention_heads&quot;: 14, &quot;num_hidden_layers&quot;: 24, &quot;num_key_value_heads&quot;: 2, &quot;rms_norm_eps&quot;: 1e-06, &quot;rope_theta&quot;: 1000000.0, &quot;sliding_window&quot;: 32768, &quot;tie_word_embeddings&quot;: true, &quot;torch_dtype&quot;: &quot;bfloat16&quot;, &quot;transformers_version&quot;: &quot;4.43.1&quot;, &quot;use_cache&quot;: true, &quot;use_sliding_window&quot;: false, &quot;vocab_size&quot;: 151936&#125; &quot;transformers_version&quot;: &quot;4.43.1&quot; ？？这里可以通过另一种方式将他修改成对应transformers版本的内容 1234567891011121314151617181920import transformersfrom transformers import AutoModelForCausalLM, AutoTokenizerfrom huggingface_hub import snapshot_downloaddef download_from_huggingface(model_api, tokenizer_api, model_name):\tprint(f&quot; Downloading \\033[32m\\033[1m &#123;model_name&#125; \\033[0m from huggingface ... &quot;)\tmodel_dir = os.path.join(os.path.dirname(__file__), model_name)\tmodel_path = snapshot_download(repo_id=model_name) try: model = model_api.from_pretrained(model_path, trust_remote_code=True) tokenizer = tokenizer_api.from_pretrained(model_path, trust_remote_code=True)\texcept Exception as e: raise os.error(f&quot;\\033[31m Download &#123;model_name&#125; has error. Make sure it&#x27;s a valid repository. Or check your network!\\033[0m&quot;) model.save_pretrained(model_dir)\ttokenizer.save_pretrained(model_dir)\tprint(f&quot;\\033[32m\\033[1m &#123;model_name&#125; \\033[0m has been downloaded into \\033[34m\\033[5m &#123;model_dir&#125; \\033[0m&quot;)return model_dir 然后你就会发现这个新的路径下的config.json 就会编程当前transformers的版本了 1234567891011121314151617181920212223242526272829&#123;\t&quot;_name_or_path&quot;: &quot;/data/huggingface_cache/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775&quot;,\t&quot;architectures&quot;: [ &quot;Qwen2ForCausalLM&quot;\t],\t&quot;attention_dropout&quot;: 0.0,\t&quot;bos_token_id&quot;: 151643,\t&quot;eos_token_id&quot;: 151645,\t&quot;hidden_act&quot;: &quot;silu&quot;,\t&quot;hidden_size&quot;: 896,\t&quot;initializer_range&quot;: 0.02,\t&quot;intermediate_size&quot;: 4864,\t&quot;max_position_embeddings&quot;: 32768,\t&quot;max_window_layers&quot;: 21,\t&quot;model_type&quot;: &quot;qwen2&quot;,\t&quot;num_attention_heads&quot;: 14,\t&quot;num_hidden_layers&quot;: 24,\t&quot;num_key_value_heads&quot;: 2,\t&quot;rms_norm_eps&quot;: 1e-06,\t&quot;rope_scaling&quot;: null,\t&quot;rope_theta&quot;: 1000000.0,\t&quot;sliding_window&quot;: 32768,\t&quot;tie_word_embeddings&quot;: true,\t&quot;torch_dtype&quot;: &quot;float32&quot;,\t&quot;transformers_version&quot;: &quot;4.49.0&quot;,\t&quot;use_cache&quot;: true,\t&quot;use_sliding_window&quot;: false,\t&quot;vocab_size&quot;: 151936&#125; 结束语通过上面的一系列操作就可以得到你想要的模型，解决模型版本的问题，这种方法同样可以修改你已经下载了的模型，让他的参数和你的transformers版本对齐。 撒花！"},{"title":"「LLM」Attentions","path":"/undefined/tech/LLM/00_LLM_attention/","content":"attention种类 MHA一图看懂Multi-HeadAttention MQAMHA kvcache的巨大开销,直接全部共享,损失推理的效果 GQA Paged Attention 首先说明一点paged attention只是一种attention中的kvCache管理方案，并不是新的attention计算方式。 Paged顾名思义是与操作系统中的分页管理有关系，paged attention就是将kvCache进行分页管理，至于为什么要进行分页管理就需要先回顾一下LLM中的kvCache的特点。在首次prefill阶段，kvcache的大小是0，也就是输入到模型的是一个空的tensor，当然也可以不输入给模型（ps：这就需要模型支持不确定数量的输入节点，这在python代码中是很容易的，但是在端侧设备中多数是不支持可变数量的输入的），prefill阶段结束后会产生prefill输入的tokens数量+1个的kvCache，而每次输入的tokens数量并不固定，那么如何管理kvCache的内存呢？ 静态分配？ kvCache的大小是会不断增长的，静态分配是的大小设置为多少都不见得合适，太大了会占用过多内存，且可能出现大部分申请的内存并没有利用的情况，小了就不够多轮对话产生的kvcachekvcache存储。 （X） 动态分配？动态内存相对来说较为合适，但是从现存的操作系统的内存管理来看，内存碎片问题是一个很难解决的问题，这对于LLM推理来说也是一个麻烦，kvCache的大小会导致在多次对话后，没有合适的连续空间来分配给kvCache（碎片化的内存会影响这种连续分配的方式）但是kvCache还有一个特点，在一个确定的模型中，大小只受限于sequence Length这一个维度，也就是输入tokens数+输出tokens数，这也就意味着如何以这个维度为单位来存储kvCache就可以完全确定每次存储的kvCache大小。 总结来说kvCache的特点如下： 理论上大小可能不存在上限 除了sequenceLength这个维度外全是固定shape 分页式管理方案以Qwen2.5为例，kvCache的大小为（decodeLayer, k&amp;v, batchSize, numKVHead, sequenceLength, headDim）e.g. (24, 2, 1, 2, x, 128)那么这对于paged attention 来说就很容易表示存储关系了，分页式存储维护block table来管理虚拟内存和物理内存，虚拟内存中是连续分配的，而物理内存中可以是不连续分配的，而block table就是用来映射物理内存和虚拟内存关系的。每次分配一个block，一个block中可以存储多个kvCache，存储的kvCache数量为block size，因此以paged attention方式管理的kvCache的表达方式为（numBlock, blockSize, deocdeLayer,k&amp;v,batchSize,numKVHead,headDim）。下图为两次sample同时请求时的管理方式 来源七月大佬csdn 首先每个用户都会对应一个logical kv block table，用于管理自己的kvCache，多个logical table会对应一个physical kv block table，这个物理table用于非连续存储kvCache。其次，当多个用户的请求存在完全相同的内容时，会完全共用存储中的kvCache，这时也会有block的引用计数，例如block7最后，当两个用户请求产生的token出现不同时，会触发复制相同token kvCache的机制，减少计算例如block1和3。 映射方式这里就产生了一个问题，模型输入中如果只有logical table，那怎么知道这个逻辑表对应的物理表中的位置呢，这就需要一个新的属性来表示，称为slot mapping。这个slot mapping就是用来描述当前的这个作为输入的kvCache与物理设备的映射关系，数据结构通常就是一个list[N]，将logical table [0-n]映射到physical table[list[N]] 上。且并不一定logical 的最后一个block会被填满， 因此还需要一个参数来表明最后填充的数量。 123slot mapping： list[int]logical table：dict&#123;int, List[int]&#125;physical: dict&#123;int, kvCache?&#125;"},{"title":"「LLM」llama architectures","path":"/undefined/tech/LLM/01_LLM_llama/","content":"LlamaForCausalLM12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091graph TD %% 主图方向 direction RL %% 全局样式定义 classDef preprocess fill:#e8f8f5,stroke:#45b7d1; classDef decoder fill:#f9d5e5,stroke:#e892a2; classDef attention fill:#d4efdf,stroke:#7dcea0; classDef ffn fill:#e8daef,stroke:#c39bd3; classDef layer fill:#fdebd0,stroke:#eb984e; subgraph &quot;Model&quot; direction TB %% 预处理模块 subgraph &quot;Preprocessing&quot; direction TB A[Input Text] --&gt; B[Token Embedding] B --&gt; B1[Get KVCache] B1 --&gt; B2[Get cache_position] B2 --&gt; B3[Get position_ids] B3 --&gt; B4[Compute causal_mask] B4 --&gt; B5[rotary_emb] class B,B1,B2,B3,B4,B5 preprocess; end %% 解码器堆叠层 subgraph &quot;Decoder Stack&quot; direction TB B5 --&gt; D[Decoder Layer 1] D --&gt; E[Decoder Layer 2] E --&gt; F[...] F --&gt; G[Decoder Layer N] G --&gt; G1[Last LayerNorm] class D,E,F,G layer; end %% 输出模块 subgraph &quot;Output&quot; direction TB G1 --&gt; H[Lm_head] H --&gt; I[Output Layer] end end %% 解码器层详细结构 subgraph &quot;Decoder Layer Detail&quot; style D decoder direction TB D --&gt; D0[hidden_states] D0 --&gt; D1[Layer Norm] D1 --&gt; D2[Self-Attention] D2 --&gt; D3[Add] D0 -.- D3 D3 --&gt; D5[post LayerNorm] D5 --&gt; D6[MLP] D6 --&gt; D7[Add] D3 -.- D7 class D2 attention; class D5,D6 ffn; end %% 注意力层详细结构 subgraph &quot;Attention Layer Detail&quot; style D2 attention direction LR D2 --&gt; D20[attention_input] D20 --&gt; D201[K] D20 --&gt; D202[V] D20 --&gt; D203[Q] D20 --&gt; D204[attention_mask] D201 --&gt; D211[repeat_kv_k] D202 --&gt; D212[repeat_kv_v] D211 --&gt; D21[Matmul] D203 --&gt; D21 D204 --&gt; D214[slice] D214 --&gt; D22[Add] D21 --&gt; D22 D22 --&gt; D23[softmax: float32] D23 --&gt; D24[Matmul:TP&lt;1,2&gt;] D212 --&gt; D24 end %% 不可见连接引导 %% G1 -.-|缓冲连接| H %% %% D7 -.-|隐藏连接| E %% F -.-|...| G"},{"title":"「MLIR」从零开始","path":"/undefined/tech/MLIR_src/MLIR_0/","content":"LLVM、MLIR的编译先行编译基础的工具，后期用到别的工具再进行编译 1234567891011121314git clone https://github.com/llvm/llvm-project.gitcd llvm-projectmkdir build &amp;&amp; cd build# CMake配置（仅编译LLVM和MLIR， clang也可以加上）cmake -G Ninja ../llvm \\ -DLLVM_ENABLE_PROJECTS=&quot;mlir;llvm;clang&quot; \\ -DLLVM_TARGETS_TO_BUILD=&quot;host&quot; \\ -DCMAKE_BUILD_TYPE=Release \\ -DLLVM_ENABLE_ASSERTIONS=ON \\ -DCMAKE_INSTALL_PREFIX=./installninja -j$(nproc)ninja install 等待编译完成后，检查一下install目录下都生成了哪些工具，依次来学习一下。以下内容都需要在后续学习中用到后，再次进行更新，确认当前查询到的信息是正确的。 一、MLIR核心工具 ​mlir-opt​ ​用途：MLIR优化与转换的核心工具 ​功能： 执行自定义Pass管道（如 --pass-pipeline=&quot;builtin.module(my-pass)&quot;） 验证IR合法性（-verify） 调试方言降级（如从 tensor 到 memref 的转换） ​示例： bash 1mlir-opt input.mlir --convert-linalg-to-loops -o output.mlir ​mlir-translate​ ​用途：MLIR与其他中间表示（LLVM IR&#x2F;SPIR-V）的互转 ​常用选项： --mlir-to-llvmir：将MLIR转换为LLVM IR --export-llvmir：导出为LLVM可读格式 ​示例： bash 1mlir-translate --mlir-to-llvmir input.mlir -o output.ll ​mlir-tblgen​ ​用途：生成MLIR方言（Dialect）的C++代码 ​场景：定义新方言时自动生成ODS（Operation Definition Specification）代码。 ​mlir-cpu-runner​ ​用途：在CPU上直接运行MLIR代码（支持JIT编译） ​示例： bash 1mlir-cpu-runner --entry-point-result=void input.mlir ​二、LLVM核心工具 ​llc​ ​用途：将LLVM IR编译为特定目标（如CPU&#x2F;GPU）的汇编代码 ​关键选项： -mtriple=amdgcn-amd-amdhsa：指定AMD GPU目标 -O3：启用优化级别3 ​示例： bash 1llc -O3 input.ll -o output.s ​opt​ ​用途：LLVM IR优化器，用于调试自定义Pass ​示例： bash 1opt -passes=&#x27;loop-unroll&#x27; input.ll -o output.ll ​llvm-dis &#x2F; llvm-as​ ​用途：LLVM IR二进制格式（bitcode）与文本格式互转 ​示例： bash 12llvm-dis input.bc -o output.ll # 反汇编llvm-as input.ll -o output.bc # 汇编 ​三、调试与分析工具 ​llvm-objdump​ ​用途：反汇编目标文件（如检查生成的GPU二进制） ​示例： bash 1llvm-objdump -d output.o ​llvm-dwarfdump​ ​用途：调试信息分析（DWARF格式），用于检查符号表与源码映射 ​示例： bash 1llvm-dwarfdump output.o ​FileCheck​ ​用途：LLVM测试框架核心工具，用于验证输出是否符合预期模式 ​示例： bash 1mlir-opt input.mlir | FileCheck check.txt ​四、硬件相关工具 ​amdgpu-arch &#x2F; nvptx-arch​ ​用途：检测当前系统的AMD&#x2F;NVIDIA GPU架构 ​示例： bash 1nvptx-arch # 输出如 &quot;sm_80&quot;（对应NVIDIA A100） ​llvm-mca​ ​用途：静态分析机器代码的流水线吞吐量（用于性能调优） ​示例： bash 1llc -O3 input.ll -o output.s &amp;&amp; llvm-mca output.s ​五、辅助开发工具 ​clang-format​ ​用途：格式化C++&#x2F;MLIR代码（维护代码风格一致性） ​示例： bash 1clang-format -i my_pass.cpp ​mlir-reduce​ ​用途：自动缩小触发错误的MLIR测试用例（类似Delta Debugging） ​示例： bash 1mlir-reduce crash.mlir --bugpoint=mlir-opt -o reduced.mlir ​mlir-lsp-server​ ​用途：MLIR语言服务器，支持IDE语法高亮与自动补全（如VSCode插件）。"},{"title":"[LLM] 端侧芯片K230上部署大模型(简易教程)","path":"/undefined/tech/llama.cpp_on_K230/","content":"目前大模型研究非常广泛，但是对于个人用户如何部署一个大模型通常是比较麻烦的，而llama.cpp项目作为一个使用cpp来完成大模型推理的开源工具，对于没有丰富硬件资源的用户来说是一个很好用的工具，可以将中意的大模型十分简单的部署在自己的设备上。以下内容为使用llama.cpp将大模型部署在端侧设备K230上。 1. llama.cpp 下载源码 1git clone https://github.com/ggerganov/llama.cpp.git 准备K230编译工具链 首先说明一点，llama.cpp是一个纯c/c++的项目，虽然有相应的python文件作为模型转换工作，但是对于直接部署模型来说并不是很重要(毕竟个人用户也没有什么机会自己训练一个大模型，而预训练好的大模型在huggingface是很多的，同时llama.cpp需要的gguf格式的模型也可以在fuggingface上找到)。既然是c项目，那么很容易就可以通过交叉编译来生成K230上可运行的程序。 也可以使用带有rvv intrinsic的工具链进行编译，例如gcc14，这个自行尝试即可 K230的编译工具链很容易获取，编译过K230_sdk镜像的话，是可以在其toolchain目录下找到对应的工具链，拿来直接用就好。(现在我假设你已经有了噢) 1drwxr-xr-x 10 curio curio 4.0K 11月 20 12:24 Xuantie-900-gcc-linux-6.6.0-glibc-x86_64-V3.0.1 现在还缺少K230相关的编译选项，可以通过K230的编译器nncase repo来获取，保留以下内容即可： 1234567891011121314151617181920212223242526272829set(CMAKE_SYSTEM_NAME Linux)set(CMAKE_SYSTEM_PROCESSOR riscv64)if(DEFINED ENV{RISCV_ROOT_PATH}) file(TO_CMAKE_PATH $ENV{RISCV_ROOT_PATH} RISCV_ROOT_PATH)endif()if(NOT RISCV_ROOT_PATH) message(FATAL_ERROR \"RISCV_ROOT_PATH env must be defined\")endif()set(RISCV_ROOT_PATH ${RISCV_ROOT_PATH} CACHE STRING \"root path to riscv toolchain\")set(CMAKE_C_COMPILER \"${RISCV_ROOT_PATH}/bin/riscv64-unknown-linux-gnu-gcc\")set(CMAKE_CXX_COMPILER \"${RISCV_ROOT_PATH}/bin/riscv64-unknown-linux-gnu-g++\")set(CMAKE_FIND_ROOT_PATH \"${RISCV_ROOT_PATH}/riscv64-unknown-linux-gnu\")set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)set(ENABLE_VULKAN_RUNTIME OFF)set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -march=rv64imafdcv -mabi=lp64d -mcmodel=medany -fstack-protector-strong -fPIE -pie -Wl,-z,now -Wl,-z,relro\")set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -march=rv64imafdcv -mabi=lp64d -mcmodel=medany -fstack-protector-strong -fPIE -pie -Wl,-z,now -Wl,-z,relro\")// 开启rvv， K230的工具链没有添加这个宏定义，如果使用gcc14的话不需要定义。// add_definitions(-D__riscv_v_intrinsic)// 当前不推荐开启，llama.cpp 的rvv实现有问题 将以上内容写入到.cmake 文件中，在后面编译中会用到 编译1234export RISCV_ROOT_PATH=/PATH/TO/YOUR/TOOLCHAIN/Xuantie-900-gcc-linux-6.6.0-glibc-x86_64-V3.0.1/cd llama.cppcmake . -B build -DCMAKE_TOOLCHAIN_FILE=/PATH/TO/YOUR/TOOLCHAIN/k230.cmake -DCMAKE_BUILD_TYPE=Releasecmake --build build --config Release -j 成功编译后可以在build/bin目录下找到llama-cli 下载模型直接在huggingface上下载 准备上板执行将llama.cpp 和 qwen2.5-0.5b-instruct-q2_k.gguf拷贝到K230上 执行命令 1./llama-cli -m qwen2.5-0.5b-instruct-q2_k.gguf -p \"system name whatever\" -cnv 这里的-cnv是用于开启对话模式，稍等几分钟就可以进行对话(虽然现在推理还很慢，如果开启rvv优化，这个2bit 量化的模型在K230上性能可以达到0.8t/s)"},{"title":"通过cmake依赖分析项目结构","path":"/undefined/tech/cmake_dependence/","content":"拿到一个大的项目是否无从下手，不知道从哪里开始阅读代码，分析逻辑，最简单的办法应该就是通过cmake来分析库之间的依赖，然后逐个模块进行代码阅读。那么如果对cmake代码不是非常熟悉，直接看CMakeLists是很痛苦的，今天就就少一个简单的工具来进行cmake依赖分析 Graphviz安装在Linux平台直接通过apt安装 1sudo apt-get install graphviz 使用在cmake config命令中加入–graphviz=dep.dot例如： 1cmake --preset conan-runtime-release --graphviz=dep.dot 之后会在当前目录下生成dep.dot 以及各个可执行程序单独的模块依赖"},{"title":"windows-dll-missing-debug","path":"/undefined/tech/DLL missing debug/","content":"当程序出现类似于下面的错误时： 1234567# python&gt;&gt;&gt; import nncaseTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;D:\\a ncase ncase\\install\\python ncase\\__init__.py&quot;, line 34, in &lt;module&gt; import _nncaseImportError: DLL load failed while importing _nncase: The specified module could not be found. 这意味着系统中存在某个dll确实的情况，一种简单的排查手段是使用Dependency Walker来进行定位，github链接为：Dependency Walker github Dependency Walker可以直接使用wget下载： 12wget https://github.com/lucasg/Dependencies/releases/download/v1.11.1/Dependencies_x64_Release.zipunzip xxx.zip 命令行模式进入对应的目录下，找到Dependencies.exe（命令行执行） 1./Dependencies.exe -depth 2 -chain _nncase.cp38-win_amd64.pyd log如下： 12345678910111213141516# ./../../../Dependencies.exe -depth 2 -chain _nncase.cp38-win_amd64.pyd ├ _nncase.cp38-win_amd64.pyd (ROOT) : _nncase.cp38-win_amd64.pyd | ├ Nncase.Runtime.Native.dll (Environment) : D:\\a ncase ncase\\install\\bin\\Nncase.Runtime.Native.dll | | ├ libomp140.x86_64.dll (WindowsFolder) : C:\\Windows\\system32\\libomp140.x86_64.dll | | ├ KERNEL32.dll (WellKnownDlls) : C:\\Windows\\system32\\kernel32.dll | | | ├ api-ms-win-core-rtlsupport-l1-1-0.dll (ApiSetSchema) : C:\\Windows\\system32 tdll.dll | | | ├ api-ms-win-core-rtlsupport-l1-2-2.dll (ApiSetSchema) : C:\\Windows\\system32 tdll.dll | | | ├ ntdll.dll (WellKnownDlls) : C:\\Windows\\system32 tdll.dll | | | ├ api-ms-win-core-appcompat-l1-1-1.dll (ApiSetSchema) : C:\\Windows\\system32\\kernelbase.dll| | | ├ ext-ms-win-oobe-query-l1-1-0.dll (NOT_FOUND) :| | | ├ RPCRT4.dll (WellKnownDlls) : C:\\Windows\\system32\\rpcrt4.dll| | ├ ADVAPI32.dll (WellKnownDlls) : C:\\Windows\\system32\\advapi32.dll| ├ python38.dll (Environment) : C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\python38.dll| | ├ VERSION.dll (WindowsFolder) : C:\\Windows\\system32\\VERSION.dll| | ├ api-ms-win-crt-filesystem-l1-1-0.dll (ApiSetSchema) : C:\\Windows\\system32\\ucrtbase.dll| ├ KERNEL32.dll (WellKnownDlls) : C:\\Windows\\system32\\kernel32.dll 可以很容易发现ext-ms-win-oobe-query-l1-1-0.dll这个dll不存在，然后google对应的问题解决 目前该问题没有合适的解决方法，只是操作系统版本不匹配导致 gflags该方法目前没有找到非常合适的使用案例，tmate中无法使用event viewer工具，无法查看系统日志gflags.exe 的命令使用如下e 1/c/Program\\ Files\\ \\(x86\\)/Windows\\ Kits/10/Debuggers/x64/gflags.exe -i c:/hostedtoolcache/windows/Python/3.8.10/x64/python.exe +sls GUI界面如果要使用GUI，则执行DependenciesGUI.exe Dumpbin如何在tmate中使用dumpbin首先需要激活vscmd环境，不能直接使用windows server中的命令行，否则无法找到dumpbin工具$ cd &#x2F;c&#x2F;Program Files&#x2F;Microsoft Visual Studio&#x2F;2022&#x2F;Enterprise&#x2F;Common7&#x2F;Tools$ .&#x2F;LaunchDevCmd.bat之后就激活了vscmd命令行环境，vs相关的工具都可以使用 dumpbin使用首先使用&#x2F;DEPENDENTS 来查找当前库所依赖的dll，之后逐级查找dumpbin &#x2F;DEPENDENTS _nncase.cp38-win_amd64.pyd 123456789101112Microsoft (R) COFF/PE Dumper Version 14.41.34120.0Copyright (C) Microsoft Corporation. All rights reserved.Dump of file _nncase.cp38-win_amd64.pydFile Type: DLL Image has the following dependencies: Nncase.Runtime.Native.dllpython38.dllKERNEL32.dll dumpbin &#x2F;DEPENDENTS D:\\a ncase ncase\\install\\bin\\Nncase.Runtime.Native.dll 123456789Dump of file D:\\a ncase ncase\\install\\bin\\Nncase.Runtime.Native.dllFile Type: DLL Image has the following dependencies: libomp140.x86_64.dllKERNEL32.dllADVAPI32.dll dumpbin &#x2F;DEPENDENTS C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\python38.dll 123456789101112131415161718192021222324Dump of file C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\python38.dllFile Type: DLL Image has the following dependencies: VERSION.dllSHLWAPI.dllWS2_32.dllADVAPI32.dllKERNEL32.dllVCRUNTIME140.dllapi-ms-win-crt-math-l1-1-0.dllapi-ms-win-crt-locale-l1-1-0.dllapi-ms-win-crt-string-l1-1-0.dllapi-ms-win-crt-runtime-l1-1-0.dllapi-ms-win-crt-stdio-l1-1-0.dllapi-ms-win-crt-convert-l1-1-0.dllapi-ms-win-crt-time-l1-1-0.dllapi-ms-win-crt-environment-l1-1-0.dllapi-ms-win-crt-process-l1-1-0.dllapi-ms-win-crt-heap-l1-1-0.dllapi-ms-win-crt-conio-l1-1-0.dllapi-ms-win-crt-filesystem-l1-1-0.dll dumpbin &#x2F;DEPENDENTS C:\\Windows\\System32\\kernel32.dll 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111Dump of file C:\\Windows\\System32\\kernel32.dllFile Type: DLL Image has the following dependencies: api-ms-win-core-rtlsupport-l1-1-0.dllapi-ms-win-core-rtlsupport-l1-2-2.dllntdll.dllKERNELBASE.dllapi-ms-win-core-processthreads-l1-1-0.dllapi-ms-win-core-processthreads-l1-1-3.dllapi-ms-win-core-processthreads-l1-1-2.dllapi-ms-win-core-processthreads-l1-1-1.dllapi-ms-win-core-registry-l1-1-0.dllapi-ms-win-core-heap-l1-1-0.dllapi-ms-win-core-heap-l2-1-0.dllapi-ms-win-core-memory-l1-1-1.dllapi-ms-win-core-memory-l1-1-0.dllapi-ms-win-core-memory-l1-1-2.dllapi-ms-win-core-handle-l1-1-0.dllapi-ms-win-core-synch-l1-1-0.dllapi-ms-win-core-synch-l1-2-1.dllapi-ms-win-core-synch-l1-2-0.dllapi-ms-win-core-file-l1-1-0.dllapi-ms-win-core-file-l1-2-0.dllapi-ms-win-core-file-l1-2-2.dllapi-ms-win-core-file-l1-2-4.dllapi-ms-win-core-file-l1-2-1.dllapi-ms-win-core-delayload-l1-1-0.dllapi-ms-win-core-io-l1-1-0.dllapi-ms-win-core-io-l1-1-1.dllapi-ms-win-core-job-l1-1-0.dllapi-ms-win-core-threadpool-legacy-l1-1-0.dllapi-ms-win-core-threadpool-private-l1-1-0.dllapi-ms-win-core-largeinteger-l1-1-0.dllapi-ms-win-core-libraryloader-l1-2-3.dllapi-ms-win-core-libraryloader-l1-2-2.dllapi-ms-win-core-libraryloader-l1-2-0.dllapi-ms-win-core-libraryloader-l1-2-1.dllapi-ms-win-core-libraryloader-l2-1-0.dllapi-ms-win-core-namedpipe-l1-2-2.dllapi-ms-win-core-namedpipe-l1-1-0.dllapi-ms-win-core-namedpipe-l1-2-1.dllapi-ms-win-core-datetime-l1-1-1.dllapi-ms-win-core-datetime-l1-1-0.dllapi-ms-win-core-datetime-l1-1-2.dllapi-ms-win-core-sysinfo-l1-2-0.dllapi-ms-win-core-sysinfo-l1-2-1.dllapi-ms-win-core-sysinfo-l1-1-0.dllapi-ms-win-core-sysinfo-l1-2-3.dllapi-ms-win-core-timezone-l1-1-0.dllapi-ms-win-core-localization-l1-2-0.dllapi-ms-win-core-processsnapshot-l1-1-0.dllapi-ms-win-core-processenvironment-l1-1-0.dllapi-ms-win-core-processenvironment-l1-2-0.dllapi-ms-win-core-string-l1-1-0.dllapi-ms-win-core-debug-l1-1-0.dllapi-ms-win-core-debug-l1-1-1.dllapi-ms-win-core-errorhandling-l1-1-0.dllapi-ms-win-core-errorhandling-l1-1-3.dllapi-ms-win-core-fibers-l1-1-0.dllapi-ms-win-core-util-l1-1-0.dllapi-ms-win-core-profile-l1-1-0.dllapi-ms-win-security-base-l1-1-0.dllapi-ms-win-security-base-l1-2-0.dllapi-ms-win-security-appcontainer-l1-1-0.dllapi-ms-win-core-comm-l1-1-0.dllapi-ms-win-core-realtime-l1-1-0.dllapi-ms-win-core-wow64-l1-1-1.dllapi-ms-win-core-wow64-l1-1-0.dllapi-ms-win-core-wow64-l1-1-3.dllapi-ms-win-core-systemtopology-l1-1-1.dllapi-ms-win-core-systemtopology-l1-1-0.dllapi-ms-win-core-processtopology-l1-1-0.dllapi-ms-win-core-namespace-l1-1-0.dllapi-ms-win-core-file-l2-1-2.dllapi-ms-win-core-file-l2-1-0.dllapi-ms-win-core-file-l2-1-1.dllapi-ms-win-core-file-l2-1-3.dllapi-ms-win-core-xstate-l2-1-0.dllapi-ms-win-core-xstate-l2-1-1.dllapi-ms-win-core-xstate-l2-1-2.dllapi-ms-win-core-localization-l2-1-0.dllapi-ms-win-core-normalization-l1-1-0.dllapi-ms-win-core-fibers-l2-1-0.dllapi-ms-win-core-fibers-l2-1-1.dllapi-ms-win-core-localization-private-l1-1-0.dllapi-ms-win-core-sidebyside-l1-1-0.dllapi-ms-win-core-appcompat-l1-1-0.dllapi-ms-win-core-windowserrorreporting-l1-1-1.dllapi-ms-win-core-windowserrorreporting-l1-1-2.dllapi-ms-win-core-windowserrorreporting-l1-1-0.dllapi-ms-win-core-windowserrorreporting-l1-1-3.dllapi-ms-win-core-console-l1-1-0.dllapi-ms-win-core-console-l1-2-0.dllapi-ms-win-core-console-l1-2-1.dllapi-ms-win-core-console-l2-1-0.dllapi-ms-win-core-console-l2-2-0.dllapi-ms-win-core-console-l3-2-0.dllapi-ms-win-core-psapi-l1-1-0.dllapi-ms-win-core-psapi-ansi-l1-1-0.dllapi-ms-win-eventing-provider-l1-1-0.dllapi-ms-win-core-apiquery-l1-1-0.dllapi-ms-win-core-delayload-l1-1-1.dllapi-ms-win-core-appcompat-l1-1-1.dllImage has the following delay load dependencies:ext-ms-win-oobe-query-l1-1-0.dllRPCRT4.dll"},{"title":"tmate ssh github CI","path":"/undefined/tech/ssh调试github CI/","content":"CI workflow中增加远程配置123456- name: Start tmate session for debugging uses: mxschmitt/action-tmate@v3 continue-on-error: true# 例如，如果编译失败，可以通过SSH进行调试- run: echo &quot;Run after tmate session&quot; 这一步添加到你想要开始调试的位置,CI运行后会出现如下log: 12ssh nM72R3SZDXgCtPqB4DGha8yax@nyc1.tmate.ioSSH: ssh nM72R3SZDXgCtPqB4DGha8yax@nyc1.tmate.io 在命令行运行即可:q 退出当前界面即可登入github CI环境"},{"title":"北京半日游攻略","path":"/undefined/Beijing_half_day_travel/","content":"好玩的线路1天安门广场 –&gt; 毛主席纪念堂–&gt; 故宫 &#x2F;国家博物馆 该路线可以在周六早上进行(故宫&#x2F;国博二选一能一上午结束) 地点 预约时间 预约方式(公众号或者小程序) 8月30日 8月31日(周六) 9月1日 备注(很多地方周一是闭馆的) 预约日期 升旗 00:00 天安门印象 2105:39 2205:40 2305:40 天安门广场禁止打火机升旗时间 纪念堂 12:00 毛主席纪念堂预约瞻仰 24 25 26 大约不到1小时(前提是能约到最早的那个时间段) ,不能穿的太随意 故宫 20:00 故宫博物院 23 24 25 国博 17:00 国家博物馆 23 24 25 线路2天坛(提前7天 00:00放票 天坛公园官网购票平台 &lt;上次去门口买票就行&gt;) –&gt; 前门大街天坛: 可以悠闲溜达溜达, 人比颐和园少前门: 各种小吃, 至于它地不地地道道(不知道没去过), 人多警告 线路3颐和园(提前7天 21:00放票) “颐和园”公众号人多, 但是逛起来还是可以的, 推荐坐个船啥的(接近中午的时候体验比较好), 体验下皇帝同款? 简单的游玩，而且比较有北京特色的，大概就这些个了 Tips:长城至少得一天时间玩，远！时间少的就不推荐了 好吃的 烤鸭，炸酱面，小吊梨汤： 四季民福 (故宫里也有)，有钱可以去吃大董(没吃过，吃不起)，没必要专门去方砖厂吃炸酱面，烤鸭店基本都有 涮羊肉，爆肚： 个人觉得哪都差不太多，反正麻酱沾一沾都挺好吃 卤煮：比较重口味，味道独特可以尝试， 大众点评找吧，没吃过几次 炒肝： 早餐店基本都会有，配着包子吃 稻香村：建议去店里各样买一块常常，再决定要不要给家里带大盒的，不要买直接装好的盒装产品，里面的真不一定是最好吃的！"},{"title":"cross-compile-debug","path":"/undefined/tech/cross-compile-dbg/","content":"交叉编译的程序如何调试1. 调试工具工具链gdb+gdbserver 2. 调试方法 需要使用交叉编译工具链先完成gdbserver的编译，然后放到target上，之后使用如下命令启动 123gdbserver host_ip:port /path/to/program args..../gdbserver 192.168.1.2:1234 ./nncase_test_v2.elf $&#123;model&#125;/test.kmodel $&#123;model&#125;/input_0_0.bin $&#123;model&#125;/nncase_result_0.bin 在host上使用gdb连接 1234gdbtarget remote target_ip:porttarget remote 192.168.1.22:1234 之后在host上正常使用gdb调试命令进行调试 1234b rvv_memcpy # 设置断点run # 开始执行stepi # step ininfo registers # 查看寄存器 3. host配置问题 安装nfs 1sudo apt-get install nfs-kernel-server 配置共享文件夹 1234vim /etc/exports#添加下面内容/home/curio/project/k230/rebuild-ir/k230_linux 192.168.1.22(rw,sync,no_root_squash,no_all_squash) #192.168.1.22也可以替换成 * 表示任意地址可以访问 target上配置 123ifconfig eth0 192.168.1.22 # 配置target ipmkdir /modules # mount -t nfs -o nolock 192.168.1.2:/home/curio/project/k230/rebuild-ir/k230_linux/ /modules #nfs挂载host目录"},{"title":"Latex语法归纳","path":"/undefined/tech/latex/","content":"数学公式等式关系 小于等于121. a\\le b2. a\\leq b 大于等于121. a\\ge b2. a\\geq b 不等于11. a eq b 行内换行(多行表达式在一行显示)123\\atop1. aa \\atop bb + cc2. \\min_{aa \\atop bb} 上下标121. a_{aa}2. b^{bb} 公式编号11. \\tag{1}"},{"title":"分布式计算和并行编程中常用的通信原语","path":"/undefined/tech/Communication_primitives/","content":"1. broadcast目的：将一个进程（通常称为根进程）持有的数据复制到所有其他参与通信的进程中。操作： 根进程发送其数据至所有其他进程。 其他进程接收并存储根进程发送的数据。结果：所有进程中都有一份与根进程相同的数据副本。应用场景： 初始化阶段，将初始参数或配置信息分发到所有计算节点。 从主节点向工作节点广播最新的模型权重或状态信息。 2. reduce目的：将所有进程各自拥有的相同类型和形状的数据进行某种全局运算（如求和、最大值、最小值、乘积等），并将运算结果保存在指定的一个进程中（通常称为根进程）。操作： 各进程通过高效通信算法逐级或逐环地局部聚合数据，每次聚合后将结果传递到下一个阶段。 最终，根进程保留全局运算的结果。结果：只有根进程拥有经过全局运算后得到的单一结果数据，其他进程不保留结果。应用场景： 计算全局统计指标，如总损失、平均精度等，只需一个进程知道全局结果即可。 并行计算中的收敛检测，根进程收集各进程的局部收敛信息以判断全局收敛状态。 3. scatter目的：将根进程持有的数据分割成多个片段，并将这些片段均匀地分发给所有参与通信的进程。操作： 根进程将数据按照一定的规则（如均等分割、按需分配等）拆分成多个部分。 根进程将每个数据片段发送给相应的目标进程。 目标进程接收并存储分配给自己的数据片段。结果：每个进程最终拥有整个数据集中的一部分，这些部分合起来构成原始数据。应用场景： 分布式训练中，将大型数据集均匀划分给各个计算节点进行并行处理。 分配任务参数或初始化数据到各个工作节点。 4. gather目的：将所有进程各自拥有的数据片段收集到一个指定进程（通常称为根进程）。操作： 各进程将其本地数据发送给根进程。 根进程接收到所有进程发送过来的数据后，将其拼接整合成完整的数据集。结果：只有根进程拥有所有进程发送过来的数据片段组成的完整数据集，其他进程不保留结果。应用场景： 收集各个计算节点的中间结果或局部统计信息，汇总到主节点进行后续处理或决策。 训练完成后，从各个工作节点收集模型权重以进行模型融合或持久化存储。 5. reduce-scatter目的：将所有进程各自拥有的相同类型和形状的数据进行某种全局运算（如求和、最大值、最小值、乘积等），并将运算后的结果分散到各个进程中，每个进程获得一部分结果。操作： 各进程先进行类似reduce的操作，逐步聚合数据并产生全局运算结果。 然后将全局结果按照特定规则分割，并将分割后的片段分散到各个进程。结果：每个进程最终拥有全局运算结果的一部分，这些部分合起来构成全局运算结果。应用场景： 分布式矩阵乘法中，计算矩阵乘积后将其分割回各个节点，用于后续计算。 高效实现某些特定算法，如分布式K-means聚类中的中心点更新。 6. all-to-all目的：每个进程拥有一个数据向量，通信完成后，每个进程获得一个新向量，其中的每个元素来自原来不同进程对应位置的元素。操作： 各进程按照通信协议，交换各自向量中的元素。 每个进程接收到来自其他进程对应位置的元素，并将其放置在新向量的相应位置。结果：所有进程都得到了一个新的数据向量，其中的每个元素来源于参与通信的其他进程。应用场景： 实现数据块的全排列或重新分布，如在神经网络训练中进行层间数据交换。 并行FFT（快速傅里叶变换）等算法中的数据重组。 7. all-gather目的：将参与通信的所有进程各自拥有的不同数据片段合并成一份完整的数据集，并确保这份完整的数据集被所有进程所拥有。操作： 每个进程拥有一个本地的数据片段。 通过通信协议（如MPI、NCCL、Gloo等），每个进程将其本地数据发送给所有其他进程。 所有进程接收到其他进程发送过来的数据后，将其拼接到本地已有的数据片段上，形成完整的数据集。结果：每个进程最终都拥有了一份完整的数据集，包含了所有参与通信进程最初各自拥有的数据片段。应用场景： 在分布式训练中，可能需要将各个设备上的梯度片段收集起来，以便在全局视角下进行参数更新。 在并行计算中，可能需要汇总各个进程计算得到的部分结果，形成全局结果。 8. all-reduce目的：将参与通信的所有进程各自拥有的相同类型和形状的数据进行某种全局运算（如求和、最大值、最小值、乘积等），并将运算结果广播回所有进程。操作： 每个进程拥有一个相同类型和形状的数据元素。 通过高效的通信算法（如树形Reduce、环形Reduce、层级Reduce等），进程间按照指定的运算规则逐步聚合数据。 经过多轮通信和局部运算，每个进程最终得到全局运算的结果。结果：所有进程都拥有经过全局运算后得到的同一份结果数据。应用场景： 在分布式训练中，计算全局的梯度平均值，用于参数更新。 在并行计算中，求解全局的最大值、最小值、累计值等统计量。 总结： all-gather 的目的是将所有进程的独立数据片段合并成一份完整的数据集，每个进程最终拥有全部数据。 all-reduce 的目的是对所有进程的相同数据进行全局运算，每个进程最终拥有运算后的同一份结果数据。 两者都是为了实现分布式环境下的数据聚合，但前者侧重于数据的收集与复制，后者侧重于数据的并行运算与结果广播。在实际应用中，根据任务需求选择合适的通信原语可以有效提升并行计算的效率和可扩展性。这些通信原语构成了分布式计算中基本的数据交换机制，根据实际应用需求的不同，可以单独或组合使用这些原语来构建高效的并行算法和分布式系统。"},{"title":"Alpa论文阅读","path":"/undefined/paper/Alpa/","content":"论文链接：[https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin] 内容简介Alpa通过生成统一数据、操作符和管道并行性的执行计划，自动化大型深度学习(DL)模型的模型并行训练。现有的模型并行训练系统要么要求用户手动创建并行化计划，要么从有限的模型并行配置空间中自动生成并行化计划。它们不足以在分布式计算设备上扩展复杂的深度学习模型。Alpa通过将并行性视为两个层次:操作符间并行性和操作符内并行性来分配大型DL模型的训练。在此基础上，Alpa为大规模模型并行执行计划构建了一个新的分层空间。Alpa设计了许多编译通道，以便在每个并行级别自动派生有效的并行执行计划。Alpa实现了一个高效的运行时来协调分布式计算设备上的两级并行执行。我们的评估表明，Alpa生成的并行化计划可以匹配或优于手动调整的模型并行训练系统，甚至在它们设计的模型上也是如此。与专门的系统不同，Alpa还泛化到具有异构体系结构的模型和没有手动设计计划的模型。 通过inter-op pass将计算图分割为子图，设备集群分割为device mesh，并寻找将子图分配给device mesh的最佳方式。 通过intra-op pass为inter-op pass找到每个流水线阶段的子图-device mesh的最佳并行方案 运行时编排 生成静态指令，按照顺序排列计算和通信操作，并启动设备集群上的分布式计算图推理。 出发点 创新点在两个层级上进行分布式并行执行策略：intra-op 和 inter-op ，并且这两种策略可以混合使用。 intra-op的并行&lt;个人理解为算子内的并行&gt;，通过整数线性规划(ILP)来最小化执行成本，将其算子内的并行策略分配到分布式设备中的不同网格(mesh)，并返回其最小的执行成本给inter-op并行过程。 inter-op的并行&lt;个人理解为算子间的并行&gt;，将模型分割放在分布式设备中的不同网格(mesh)中执行，在该过程中通过DP来最小化不同设备通信的latency成本，最终确定最佳的分布式策略。 以下为这两点的基础示意图 device mesh概念是将一组物理设备视作2D的逻辑设备。每个设备(x,y)都可以沿着这两个维度以不同的带宽进行通信。假设不同组的设备沿着同一个维度进行通信时具备同样的性能。device mesh可以由节点和GPU灵活组成，例如2个节点，每个节点8个GPU，通过inter-op pass可以将其组成2x8，1x16，4x4，8x2的device mesh，而不会局限于单个节点，这些不同的组合方式会导致不同的设备通信代价&lt;意味着同一个mesh中可能存在跨节点的设备，这就相比与同一个节点设备有更高的通信成本&gt;，后续也会通过inter-op pass 来优化物理设备和逻辑device mesh的映射。 4. Intra-op 并行在所给的device mesh上，最小化执行计算子图的算子内并行成本。根据工作负载的分配，不同的mesh可能会有不同数量的计算设备。采用SPMD风格的算子内并行，认为单个device mesh中所有的设备都有等效的计算能力，将所有计算平均分配到所有的设备上，执行相同的指令。 平均分配的方式减少了分布策略的搜索空间，更加方便的表示数据并行，算子并行等策略。这种方式与Tofu和FlexFlow(自动算子并行)不同。与FlexFlow采用的随机搜索和Tofu采用的假设线性不同的是，Alpa采用ILP来求解具有数万个运算符的模型。 算子内并行空间对于计算图上的一个算子，在mesh上可能有多种并行算法。 对于矩阵乘来说，并行可以在ijk任意维度，或者[跨设备并行化ijk的组合?]，不同的方式就会有不同的计算和通信成本，不同的方式也都要求输入tensor有不同的layout，也导致输出tensor有不同的layout。如果输入的layout和并行方式需求的layout存在差异，那么久需要额外的转换，这也进一步增加通信开销。intra-op pass的目的就是给每个算子选择一个并行策略，来让整个图的执行时间最小。 Sharding Spec分片规范上面这个表格是一组Sharding Spec，用于描述一个tensor的不同layout，对于一个N-D的tensor来说，他的Sharding Spec会被定义为 ， , S表示切分，R表示复制。Table 1.中最左侧就是可选择的layout， 表示在mesh的第一个维度进行切分，表示在mesh的两个维度都进行切分，Spec中的两个字母表示tensor的两个维度的sharding策略。 Resharding当一个算子的输入tensor layout不满足当前为该算子选择的并行策略，且这个并行策略可能会有跨设备的通信，就需要加入 resharding这个layout转换。表格2. 就是几个resharding的例子。例如，a)要将一个全复制的tensor转换成其他的sharding spec时(case 1) ，可以在不通信的情况下局部切分tensor；b)交换切分轴(case 4)，需要使用all-to-all的通信源语了，详见Communication_primitives。 算子的并行算法结合以上内容，考虑在2D的mesh上将一个3D矩阵乘 并行化。表3. 列出了几种算子内并行策略最左侧一列表示将哪个维度进行并行，0和1 表示并行于mesh哪个维度。此外例如conv和reduction也可以通过分析其数学表达式来得到类似的所有可并行策略。Alpa中使用XLA的HLO作为基础算子，(为啥是XLA呢，因为算子少，工作量少 :-) ) ILP 公式一个计算图全部的执行开销包含：所有算子的计算开销、通信开销以及所有路径的resharding开销。这里将成本最小化表述为ILP，并通过一个现成的求解其进行最优求解。以下是论文，后续再看。 http://www.decom.ufop.br/haroldo/proglinear/files/cbcUserGuide.pdf 对于一个算子, 可能存在的并行策略数量是 ，有一个通信开销向量 长度为 ，或者可以描述为 &lt;长度为的实数向量空间&gt;，表示第i个策略的通信开销。类似的，计算开销向量表示为 。同时有一个决策向量 来表示那个策略被使用， 表示我们给算子选择了第i个并行策略。对于算子之间的Resharding开销来说，定义了一个矩阵 ，表示算子的第i个并行策略的输出到算子的第个并行策略的输入的Resharding开销目标函数如下: 前一部分是算子的计算开销和通信开销，后面一部分是从算子到算子 的resharding开销。在上面这个公式中是变量，其余都是常量值。第二部分是二次方程式，所以无法使用ILP求解器。这里通过引入一个新的决策向量将其线性化，这个向量表示算子之间的resharding决策。新的公式如下： 虽然可以通过分析来获得上面三个常量的准确成本，但是为了简单起见，采用了以下方法来评估它们。 用通信字节数并除以mesh维度带宽来得到通信开销、 将所有的计算开销设置为0，动机与https://arxiv.org/abs/1807.08887 一致， 具体原因需要阅读该论文 之所以进行这样的简化是因为： 对于计算量很高的算子，例如matmul，不允许在重复计算。所有的并行策略总是会把所有的工作都分配到所有的设备上，所以一个算子所有的并行策略都会具有相同的算数复杂度。 对于计算量不高的算子，例如element-wise，允许复制操作，但是它们的计算开销可以忽略不计。为了简化计算图，将计算量极少的算子进行融合，比如element-wise、transpose、reduction等，将这些算子融合到计算图的其他算子中，并将目标算子的sharding spec进行传播，从而使这些计算量很少的算子尽可能的隐藏到其他算子庞大的计算开销、通信开销中。这种方式极大的减少了计算图中算子的数量，从而减少了ILP问题的大小。通过广度优先遍历的方式来计算每个算子的深度，然后将这些计算量少的算子尽可能的融合到最深的算子里。一旦并行策略被ILP决定，就会应用一系列post-ILP通信优化，就比如在合适的情况下将all-reduce操作用reduce-scatter和all-gather取代，因为后者减少了复制tensor的数量和相应的计算量，同时保持通信量不变，这也达到了与权重sharding或者ZrRO优化其一样的效果。 5. Inter-op 并行在最小化算子间并行的延迟问题上，主要考虑如何切分模型和device cluster到每个阶段和mesh，以及如何将他们映射成stage-mesh对。优化的目标是最小化整个计算图端到端pipeline的执行延迟，[17，33] 论文中已经考虑了一些简单的问题，例如假设每个阶段的设备是预先分配好的，并且所有阶段都是有固定的数据或者算子并行策略。Apla中通过联合考虑device mesh的分配和每个阶段存在不同的算子内并行策略舍弃了这些假设。 算子间并行策略的搜索空间假设计算图中包含了一个算子序列，并且具备图一样的拓扑顺序，标记为 ，其中操作的输入来自。将其切分为S个阶段 ，每个阶段都有一些算子组成 ,并且将每个阶段分配到一个大小的的子mesh上，子mesh是从包含deivce的计算机集群中切分出来的，这个计算机集群标记为带有形状为的cluster mesh。让 作为执行 阶段在大小为的子mesh上执行的延迟，这是通过ILP得出的最小延迟，由intra-op pass中返回给inter-op pass。 Curio：上图中abcd..表示一个输入的不同batch，stage表示子图和device mesh的组合，时间表示每个stage执行一个batch的时间开销。总体的延迟就是最耗时的子图的所有batch的计算时间加上一个batch的其他子图的执行时间(其他batch的其他子图执行的时间就被掩盖到最耗时的子图执行时间中，这部分的计算中主要是取决于子图-device mesh的划分，尽可能将数据依赖和子图的执行时间开销平衡) 如图所示，假设有B个不同的输入微batch，对于整个计算图的最小延迟可以表示为如下公式。 Missing or unrecognized delimiter for \\left T^{*} = \\min_{\\substack{s_1,…,s_S \\ (n_1,m_1),…,(n_S,m_S)}} \\left { \\sum^S_{i=1} t_i + (B-1) \\cdot \\max_{1\\leq j\\leq S}{t_j} \\right } \\tag{2} 总延迟包含两个部分：第一部分是所有阶段的总延迟，解释为第一个微batch通过整个流程的延迟；第二部分是剩余个微batch的执行时间，该时间由最慢的阶段所限制，如上图中的第三阶段。目标是解决上述带有两个约束的公式：1）对于计算图前向传播的算子，我们想要将其与对应的反向算子放在同一个子mesh上。因为反向传播通常使用一系列与正向传播相似的tensor，这有效地减少从前向传播中获取所需的tensor到反向传播阶段的通信量。使用前向和后向的延迟之和作为 ，所以公式2反映了总体的延迟，包括了前向和后向。2）需要已经切分的子mesh能够完全地覆盖的集群mesh——不浪费任何计算设备资源。接下来详细说明DP公式。 DP 公式为了确保所有的子mesh能够完整的覆盖的mesh集群，将可用的子mesh的形状转化为两种类型：1）1D的子mesh，形状为 ；2）2D的子mesh，形状为，这种方式充分地利用了mesh集群的第二个维度（例如，在一个GPU集群中，这意味着每一个物理机器充分使用所有的计算设备）；具体的定理证明见附录A，表明了这些特定的子mesh能够始终完全覆盖mesh集群。为了将集群中的物理设备分配给DP算子找到的结果子mesh，分配设备时按照从大尺寸mesh到小尺寸的策略，即优先为较大的子mesh分配设备，然后再分配给小的子mesh。 &lt;这样的分配逻辑有助于充分利用高性能的通信链路，因为在大型子网格中，设备间的通信往往更为高效。这是因为大型子网格可能横跨多台物理机器，而这些机器之间的数据传输速度通常高于单台机器内部设备间的传输速度。？？？？源自LLM的解释&gt; 当有多个相同大小子mesh的pipeline阶段时，倾向于将相邻的pipeline阶段放的更近，从而减少通信延迟。 –==剪枝的策略==–对于一系列子mesh， 的组合配剔除集合中，这些组合有着较差的表现，因为作为可选的子mesh具有更多可以用高带宽进行通信的设备，这种简化中只需要确保 为了求解公式2的 ，这里开发了一个动态规划算法。首先枚举了第二项 并且对于每个不同的最小化第一项 ，具体来说，当将算子使用函数 来表示把从第个算子到最后的算子序列切分为个阶段后，分配到个设备上的最小总延迟，任何一个阶段的延迟都不超过 。开始时设置 ，推导出F的最优子结构为： $$F(s,k,d; t_{\\max}) = \\min_{\\substack{k \\leq i \\leq K \\ n_s \\cdot m_s \\leq d}}\\begin{cases}t_{\\text{intra}}((o_k, …, o_i), \\text{Mesh}(n_s, m_s), s) \\ F(s-1, i+1, d-n_s \\cdot m_s; t_{\\max}) \\| t_{\\text{intra}}((o_k, …, o_i), \\text{Mesh}(n_s, m_s), s) \\leq t_{\\max}\\end{cases} \\tag{3}$$ 并推导出最佳总延迟为: 其中是由intra-op pass来决定的，这是子序列阶段在上执行子图的最低延迟。这个Mesh是一系列物理设备，因此枚举出所有逻辑mesh潜在的shape可能性，并通过intra-op pass查询子图、当前mesh和子图的输入的来自其他intra-op 的设置，并得到一个intra-op的计划。之后按照这个计划和其他的一些low-level编译器优化(fusion,mem)来编译这个子图，得到一个可执行文件来进行精确的分析。这个可执行程序是为了获取这个stage的延迟和每个设备上执行这个阶段所需要的mem，以及保存中间结果所需要的mem。之后根据所选择的pipeline执行计划来检查这个需要的mem是否与设备的mem合适，这里存在一个约束： 也就是说执行这个stage需要的内存加上保存中间结果的内存必须小于设备内存(显而易见的约束，不然多出来的放到设备外？增加通信代价，得不偿失)，选择一个满足最小延迟同时满足设备mem的逻辑mesh，如果都不满足，就把置0。这个DP算法是基于TeraPipe的，然而TeraPipe中假设所有pipeline stage都是相同的，并且目标是找到一个最优的方式将带Batch的输入token组织成大小不同的微batch。alpa的目标是将计算图的算子划分成不同的组，放到不同的pipeline stages，是在假设所有微batch都有相同的大小。此外alpa用DP算法在inter-op pass中优化每个stage pipeline 的mesh shape。 TeraPipe论文阅读 复杂度在 的时间内计算固定的切分。最多有 种选择: 和所有子mesh选择。因此总的复杂度是 。这个时间复杂度对于大的计算图(超过1W个算子)来说并不可行。为了加速DP，做以下实用的优化 early pruning提前剪枝，1）递增枚举：方法类似于TeraPipe，将从小到大列举，2）当大于当前最优的，立刻停止枚举。这是因为较大的无法提供一个更好的解决方案。3）同样，在枚举的时候，每次只评估比上一个大至少ε的. 虽然找到的解决方案不一定是最优的，但是全局最优条件的差距最大为 。 按照经验设定 ，实验中该算法找到的方案基本上和最优解一致。 运算符聚类计算图上的很多算子并不是计算密集的，这些算子的确切位置对总体执行的时间影响很小。这里开发了另一个DP算法来聚类相邻的算子，从而减少计算图的大小。将K个算子聚类成L层，L是远小于K的。 这个算法的目的是将两种类型的算子合并：1）不会调用大量的计算，但是会增加计算图的长度；2）相邻的算子如果放在不同的设备上可能会产生实质性的通信。定义函数 作为聚类算子时划分到聚类层r时接收的最大数据量的最小值。最优子结构如下 其中， 表示从到的输入数据总量，表示整个计算图的FLOP。确保每个聚类层的FLOP都是在 1 + δ乘以每层平均FLOP之内的同时，也最小化了个层间FLOP的方差，从而达到更加均匀的结构分布。对于通信成本相同的方案，我们选择结构最均匀的一个，同事最小化每层FLOP的方差。通过这个DP算法，我们可以在的时间内算出最优的层聚类。 是算法的超参数，根据经验我们基于设备数量和计算图中计算量大的算子数量选择一个小的。其实这个选择对于最终性能并不会有跟明显的影响。 算法描述Inter-op pass总结输入：计算图G , 集群C {with shape (N,M)}输出：最小的执行延迟 1234567891011121314151617181920212223242526272829303132333435# 计算图前处理1. 序列化模型 (o_1,...,o_k) = Flatten(G)2. 算子层聚类 (l_1,...,l_L) = OperatorClustering(o_1,...,o_K)# 执行intra-op pass，获取不同stage-mesh组合submesh_shapes = {(1,1),(1,2),(1,4),...,(1,M)}∪{(2,M),(3,M),...,(N,M)}for 1&lt;=i&lt;=j&lt;=L :\tstage = (l_i,...l_L)\tfor (n,m) in submesh_shapes: # 初始化所有intra-op阶段的 for s in range(1,L): t_intra(stage, Mesh(n, m), s) = INF for (n_l,m_l),opt in LogicalMeshShapeAndIntraOpOptions(n,m): plan = IntraOpPass(stage, Mesh(n_l,m_l),opt) t_l,mem_stage,mem_act = Profile(plan) for s 满足公式5 # mem_stage + s · mem_act ≤ mem_device . if t_l &lt;= t_intra(stage, Mesh(n, m), s): t_intra(stage, Mesh(n, m), s) = t_lreturn t_l# inter-op pass （DP）T* = INFfor t_max in SortedAndFilter(t_intra, ε):\tif B·t_max &gt;= T*: break\tF(0,L+1,0; t_max) = 0\tfor s in range(1,L): for l in range(L,1): for d in range (1,N*M): Compute F(s,l,d; t_max) 按照公式3\tT*(t_max) = min_s{F(s,0,N·M;t_max)}+(B−1)·t_max\tif T*(t_max) &lt; T*: T* = T*(t_max)return T* 6. Parallelism Orchestration并行编排当Stages，device mesh的分配决定以后，在intra-op pass阶段，Alpa遵循ILP求解器得出的intra-op并行计划，根据分配给它的device mesh编译每个阶段。编译依赖于XLA和GSPMD，并生成每个stage-mesh的并行可执行程序。当需要时，编译会自动插入第四节中提到的通信源语来处理算子内并行所需要的resharding。在inter-op pass阶段，Alpa实现了一个额外的并行编排pass来处理跨mesh的stage间通信，并为算子间并行执行生成静态指令。 跨mesh的Resharding现存的手动系统，就像Megatron-LM，约束所有的pipeline stage都有相同的数据和tensor模型并行度，因此pipeline stage之间的通信通常通过两个等效设备网格的对应设备之间的P2P发送/接收来实现(图6a)。在Alpa中包含两个相邻stage的device mesh可能有不同的mesh shape，并且tensor在两个stage之间的通信可能有着不同的sharding spec(图6b/c)，这里称这种通信模式为cross-mesh resharding，这是一个多对多的多点传送问题。给定tensor在发送和接收方的mesh上的sharding spec， Alpa在两次迭代中生成一个用于解决 corss-mesh sharding 的通信方案。第一次迭代中，计算在源mesh和目标mesh之间tensor tile的对应关系。基于此，生成源设备和目标设备之间的P2P发送/接收源语来实现通信。然后，第二次迭代中来识别目标tensor在其sharding spec中存在复制的机会。在这种情况下tensor只需要在mesh之间传输一次，之后利用目标mesh上利用更高的带宽通过 all-gather来完成交换–这会重写第一次迭代中生成的发送/接收，避免重复通信。称这种方法为 local all-gather跨设备resharding。因为在Alpa的设计中stage之间的通信通常都很小，实验符合设计的期望，后续还会开发更优的跨设备resharding策略。 生成可执行指令最后一步，Alpa生成静态执行指令在集群上启动训练，因为每个stage都有不同的算子集合，并且在放在有不同shape 的mesh上 （与很多APMD 并行训练系统不同），Alpa采用MPMD风格的runtime来安排算子间并行执行–Alpa生成确定的静态执行指令给每一个device mesh。Alpa给算子间并行执行开发了一系列指令，包括stage内tensor的内存分配和释放指令，按照cross-mesh resharding方案在stage间的tensor通信指令，同步指令和计算指令等。。。根据用户选择的pipeline schedule，Alpa使用一个驱动进程提前生成指令，并在执行之前整个指令列表分发给每个worker，避免了运行时的驱动和worker之间协调的开销。 7. 限制和讨论相比于现有的一些手动组合数据、算子、和pipeline并行(比如3D-parallelism [45]和PTD-P[40])，Alpa对inter-op、intra-op并行性的分层视图显著的提高了他们的三个主要灵活性：1）stage可以包含不均匀数量的算子和层；2）stages可能被映射到不同shape的device mesh上；3）在每个stage中，数据和算子并行结构配置对于每个算子来说都是单独定制的。综上，是的Alpa能够整合所有现存的模型并行方法并推广到具有更多异构性的模型结构和集群设置。抛开这些改进，Alpa的优化算法目前还是存在一些限制： 并没有对不同stage的通信成本进行建模，因为cross-stage的通信成本本来就很小。但是实际上在DP或者ILP中对cost进行建模都是可行的，但是会需要枚举更多矢量的intra-op pass和DP状态。 inter-op pass目前有一个超参数：微batch ，目前的公式中并没有被优化，但是可以通过枚举的方式进行搜索。 inter-op pass用静态线性schedule建模pipeline的并行结构，而没有考虑更多动态schedule，例如在不同的deivce上并行计算图中的不同分支 对于overlap计算和通信并没有优化一个最好的方案，Alpa值能处理静态计算图，所有tensor的shape编译时已知。尽管如此，Alpa都是可以将现在一些常见的模型生成出接近最优的执行方案。 附录关于算子聚类的疑惑解释在Alpa论文中的动态规划（DP）公式里，递推关系中取两部分的最大值的原因是为了确保负载均衡和计算效率。这种方法有助于找到一个平衡点，使得每个阶段（或者子任务）的最大开销最小化，从而实现整体优化。 公式回顾公式如下： 解释 ( G(k, r) )：表示将前 (k) 个算子分成 (r) 个阶段时的最小成本。 递推关系： 为什么要取最大值 最大值的含义：在每个划分点 (i)，我们计算两部分的最大值： G(i-1, r-1) ：前 (i-1) 个算子分成 (r-1) 个阶段的最小成本。 ( C(i, k) )：从第 (i) 个算子到第 (k) 个算子的聚类成本。 最大值的目的是为了确保最坏情况下的开销最小化： 在分布式计算中，如果某一个阶段的计算开销特别大，它将成为整个系统的瓶颈，导致整体性能下降。 通过取两部分中的最大值，公式确保即使在最坏情况下（即某一阶段的开销最大），整体方案依然是最优的。这种策略有效地平衡了各个阶段的负载，避免单点过载。 为什么要取最小值 最小值的含义：在所有可能的划分点 (i) 中，选择使得最大值最小的那个划分点。 最小值的目的是找到全局最优解： 动态规划通过递推关系逐步寻找全局最优解。在每一步中，选择最优的划分点，以确保整体开销最小。 通过在所有可能的 (i) 中取最小值，公式确保选择的划分点能够使整体计算开销最小化。 负载均衡与效率优化 负载均衡： 分布式系统中，负载均衡至关重要。均衡的负载分布可以避免某些设备过载，从而提高整体系统的效率。 通过限制每个阶段的FLOP数，并取最大值来衡量最坏情况下的开销，公式确保每个设备的计算任务不会超过其承受能力。 效率优化： 通过动态规划逐步优化，从而找到整体计算开销最小的方案。 在每一步中选择使得当前阶段最优的划分点，确保最终得到的划分方案是全局最优的。 总结在Alpa论文中的DP公式里，取两部分的最大值是为了确保分布式计算的负载均衡和效率优化。这种方法通过考虑最坏情况下的开销，避免了单点过载，从而提升了整体系统的性能和稳定性。最终通过在所有可能的划分点中取最小值，公式实现了全局最优的算子聚类和任务分配方案。"},{"title":"gperf 使用","path":"/undefined/tech/perftools/","content":"安装12345678910111213# 克隆代码git clone https://github.com/gperftools/gperftools.git# 安装依赖库sudo apt-get install libunwind8-dev# 程序编译cd gperftools./autogen.sh./configuremake -j 8# 安装到系统文件夹sudo make install# 刷新动态库文件ldconfig 使用非侵入式顾名思义，不需要在程序中加入任何代码，直接运行即可。需要在CmakeList中加入相关的编译选项 -Wl,--no-as-needed,-lprofiler,--as-needed。 1target_link_libraries(ncnn_test ncnn -Wl,--no-as-needed,-lprofiler,--as-needed) 然后在命令行中使用CPUPROFILE进行执行前的配置，同时执行需要分析的程序。 1CPUPROFILE=./ncnn_test.prof ./ncnn_test 之后，即可生成当前程序的性能分析文件。 侵入式需要在程序中加入相关的代码，然后运行程序。这种方式需要在每个需要分析的函数中加入相关的代码。但是CmakeList中仅需要加入-lprofiler即可。 1target_link_libraries(ncnn_test ncnn -lprofiler) 代码中的相关代码如下： 12345678#include &lt;gperftools/profiler.h&gt;int main() &#123; ProfilerStart(&quot;./ncnn_test.prof&quot;); // ... ProfilerStop(); return 0;&#125; 之后，执行程序即可得到性能分析文件。 性能分析需要使用pprof进行文件信息解析 123456789&gt; pprof --text ./ncnn_test ncnn_test.profUsing local file ./ncnn_test.Using local file test.prof.addr2line: DWARF error: section .debug_info is larger than its filesize! (0x93f189 vs 0x530f70)Total: 8 samples 8 100.0% 100.0% 8 100.0% omp_get_num_procs 0 0.0% 100.0% 8 100.0% clone 0 0.0% 100.0% 8 100.0% omp_in_final 0 0.0% 100.0% 8 100.0% start_thread 其输出形式有--text控制，还可以使用其他形式进行展示，如--pdf生成pdf文件，--web生成网页。 注意事项当你的程序很短，或者运行速度很快时，使用--text形式进行性能分析时，可能无法得到有效的性能分析信息。只会得到如下信息 12Using local file ./ncnn_test.Using local file test.prof. 此时，可以通过设置CPUPROFILE_FREQUENCY为更大的数值，来增加性能分析的频率。从而避免出现上述情况。采样频率越高，性能分析的准确度越高。 1export CPUPROFILE_FREQUENCY=9999999 或者 12# RECOMMENDCPUPROFILE_FREQUENCY=9999999 ./ncnn_test"},{"title":"shell脚本总结","path":"/undefined/tech/shell/","content":"1. 获取特定的目录提取特定目录现有一些文件路径，例如： “&#x2F;home&#x2F;user&#x2F;project&#x2F;src&#x2F;main&#x2F;java&#x2F;com&#x2F;example&#x2F;demo&#x2F;DemoApplication.java” “&#x2F;home&#x2F;user&#x2F;project&#x2F;src&#x2F;main&#x2F;cpp&#x2F;com&#x2F;example&#x2F;demo&#x2F;DemoApplicationTests.cpp” “&#x2F;home&#x2F;user&#x2F;project&#x2F;src&#x2F;main&#x2F;c&#x2F;com&#x2F;example&#x2F;demo&#x2F;controller&#x2F;DemoController.c” 需要提取出java,cpp,c这个子目录。 123456#!/bin/bashpath=&quot;/home/user/project/src/main/java/com/example/demo/DemoApplication.java&quot;target_dir=&quot;$&#123;path#*/main/&#125;&quot; # 移除“main/”及之前的部分desired_dir=&quot;$&#123;target_dir%%/*&#125;&quot; # 取得剩余字符串中的第一个目录名echo &quot;$desired_dir&quot;&gt; java 可以用于处理测试报告，提取出java,cpp,c这三个子目录。 2. 定制log颜色编译脚本执行完毕以后，输出的log信息默认是和终端颜色一致的，无法快速帮助我们获取特定信息，那么就可以通过颜色来区分不同类型的log信息。 12345678#!/bin/bash# ANSI转义码颜色定义RED=&#x27;\\033[0;31m&#x27;GREEN=&#x27;\\033[0;32m&#x27;NC=&#x27;\\033[0m&#x27; # No Colorecho -e &quot;$&#123;RED&#125; not convert success !$&#123;NC&#125;&quot;"},{"title":"一些好用的工具","path":"/undefined/tech/utils/","content":"命令行 hyperfine 命令行基准测试工具 VSCode 插件 crayons 高亮选中word Markdown借助html标签来分栏 1. 对计算图进行拓扑排序 2. 找出最小的volum 1. asd 2. 啊试点单位 3. aa 梯子https://www.racknerd.com。每年只要10美元，3T的流量，地点选在洛杉矶，200ms时延，性价比超高."},{"title":"loop optimize","path":"/undefined/tech/loop_opt/","content":"背景问题如果要进行1-10000的累乘，该如何实现？最常见的做法就是使用for循环，那么for循环是否能进行优化呢，提升到极致的性能会是什么样子呢。 实现为了避免编译器过度，我们使用Debug模式进行性能分析。 方法一 12345int sum = 1;for (int i = 1; i &lt;= 10000; i++)&#123;\tsum *= i;&#125; times 0 1 2 3 4 5 6 7 8 9 duration (ms) 17.929 17.434 17.804 17.816 16.706 17.201 18.129 17.049 17.368 17.823 这是最基础的for循环执行10次的性能。接下来我们开始对其进行优化。 方法二 方法一中每次计算需要进行一次循环判断，总的指令数量为10000次循环控制指令+10000次计算指令。通常对for循环进行性能优化的办法是循环展开，即在一次循环中进行多次计算，减少循环控制相关指令的数量，使有效的计算指令的占比提升。因此在 方法二中我们减少循环指令的占比，在一次循环中进行4次计算。 12345678int sum = 1;for(int i = 1; i &lt;= 10000; i+=4)&#123;\tsum *= i;\tsum *= i+1;\tsum *= i+2;\tsum *= i+3;&#125; times 0 1 2 3 4 5 6 7 8 9 duration (ms) 16.979 17.056 16.99 17.047 17.025 17.04 16.979 17.032 17.017 17.013 这次的循环指令数量为2500次，10000次计算。可以看到，这次的for循环性能相比于 方法一有了一定的提升，但是总体来说提升并不大，我们先不进行过度深入的理解，就当作循环指令的开销比较小，所以单纯减少循环指令对性能的提升并不明显。 这里首先理解一下循环展开是为了什么，单纯的减少本身就开销比较小的控制指令，以此来优化性能？也许并不是！ 这里就需要了解下现在处理器的一个特性，就是能够在一个始终周期内执行多条指令，本文中不做详细展开，可以先通过这篇文章了解一下这一特性 &lt;&lt;指令流水线&gt;&gt;. 方法三 现在我们来实现一个可以利用处理器指令流水线特性的for循环代码。 123456789int sum0=1, sum1=1, sum2=1, sum3=1;for (int i = 1; i &lt;= 10000; i+=4)&#123;\tsum0 *= i;\tsum1 *= i+1;\tsum2 *= i+2;\tsum3 *= i+3;&#125;return sum0*sum1*sum2*sum3; times 0 1 2 3 4 5 6 7 8 9 duration (ms) 5.038 5.073 5.755 4.924 5.231 5.56 5.59 4.717 5.782 5.017 可以看到，利用了指令流水线后，程序的性能是大幅度提升的，并且这是没有进行编译器优化的代码。 总结所有软件都是在硬件的基础上运行的，所以，如果想要写出非常高效的代码，那么就必须得熟悉或者说对计算机体系结构有所了解，否则就没法使代码达到极致的性能。 同样利用硬件特性的程序，在多线程程序中也有体现，可以看看下一篇 多线程程序性能优化."},{"title":"TITLE","path":"/template.html","content":"Hello，This is a blog template.Write a new blog base on this file. Curio","tags":[null],"categories":[null]},{"title":"全局函数","path":"/Tech/after-main-function.html","content":"main函数结束后还能执行其他函数？在 main 函数结束后执行的函数或代码通常被称为”终止处理程序”（termination handler）或”退出处理程序”（exit handler）。这个概念在不同的上下文中可能有不同的具体名称： 退出函数（Exit function）当使用 std::atexit() 注册时，这个函数通常被称为”退出函数”。 终止处理程序（Termination handler）特别是在使用 std::set_terminate() 设置处理未捕获异常的函数时。 析构函数（Destructor）当使用全局对象的析构函数来执行清理操作时。 清理函数（Cleanup function）这是一个通用术语，经常用来描述这类函数的功能。 信号处理函数（Signal handler）当使用信号处理机制（如处理 SIGINT）来执行程序结束时的操作。 终结器（Finalizer）这个术语有时用于描述在程序结束时执行最终清理的函数。 钩子函数（Hook function）在某些上下文中，这种在特定时刻（如程序退出时）自动执行的函数被称为”钩子”。 atexit 处理程序（atexit handler）特指通过 std::atexit() 注册的函数。 程序终止函数（Program termination function）一个更正式的术语，描述在程序终止时执行的函数。 延迟加载函数（Deferred function）在一些编程语言和环境中，这种在程序结束时执行的函数被称为”延迟”执行的函数。 尽管没有一个统一的官方术语，但在 C++ 编程中，”退出函数”（exit function）或”清理函数”（cleanup function）是最常用的术语。选择使用哪个术语通常取决于具体的上下文和个人或团队的偏好。在技术文档或代码注释中，使用这些术语中的任何一个都是可以的，只要能清楚地表达其功能即可。 示例以下为几种实现不同的方法介绍。 析构函数 destructor attribute这是 GCC 的一个扩展语法，用于向编译器传达特殊的信息或指令。 这个属性告诉编译器，被标记的函数应该在程序结束时自动调用。它的行为类似于 C++ 全局对象的析构函数。 最简单易用的方式，但是不属于标准特性，并非所有编译器都支持，代码移植性可能会降低。 当程序正常终止（main 函数返回或调用 exit()）时，标记为 destructor 的函数会被自动调用，不需要在main函数中做任何修改。 这些函数的调用顺序与它们的定义顺序相反。 多个 destructor 函数可以共存。 1234567891011121314#include &lt;iostream&gt;void __attribute__((destructor)) cleanup1() { std::cout &lt;&lt; \"Cleanup 1 executed\" &lt;&lt; std::endl;}void __attribute__((destructor)) cleanup2() { std::cout &lt;&lt; \"Cleanup 2 executed\" &lt;&lt; std::endl;}int main() { std::cout &lt;&lt; \"Main function executing\" &lt;&lt; std::endl; return 0;} 输出 123Main function executingCleanup 2 executedCleanup 1 executed 扩展 constructor 该语法与destructor相反，用于main函数执行之前调用。 信号处理函数用于处理程序信号异常时的善后工作，或者其他处理。 123456789101112131415#include &lt;csignal&gt;void signalHandler(int signum) { std::cout &lt;&lt; \"Interrupt signal (\" &lt;&lt; signum &lt;&lt; \") received. \"; // your code exit(signum);}int main(int argc, char *argv[]){ signal(SIGINT, signalHandler); //... return 0;} 退出函数（Exit function）使用 std::atexit() 注册时，这个函数通常被称为”退出函数”，也是比较简单易用的方式。 12345678910111213141516#include &lt;iostream&gt;#include &lt;cstdlib&gt;void cleanup() { std::cout &lt;&lt; \"Performing cleanup operations...\" &lt;&lt; std::endl; // clean up.}int main() { // register exit func std::atexit(cleanup); // main code return 0;} 钩子函数（Hook Function） TODO","tags":[null],"categories":[null]},{"title":"About Me","path":"/about/index.html","content":"我年近三十岁，是一个普通的程序员，有着普通的生活，有着普通的梦想。"},{"title":"Wiki使用指南","path":"/wiki/index.html","content":"Wiki界面说明首先，这里会作为个人知识的记录，内容不一定准确，阅读需要有一定辨识度。"}]