[{"title":"rvv 相关参数说明","path":"/tech/RVV/RVV_Args/","content":"rvv 相关配置参数说明前言引言 今天面试中遇到了一些关于rvv的参数相关的问题，本身比较一知半解，所以还是挺尴尬的，说自己会写rvv。所以这里对于常用的一些参数进行记录。 基础配置参数按照在写rvvkernel中遇到的来说明，主要有一下几个： vl 向量长度，通常是通过__riscv_vsetvl_e32m1来自动获取当前一个向量的元素数 vlen 和硬件相关的单个向量寄存器宽度，128、256、512 elen 硬件支持的最大元素宽度：32、64等 sew 根据dtype来确定，fp32: sew32 ；fp16：sew16 lmul 通过mX的x来确定具体使用的向量寄存器数量 vlmax - 根据前面三者计算得到：$vlmax （vlen*lmul） seq$， RVV内在函数核心标记总览 标记字母 在函数名中的位置 全称含义 功能描述 示例 v 前缀 Vector 标识这是一个向量操作。 vadd, vle32 l s v之后 Load Store 表示内存操作是加载（读内存）还是存储（写内存）。 vle32 (加载), vse32 (存储) s l和e之间 Strided 表示使用显式步长的非连续内存访问。 **vlse32**, vsse32 x l和e之间 Indexed 表示使用向量寄存器中的索引进行散状（Gather）或散开（Scatter）访问。 vlxei32, vsxei32 e l/s之后 Element width 指示内存中单个数据元素的宽度（位数）。紧随其后的数字即为宽度。 vle**32** (32位元素) m e{width}之后 Machine register LMUL 指示操作使用的向量寄存器组（LMUL）。数字为分组因子。 vadd.**v**v**i32m1** (使用LMUL1) u f e{width}之后 Unsigned Float 指示操作数的数据类型是无符号整数或浮点数。 vle32**u** (无符号加载), vle32**f** (浮点加载) v x i 运算操作名之后 Operand Source 指示操作数的来源：v(向量), x(标量), i(立即数)。 vadd.**vv** (向量+向量), vadd.**vx** (向量+标量), vadd.**vi** (向量+立即数) m 操作数来源之后 Mask 表示这是一个掩码操作，使用v0作为谓词掩码。 vadd.vv **m** (掩码加法) t 指令后缀 Tied Operand 表示目的操作数与第一个源操作数是同一个寄存器（融合操作数）。常用于归约等操作。 vredsum.vs **t** 高级应用1. 内存操作三剑客这三种加载指令覆盖了所有主流的内存访问模式，是编程的核心： vle32： 连续访问。效率最高，是基础。 vlse32： 固定步长访问。用于处理矩阵行列、结构体数组。 vlxei32： 索引访问（Gather）。功能最强大也最灵活，允许通过一个索引向量从任意地址收集数据，适合处理完全不规则的数据（如哈希表、稀疏矩阵的非零元）。2. 掩码操作 (m)这是实现条件向量化和循环尾部处理的关键。例如，vmadd.vv 表示 “在掩码为1的通道上，执行乘法累加”，这可以避免对数组边界外或不符合条件的元素进行操作。 3. 归约与融合操作数 (t)像 vredsum.vs 这样的归约操作，会将一个向量“压缩”成一个标量。标记 t 表示其目的标量寄存器与一个源向量寄存器是同一个物理寄存器（被“绑定”了），这种设计优化了寄存器文件的压力。","tags":["RVV"],"categories":["optimize"]},{"title":"UB行为总结","path":"/tech/Code/UB/","content":"UBSTL相关struct vec_cmp\tbool operator()(vectorint a, vectorint b) return a[0] = b[0]; // 使用 = 违反了严格弱序要求，应该使用``.\t; 在C++ STL中，比较函数必须满足严格弱序，即对于相等的元素，comp(a, a) 必须返回 false。使用 = 会导致相等时返回 true，这违反了要求。","tags":["cpp"],"categories":["commen sense"]},{"title":"cpp的基础知识","path":"/tech/Code/cpp/","content":"基础三大特性 封装以class为基础单元，将属性和方法捆绑，对外部隐藏内部细节，通过访问修饰符来控制权限：private、protected、public private只能对当前类起作用，不能被子类访问，外部无法访问 protected可以被子类访问，但是外部无法访问 public可以被内部外部随意访问 继承class默认是按照private继承，struct是按照public继承 private继承子类继承后所有非私有方法成员（除构造、析构、重载、友元）都变成了私有 protected继承子类继承后所有非私有方法成员都变成了保护 public继承子类继承后所有非私有方法成员都保持原来的权限 多态编程中同一个接口或者操作在处境不同的情况下有不同的行为 编译时多态 函数重载int add(int a, int b) return a + b; float add(float a, float b) return a + b; 在C中无法进行重载哦，因为C的函数名在目标文件(.o)中的符号名就是原函数名，而cpp会有name mangling机制，将add函数的符号编码成函数名+参数类型，例如addi、addf、addd 运算符重载 模板模板函数可以接受不同类型的输入 运行时多态 虚函数通过基类的指针或者引用在运行时自动调用实际对象所属的派生类的函数版本，达到一个接口多种实现的功能， 继承 继承多态部分特殊语法final当类或者函数后加上final，则表明该类或者该函数不能被继承或者重载 增强代码可读性，更加明确表明代码含义 class Base final public: virtual void foo() final // 一些操作 virtual void bar() // 一些操作 ;class Derived : public Base // 编译错误：Base 类不能被继承public: void foo() override // 编译错误：foo 函数不能被重写 // 一些操作 void bar() override // 一些操作 ; 左值、右值、引用 左值： 变量，可以取地址 右值： 临时的对象，无法取地址， 10,std::string(“tmp”), func() 如果func()返回非引用，则返回值是右值 右值引用： 使用来表示，只能绑定到右值，为了支持移动语义和完美转发 移动语义：允许将资源从一个对象转移到另一个对象，而不需要进行复制 MyString str1(Hello);MyString str2 = std::move(str1); 你想表达 应该写 值类别 持久对象，可修改 T 左值引用 持久对象，只读 const T 万能引用（可绑右值） 临时对象，资源可偷 T 右值引用 转发任意值（模板） T + std::forward 万能引用 强制转为可移动状态 std::move(x) 产生 xvalue push_back emplace_back 特性 push_back emplace_back 参数 对象本身或隐式可转换 构造函数的参数包 构造位置 栈上临时对象，再移入 直接在 vector 内存构造 临时对象 有（除非传右值） 无 适用场景 已有对象，强调类型安全 新构造对象，性能敏感","tags":["cpp"],"categories":["commen sense"]},{"title":"单调栈","path":"/tech/Code/Monotonic Stack/","content":"单调栈（Monotonic Stack）是算法中一种非常实用的数据结构技巧，核心思想是维护一个栈，使得栈中元素始终保持单调递增或单调递减的顺序。它常用于高效解决“下一个更大更小元素”类问题，时间复杂度通常为 O(n)。 什么是单调栈？从栈底到栈顶都是单调变化的元素，单调变化指的是单调(非)递增减(包含相等的情况)。递增用于解决寻找第一个更小的元素，递减反之。 为什么用单调栈？通常遇到需要数组中两个元素进行比较，找到下一个更大或者更小的元素这类问题的时候，遍历两边能够得到答案，因为每个元素至少需要与其他元素进行比较后才知道谁是大小王！ O(n^2)的复杂度。那么对于要求下一个更大的元素是什么的时候呢？乍一看，每个元素只需要与后面的元素进行比较就行了，但是复杂度也是O(n^2)，但是如果我们使用单调栈来做的话，时间复杂度就可以降低到O(n)了。为什么可以降低到O(n) 呢？因为在单调栈方法下，每个元素只会遍历一次，而且每个元素也最多只会入栈一次、出栈一次，那综合下来也就是只有O(n)了。 用简单的逻辑来描述就是：背景：求下一个更大元素0. 每个元素A都与队伍最后一个元素B进行比较 如果队伍是空的，A去排队，然后让下一个元素来； 如果B比A小，让B元素记住A，然后B退出当前队伍，因为他已经得到他的目标元素了； 如果B元素比A大，去一边排队； 最后队伍里剩下一个没有比他更大的元素，让他记住0（或者是自己） 用人话来说：背景：教室里面的学生随意排队比较身高，让每个学生记住他后面第一个比自己高的学生。0. 第一个学生出来后站旁边排队 下一个学生出来和排队的学生比身高 如果新出来的比排队的第一个人高，让排队的第一个人记住新出来的人的名字，然后离开，其他排队的同学向前一步走，队伍最前面的人继续比身高 直到队伍最前面的人比新出来的人高，这时候让新出来的人去队伍最前面排队，然后教室里出来下一个学生 最后队伍剩下一个人，让他记住自己好了。 场景1寻找每个元素右边第一个比它大的元素（Next Greater Element） 暴力求解 for i in range(n): for j in range(i+1, n): if a[j] a[i]: result[i] = a[j] break 单调栈求解 stack = []for i in range(n): while stack and a[i] a[stack[-1]]: idx = stack.pop() result[idx] = a[i] stack.append(i) 场景2leetcode题目： 给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。示例 1:输入: temperatures = [73,74,75,71,69,72,76,73]输出: [1,1,4,2,1,1,0,0]示例 2:输入: temperatures = [30,40,50,60]输出: [1,1,1,0]示例 3:输入: temperatures = [30,60,90]输出: [1,1,0]提示： 1 = temperatures.length = 105 30 = temperatures[i] = 100 代码实现： class Solution public: vectorint dailyTemperatures(vectorint temperatures) vectorint res(temperatures.size(), 0); stackint st; for(int i = 0; i temperatures.size(); i++) while(!st.empty() temperatures[i] temperatures[st.top()]) auto index = st.top(); st.pop(); res[index] = i - index; st.push(i); return res; ; 场景3柱状图中最大的矩形实现代码： class Solution public: int largestRectangleArea(vectorint heights) stackint st; int res = 0; int length = heights.size(); for(int i = 0; i length; i++) while(!st.empty() heights[i] heights[st.top()]) int index_of_high = st.top(); st.pop(); int left = st.empty() ? -1 : st.top(); res = std::max(res, (i - left -1)*heights[index_of_high]); st.push(i); // 处理剩余的单调元素 1,2,3 index 1,4,5 while(!st.empty()) int now = st.top(); st.pop(); int before = st.empty() ? -1 : st.top(); res = std::max(res, heights[now] * (length - before - 1)); return res; ; 这个题目的暴力求解就是获取每个元素在队列中比他小的左边界和右边界，然后再遍历求出每个矩形的大小。 上一种解法最后会剩下一个单调的元素序列需要进行额外一次迭代处理，那如果扩充一下原来的数据添加两个小于所有元素的元素进去，也就是说会打断元素们构建矩形的元素(0)，这样最后就不会有剩余的单调元素了。 stackint st;vectorint expanded_heights(heights.size()+2, 0);// std::copy(heights.begin(), heights.end(), expanded_heights.begin()+1);std::memcpy(expanded_heights.data()+1, heights.data(), sizeof(int)*heights.size());// for(int i = 1; i = heights.size(); i++) expanded_heights[i] = heights[i-1];int res =0;for(int i = 0; i expanded_heights.size(); i++)\twhile(!st.empty() expanded_heights[i] expanded_heights[st.top()]) int index = st.top(); st.pop(); int left_index = st.empty() ? -1 : st.top(); res = std::max(res, (i - left_index -1) * expanded_heights[index]); st.push(i);return res; 场景4最大矩形在二维矩阵中找只包含1的最大矩形，可以通过一定的分析发现，这其实就是在场景3的基础上额外多增加了一维的遍历，如下图所示。这样来理解代码就非常容易理解了，在场景3的代码外面加一个将当前矩阵处理为右侧直方图的步骤即可，然后循环内容调用场景3的代码来完成。 class Solution public: int largestRectangleArea(vectorint heights) stackint st; int res = 0; int length = heights.size(); for(int i = 0; i length; i++) while(!st.empty() heights[i] heights[st.top()]) int index_of_high = st.top(); st.pop(); int left = st.empty() ? -1 : st.top(); res = std::max(res, (i - left -1)*heights[index_of_high]); st.push(i); // 处理剩余的单调元素 1,2,3 index 1,4,5 while(!st.empty()) int now = st.top(); st.pop(); int before = st.empty() ? -1 : st.top(); res = std::max(res, heights[now] * (length - before - 1)); return res; int maximalRectangle(vectorvectorchar matrix) vectorint raw(matrix[0].size(), 0); int res = 0; for(int i = 0; i matrix.size(); i++) for(int j = 0; j raw.size(); j++) int num = matrix[i][j] - 0; raw[j] = num == 0 ? 0 : raw[j]+num; res = std::max(res, largestRectangleArea(raw)); return res; ;","tags":["栈"],"categories":["Data Struct && Algorithm","stack"]},{"title":"健身记录","path":"/life/fit/","content":"讲在前面健身的目的不在于把肌肉线条锻炼的多么的好看，帅气，而是主打一个为了身体健康。今年已经30岁了，逐渐感觉身体机能在下降，并不喜欢这种感觉。同时我也不喜欢大块的肌肉，威猛有余灵活不足，还是更喜欢灵活的肌肉。最主要的目的： 身体健康 规律的生活习惯 健康饮食习惯（回到20年的时候，对于吃没什么欲望） 目前遇到的缺陷 肱二头肌无力5公斤的哑铃做弯举都很吃力，或者说耐力很差，第三组的时候动作变形，会依靠肩膀的力量来带动小臂，目前还不知道具体原因，待后续补充。 小臂力量不足、核心力量下降（以前仰卧起坐100+的 T_T）举腿时明显感觉手抓不住器械，同时身体大幅度会晃动（这个应该是身体核心力量变差了）。 目前的一些进展囚徒健身6艺进度 O1 完成基础训练，提高身体力量和灵活性 正常 30% KR1 俯卧撑 - 阶段3 基础力量训练，锻炼胸肌、三头肌和核心10个阶段：✅ 墙壁俯卧撑✅ 上斜俯卧撑⏳ 膝盖俯卧撑⏳ 标准俯卧撑⏳ 窄距俯卧撑⏳ 偏重俯卧撑⏳ 单臂半俯卧撑⏳ 杠杆俯卧撑⏳ 单臂俯卧撑⏳ 单臂击掌俯卧撑 正常 28% KR2 深蹲 - 阶段5 下肢力量训练，锻炼大腿、臀部和核心10个阶段：✅ 靠墙深蹲✅ 辅助深蹲✅ 箱式深蹲✅ 半深蹲⏳ 标准深蹲⏳ 窄距深蹲⏳ 偏重深蹲⏳ 单腿辅助深蹲⏳ 单腿深蹲⏳ 单腿跳箱深蹲 正常 45% KR3 引体向上 - 阶段2 （暂时缺少器械） 上肢力量训练，锻炼背部、肱二头肌10个阶段：✅ 水平引体向上⏳ 低杠引体向上⏳ 助力引体向上⏳ 标准引体向上⏳ 窄距引体向上⏳ 偏重引体向上⏳ 单臂半引体向上⏳ 单臂辅助引体向上⏳ 单臂引体向上⏳ 单臂击掌引体向上 风险 20% KR4 举腿 - 阶段3 核心力量训练，锻炼腹肌和下背部10个阶段：✅ 坐姿屈膝✅ 悬垂屈膝⏳ 悬垂举腿⏳ 悬垂直腿举⏳ 悬垂侧举腿⏳ 悬垂画圈⏳ 悬垂V字举腿⏳ 悬垂铁板桥⏳ 悬垂单腿举⏳ 悬垂单腿V字举 正常 25% KR5 桥 - 阶段4 背部力量训练，改善体态和腰椎健康10个阶段：✅ 仰卧屈膝✅ 半桥✅ 标准桥⏳ 前桥⏳ 单腿桥⏳ 窄距桥⏳ 偏重桥⏳ 单臂桥⏳ 单臂单腿桥⏳ 铁板桥 正常 35% KR6 倒立撑 - 阶段1 上肢支撑训练，锻炼肩带和核心10个阶段：✅ 靠墙倒立⏳ 靠墙半倒立撑⏳ 靠墙倒立撑⏳ 自由倒立⏳ 自由半倒立撑⏳ 自由倒立撑⏳ 窄距倒立撑⏳ 偏重倒立撑⏳ 单臂辅助倒立撑⏳ 单臂倒立撑 风险 0%"},{"title":"VPS 设置","path":"/tech/vps/","content":"搭建1. 购买VPS服务https://my.racknerd.com/ ，主打便宜，能用。 2. 登录服务器 ssh 网站提供的vnc 3. 开始安装基础服务 UbuntuDebian apt update apt upgrade -yapt install -y curl wget vim ufwapt install -y shadowsocks-libev CentOSAlmaLinux yum update -yyum install -y curl wget vim firewalldyum install -y epel-releaseyum install -y shadowsocks-libev 4. 配置文件vim /etc/shadowsocks-libev/config.json server: 0.0.0.0, server_port: 8388, local_port: 1080, password: your_password, //修改密码即可 timeout: 300, method: aes-256-gcm 5. 启动服务systemctl start shadowsocks-libevsystemctl enable shadowsocks-libev 6. 配置防火墙 DebianUbuntu：使用UFW开放端口： ufw allow 8388ufw enable CentOSAlmaLinux：使用Firewalld： firewall-cmd --permanent --add-port=8388/tcpfirewall-cmd --permanent --add-port=8388/udpfirewall-cmd --reload 访问加速1 开启TCP Fast OpenTCP Fast Open (TFO) 是一种 TCP 优化技术,可以显著提高 Shadowsocks 的连接速度和传输效率。在服务端和客户端都开启 TFO 后,可以减少握手次数,降低延迟。 CentOSRHELecho “net.ipv4.tcp_fastopen = 3” /etc/sysctl.conf sysctl -p UbuntuDebianecho “net.ipv4.tcp_fastopen = 3” /etc/sysctl.d/10-tcp-fastopen.conf sysctl -p /etc/sysctl.d/10-tcp-fastopen.conf 2 使用BBR加速BBR (Bottleneck Bandwidth and Round-trip propagation time) 是 Google 开发的一种 TCP 拥塞控制算法,可以显著提高网络吞吐量和减少延迟。在 Shadowsocks 服务器上启用 BBR 可以大幅提升网络性能。 CentOSRHELecho “net.core.default_qdisc=fq” /etc/sysctl.conf echo “net.ipv4.tcp_congestion_control=bbr” /etc/sysctl.conf sysctl -p UbuntuDebianecho “net.core.default_qdisc=fq” /etc/sysctl.d/bbr.conf echo “net.ipv4.tcp_congestion_control=bbr” /etc/sysctl.d/bbr.conf sysctl -p /etc/sysctl.d/bbr.conf 服务器开机自启动设置开机自动启动要设置Shadowsocks-libev在系统启动时自动启动，您可以按照以下步骤进行操作： 创建一个Systemd服务单元配置文件： sudo nano /etc/systemd/system/shadowsocks-libev.service 在打开的文件中，输入以下内容： [Unit]Description=Shadowsocks-libev ServerAfter=network.target[Service]ExecStart=/usr/bin/ss-server -c /etc/shadowsocks-libev/config.jsonRestart=on-failureUser=nobodyCapabilityBoundingSet=CAP_NET_BIND_SERVICEAmbientCapabilities=CAP_NET_BIND_SERVICE[Install]WantedBy=multi-user.target 这里假设Shadowsocks-libev的可执行文件路径是 /usr/bin/ss-server，配置文件路径是 /etc/shadowsocks-libev/config.json。如果实际路径不同，请相应地修改上述命令。 保存并关闭文件 (Ctrl + X，然后输入 Y 并按下 Enter)。 启用并启动Shadowsocks-libev服务： sudo systemctl enable shadowsocks-libevsudo systemctl start shadowsocks-libev 现在，Shadowsocks-libev将在系统启动时自动启动，并以指定的配置运行。您可以使用以下命令验证服务是否正在运行： sudo systemctl status shadowsocks-libev 该命令将显示服务的状态信息，包括是否处于活动状态。 参考连接 https://liujie.io/archives/shi-yong-shadowsockets-libev-jin-xing-mo-fa-fang-wen","tags":["VPS"],"categories":["Tools","vpn"]},{"title":"「LLM」MOE编译器优化","path":"/tech/LLM/03_MOE/","content":"HF的Qwen3实现 Qwen3的MOE默认版本 expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)# Loop over all available experts in the model and perform the computation on each expertfor expert_idx in range(self.num_experts):\texpert_layer = self.experts[expert_idx]\tidx, top_x = torch.where(expert_mask[expert_idx])\t# Index the correct hidden states and compute the expert hidden state for\t# the current expert. We need to make sure to multiply the output hidden\t# states by `routing_weights` on the corresponding tokens (top-1 and top-2)\tcurrent_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\tcurrent_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\t# However `index_add_` only support torch tensors for indexing so well use\t# the `top_x` tensor here.\tfinal_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)return final_hidden_states, router_logits 这个版本中会让所有的专家都进行计算，但是expert_mask中可能当前专家并没有被分配token，所以torch.where这里会返回空的两个tensor，后续计算中都是空的tensor在进行，虽然没有实际的有效计算，但是仍然会调用expertLayer，存在大量开销(为什么是大量开销？topk不会和专家数一样大么？MOE的核心概念就是稀疏激活专家！所以不存在topK和num_expert 接近的情况) Qwen3的MOE优化实现版本 expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)# Loop over all available experts in the model and perform the computation on each expertexpert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()for expert_idx in expert_hitted:\texpert_layer = self.experts[expert_idx]\tidx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\t# Index the correct hidden states and compute the expert hidden state for\t# the current expert. We need to make sure to multiply the output hidden\t# states by `routing_weights` on the corresponding tokens (top-1 and top-2)\tcurrent_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\tcurrent_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\t# However `index_add_` only support torch tensors for indexing so well use\t# the `top_x` tensor here.\tfinal_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)return final_hidden_states, router_logits 优化版本中充分利用了MoE的稀疏特性，只调用活跃的专家来参与后续流程的计算，大大减少了空调用Layer的情况。 但是，这种通过Python高级索引来取数进行计算的方式，在实际部署的时候存在一个很大的问题。编译器无法优化！或者说很难优化！ 这里就需要一种balance，计算量和非连续访存之间的平衡。目前还没有什么特别的想法 MOE的具体实现与MLP的区别总体来说MOE中是包含了大量的专家的，而每个专家进行计算都是使用MLP来完成，所以可以说MOE是对MLP的封装(当然，部分模型中是既有MOE部分也有MLP部分)。为什么会进行这样的封装呢？ MLP可以理解为一个专家对所有的tokens进行计算，得到最终的结果，相对来说模型的泛化性相比于MOE来说较低(可以理解为MOE每个专家负责不同领域的内容，而MLP是一个专家精通各个领域，当然这个行为并不可解释，主观上的臆想)。MOE相较于MLP的一个突出的特点就在这里，与此而来的一个特点就是专家的稀疏性，每次进行decode都只有部分专家被激活来进行计算。 MOE中相比于MLP就会多一个权重来进行专家任务划分gate ，整体的计算流程如下： 通过gate来计算一个路由权重 router_weights，后续通过对router_weights进行概率化得到哪些专家需要对哪些token进行计算。 将专家与他需要处理的token进行计算，然后根据专家对每个Token最终结果的权重(也就是这个专家对这个token的话语权)再次计算，得到这个专家对于这些token的结果。 汇总所有专家对他负责的Token的结果，得到MOE的最终结果。"},{"title":"「LLM」GLLM框架PP策略","path":"/tech/LLM/04_GLLM/","content":"什么是GLLM？ Global Balanced Pipeline Parallelism System for Distributed LLM Serving with Token Throttling Github Repo： https://github.com/gty111/GLLM 如何实现pipeline并行worker是什么？ 每个物理设备，负责推理的具体行为 worker之间如何分工？首先需要了解worker的rank是如何确定的： 总数rank ： PP_size x TP_size rank 每个worker的有三个rank值(这里以pp_size 4 , tp_size 2举例)： rank： [0,1,2,3,4,5,6,7] pp_rank rank tp_size ： [0, 0, 1, 1, 2, 2, 3, 3] tp_rank rank % tp_size ： [0, 1, 0, 1, 0, 1, 0, 1] def run_worker(worker: Worker): if worker.rank == 0: # 全局rank 0 worker.run_driver() # 驱动节点，负责调度 elif worker.pp_rank == 0: # Pipeline第一阶段 worker.run_first_tp() # 处理输入数据 else: # 其他Pipeline阶段 worker.run_other() # 处理中间数据 worker职责 rank0的worker作为驱动节点，负责很多工作： 系统初始化 整体pipeline控制 check_abort_seqs # 1. 检查中止请求 recv_ipc_package # 2. 接收新请求 recv_next_tokens # 3. 接收生成的tokens schedule_forward # 4. 调度和前向推理 process_output # 5. 处理输出结果 pp_rank0 的worker"},{"title":"Mac制作U盘启动镜像","path":"/tech/Mac制作U盘启动镜像/","content":"1. 插入U盘执行diskutil list，可以看到当前U盘信息 diskutil list /dev/disk4 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *31.9 GB disk4 1: Linux 512 B disk4s3 2: Windows_FAT_32 K230_APP 268.4 MB disk4s4 (free space) 31.5 GB - 2. 取消挂载diskutil unmountDisk /dev/disk4 3. 使用dd进行烧录 sudo dd if=../ubuntu-24.04.2-desktop-amd64.iso of=/dev/rdisk4 bs=1m6049+1 records in6049+1 records out6343219200 bytes transferred in 302.613571 secs (20961450 bytes/sec) bsn 代表同时设置输入输出的块大小，n 代表字节数，默认为 512，可以使用 bkmg 等字母后缀代表不同的单位 可以看到of 使用的是devrdisk4 而不是devdisk4，区别如下: 特性 /dev/disk /dev/rdisk 访问方式 缓冲访问 原始访问，绕过文件系统缓存 性能 较低 较高 适用场景 日常使用，如文件系统操作 低级操作，如磁盘映像写入 数据处理 数据经过文件系统缓存机制 数据直接与物理磁盘交互 使用频率 更常用于挂载文件系统 在需要直接操作磁盘时使用 4. ejected U盘 diskutil eject /dev/disk4 Disk /dev/disk4 ejected 用去吧你就，非常简单","tags":["MacOS"],"categories":["MacOS","dd"]},{"title":"「MLIR-1」基于torch-mlir来认识编译器","path":"/tech/MLIR_src/MLIR_1/","content":"编译torch-mlirLLVM_INSTALL_DIR=/Users/curio/workspace/llvm_installcmake -GNinja -Bbuild \\ -DCMAKE_BUILD_TYPE=RelWithDebInfo \\ -DLLVM_ENABLE_ASSERTIONS=ON \\ -DPython3_FIND_VIRTUALENV=ONLY \\ -DPython_FIND_VIRTUALENV=ONLY \\ -DMLIR_ENABLE_BINDINGS_PYTHON=ON \\ -DLLVM_TARGETS_TO_BUILD=host \\ -DCMAKE_C_COMPILER=clang \\ -DCMAKE_CXX_COMPILER=clang++ \\ -DCMAKE_C_COMPILER_LAUNCHER=ccache \\ -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\ -DLLVM_USE_LINKER=lld \\ -DMLIR_DIR=$LLVM_INSTALL_DIR/lib/cmake/mlir/ \\ -DLLVM_DIR=$LLVM_INSTALL_DIR/lib/cmake/llvm/ \\ -DLLVM_EXTERNAL_PROJECTS=torch-mlir \\ -DLLVM_EXTERNAL_TORCH_MLIR_SOURCE_DIR=`$PWD`cmake --build build -j8cmake --install build --prefix build/install 运行python测试 出现cpp系统库找不到的问题fatal error: algorithm file not foundexport SDKROOT$(xcrun –show-sdk-path)"},{"title":"「LLM」HuggingFace仓库下载及指定缓存路径","path":"/tech/LLM/02_HuggingFace_download/","content":"首先hf上是存在对国家歧视的，部分需要申请访问资格的repo会自动拒绝国内的账户（说的就是你LLaMA），这种情况有两种解决办法，一是魔塔上下载模型，另一种是从hf上其他用户手里下载（但是这种情况可能需要修改你下载好的仓库中的json文件才能用transformers的api使用）。同时国内下载模型的速度感人，可以切换国内的镜像来下载。下载前最好指定cache路径，否则你想找模型很难得啦，当然下载好的模型路径其实是有点奇怪的，最好的办法是通过transformer加载，然后保存一份，这样的目录下是很干净的模型。Let’s see see! 国内镜像export HF_ENDPOINT=https://hf-mirror.com Cache位置export HF_HOME=/path/to/your/local_dir python代码下载完整仓库import transformersfrom transformers import AutoModelForCausalLM, AutoTokenizerfrom huggingface_hub import snapshot_downloaddef download_from_huggingface(model_name):\tprint(f Downloading \\033[32m\\033[1m model_name \\033[0m from huggingface ... ) model_path = snapshot_download(repo_id=model_name) # revision=transformers.__version__ 可以指定下载版本， 但是transformers版本并不能作为仓库版本来用 print(f\\033[32m\\033[1m model_name \\033[0m has been downloaded into \\033[34m\\033[5m model_path \\033[0m)return model_path 之后你会在Cache目录下看到一些hf相关的文件，而模型会在上面打印的路径下。当然，打开后你会发现这里的文件全是软链接，并且不见得是你当前transformers对应的版本的内容。而总所周知，transformers不同版本中的同一个模型的实现是会变的，例如GLM4(在4.49版本下是继承llama的搭框架以及Phi的MLP func， 但是4.46中是其他版本，当然目前我也没法确定他们最终的推理效果是不是一致)。 models--Qwen--Qwen2.5-0.5B-Instruct/├── blobs│ ├── 07bfe0640cb5a0037f9322287fbfc682806cf672│ ├── 0dbb161213629a23f0fc00ef286e6b1e366d180f│ ├── 20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0│ ├── 443909a61d429dff23010e5bddd28ff530edda00│ ├── 4783fe10ac3adce15ac8f358ef5462739852c569│ ├── 4b8373851d093eb9f3017443f27781c6971eff24│ ├── 6634c8cc3133b3848ec74b9f275acaaa1ea618ab│ ├── a6344aac8c09253b3b630fb776ae94478aa0275b│ ├── dfc11073787daf1b0f9c0f1499487ab5f4c93738│ └── fdf756fa7fcbe7404d5c60e26bff1a0c8b8aa1f72ced49e7dd0210fe288fb7fe├── refs│ └── main└── snapshots └── 7ae557604adf67be50417f59c2c2f167def9a775 ├── config.json - ../../blobs/0dbb161213629a23f0fc00ef286e6b1e366d180f ├── generation_config.json - ../../blobs/dfc11073787daf1b0f9c0f1499487ab5f4c93738 ├── LICENSE - ../../blobs/6634c8cc3133b3848ec74b9f275acaaa1ea618ab ├── merges.txt - ../../blobs/20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0 ├── model.safetensors - ../../blobs/fdf756fa7fcbe7404d5c60e26bff1a0c8b8aa1f72ced49e7dd0210fe288fb7fe ├── README.md - ../../blobs/4b8373851d093eb9f3017443f27781c6971eff24 ├── tokenizer_config.json - ../../blobs/07bfe0640cb5a0037f9322287fbfc682806cf672 ├── tokenizer.json - ../../blobs/443909a61d429dff23010e5bddd28ff530edda00 └── vocab.json - ../../blobs/4783fe10ac3adce15ac8f358ef5462739852c569 ${HF_HOME}hubmodels–Qwen–Qwen2.5-0.5B-Instructsnapshots7ae557604adf67be50417f59c2c2f167def9a775这个目录就是你当前下载的repo了，可以看下config.json 中的参数 architectures: [ Qwen2ForCausalLM ], attention_dropout: 0.0, bos_token_id: 151643, eos_token_id: 151645, hidden_act: silu, hidden_size: 896, initializer_range: 0.02, intermediate_size: 4864, max_position_embeddings: 32768, max_window_layers: 21, model_type: qwen2, num_attention_heads: 14, num_hidden_layers: 24, num_key_value_heads: 2, rms_norm_eps: 1e-06, rope_theta: 1000000.0, sliding_window: 32768, tie_word_embeddings: true, torch_dtype: bfloat16, transformers_version: 4.43.1, use_cache: true, use_sliding_window: false, vocab_size: 151936 transformers_version: 4.43.1 ？？这里可以通过另一种方式将他修改成对应transformers版本的内容 import transformersfrom transformers import AutoModelForCausalLM, AutoTokenizerfrom huggingface_hub import snapshot_downloaddef download_from_huggingface(model_api, tokenizer_api, model_name):\tprint(f Downloading \\033[32m\\033[1m model_name \\033[0m from huggingface ... )\tmodel_dir = os.path.join(os.path.dirname(__file__), model_name)\tmodel_path = snapshot_download(repo_id=model_name) try: model = model_api.from_pretrained(model_path, trust_remote_code=True) tokenizer = tokenizer_api.from_pretrained(model_path, trust_remote_code=True)\texcept Exception as e: raise os.error(f\\033[31m Download model_name has error. Make sure its a valid repository. Or check your network!\\033[0m) model.save_pretrained(model_dir)\ttokenizer.save_pretrained(model_dir)\tprint(f\\033[32m\\033[1m model_name \\033[0m has been downloaded into \\033[34m\\033[5m model_dir \\033[0m)return model_dir 然后你就会发现这个新的路径下的config.json 就会编程当前transformers的版本了 _name_or_path: /data/huggingface_cache/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775,\tarchitectures: [ Qwen2ForCausalLM\t],\tattention_dropout: 0.0,\tbos_token_id: 151643,\teos_token_id: 151645,\thidden_act: silu,\thidden_size: 896,\tinitializer_range: 0.02,\tintermediate_size: 4864,\tmax_position_embeddings: 32768,\tmax_window_layers: 21,\tmodel_type: qwen2,\tnum_attention_heads: 14,\tnum_hidden_layers: 24,\tnum_key_value_heads: 2,\trms_norm_eps: 1e-06,\trope_scaling: null,\trope_theta: 1000000.0,\tsliding_window: 32768,\ttie_word_embeddings: true,\ttorch_dtype: float32,\ttransformers_version: 4.49.0,\tuse_cache: true,\tuse_sliding_window: false,\tvocab_size: 151936 结束语通过上面的一系列操作就可以得到你想要的模型，解决模型版本的问题，这种方法同样可以修改你已经下载了的模型，让他的参数和你的transformers版本对齐。 撒花！"},{"title":"「LLM」llama architectures","path":"/tech/LLM/01_LLM_llama/","content":"LlamaForCausalLMgraph TD %% 主图方向 direction RL %% 全局样式定义 classDef preprocess fill:#e8f8f5,stroke:#45b7d1; classDef decoder fill:#f9d5e5,stroke:#e892a2; classDef attention fill:#d4efdf,stroke:#7dcea0; classDef ffn fill:#e8daef,stroke:#c39bd3; classDef layer fill:#fdebd0,stroke:#eb984e; subgraph \"Model\" direction TB %% 预处理模块 subgraph \"Preprocessing\" direction TB A[Input Text] --> B[Token Embedding] B --> B1[Get KVCache] B1 --> B2[Get cache_position] B2 --> B3[Get position_ids] B3 --> B4[Compute causal_mask] B4 --> B5[rotary_emb] class B,B1,B2,B3,B4,B5 preprocess; end %% 解码器堆叠层 subgraph \"Decoder Stack\" direction TB B5 --> D[Decoder Layer 1] D --> E[Decoder Layer 2] E --> F[...] F --> G[Decoder Layer N] G --> G1[Last LayerNorm] class D,E,F,G layer; end %% 输出模块 subgraph \"Output\" direction TB G1 --> H[Lm_head] H --> I[Output Layer] end end %% 解码器层详细结构 subgraph \"Decoder Layer Detail\" style D decoder direction TB D --> D0[hidden_states] D0 --> D1[Layer Norm] D1 --> D2[Self-Attention] D2 --> D3[Add] D0 -.- D3 D3 --> D5[post LayerNorm] D5 --> D6[MLP] D6 --> D7[Add] D3 -.- D7 class D2 attention; class D5,D6 ffn; end %% 注意力层详细结构 subgraph \"Attention Layer Detail\" style D2 attention direction LR D2 --> D20[attention_input] D20 --> D201[K] D20 --> D202[V] D20 --> D203[Q] D20 --> D204[attention_mask] D201 --> D211[repeat_kv_k] D202 --> D212[repeat_kv_v] D211 --> D21[Matmul] D203 --> D21 D204 --> D214[slice] D214 --> D22[Add] D21 --> D22 D22 --> D23[softmax: float32] D23 --> D24[Matmul:TP] D212 --> D24 end %% 不可见连接引导 %% G1 -.-|缓冲连接| H %% %% D7 -.-|隐藏连接| E %% F -.-|...| G"},{"title":"「LLM」Attentions","path":"/tech/LLM/00_LLM_attention/","content":"attention种类 MHA一图看懂Multi-HeadAttention MQAMHA kvcache的巨大开销,直接全部共享,损失推理的效果 GQA Paged Attention 首先说明一点paged attention只是一种attention中的kvCache管理方案，并不是新的attention计算方式。 Paged顾名思义是与操作系统中的分页管理有关系，paged attention就是将kvCache进行分页管理，至于为什么要进行分页管理就需要先回顾一下LLM中的kvCache的特点。在首次prefill阶段，kvcache的大小是0，也就是输入到模型的是一个空的tensor，当然也可以不输入给模型（ps：这就需要模型支持不确定数量的输入节点，这在python代码中是很容易的，但是在端侧设备中多数是不支持可变数量的输入的），prefill阶段结束后会产生prefill输入的tokens数量+1个的kvCache，而每次输入的tokens数量并不固定，那么如何管理kvCache的内存呢？ 静态分配？ kvCache的大小是会不断增长的，静态分配是的大小设置为多少都不见得合适，太大了会占用过多内存，且可能出现大部分申请的内存并没有利用的情况，小了就不够多轮对话产生的kvcachekvcache存储。 （X） 动态分配？动态内存相对来说较为合适，但是从现存的操作系统的内存管理来看，内存碎片问题是一个很难解决的问题，这对于LLM推理来说也是一个麻烦，kvCache的大小会导致在多次对话后，没有合适的连续空间来分配给kvCache（碎片化的内存会影响这种连续分配的方式）但是kvCache还有一个特点，在一个确定的模型中，大小只受限于sequence Length这一个维度，也就是输入tokens数+输出tokens数，这也就意味着如何以这个维度为单位来存储kvCache就可以完全确定每次存储的kvCache大小。 总结来说kvCache的特点如下： 理论上大小可能不存在上限 除了sequenceLength这个维度外全是固定shape 分页式管理方案以Qwen2.5为例，kvCache的大小为（decodeLayer, kv, batchSize, numKVHead, sequenceLength, headDim）e.g. (24, 2, 1, 2, x, 128)那么这对于paged attention 来说就很容易表示存储关系了，分页式存储维护block table来管理虚拟内存和物理内存，虚拟内存中是连续分配的，而物理内存中可以是不连续分配的，而block table就是用来映射物理内存和虚拟内存关系的。每次分配一个block，一个block中可以存储多个kvCache，存储的kvCache数量为block size，因此以paged attention方式管理的kvCache的表达方式为（numBlock, blockSize, deocdeLayer,kv,batchSize,numKVHead,headDim）。下图为两次sample同时请求时的管理方式 来源七月大佬csdn 首先每个用户都会对应一个logical kv block table，用于管理自己的kvCache，多个logical table会对应一个physical kv block table，这个物理table用于非连续存储kvCache。其次，当多个用户的请求存在完全相同的内容时，会完全共用存储中的kvCache，这时也会有block的引用计数，例如block7最后，当两个用户请求产生的token出现不同时，会触发复制相同token kvCache的机制，减少计算例如block1和3。 映射方式这里就产生了一个问题，模型输入中如果只有logical table，那怎么知道这个逻辑表对应的物理表中的位置呢，这就需要一个新的属性来表示，称为slot mapping。这个slot mapping就是用来描述当前的这个作为输入的kvCache与物理设备的映射关系，数据结构通常就是一个list[N]，将logical table [0-n]映射到physical table[list[N]] 上。且并不一定logical 的最后一个block会被填满， 因此还需要一个参数来表明最后填充的数量。 slot mapping： list[int]logical table：dictint, List[int]physical: dictint, kvCache?","categories":["attention"]},{"title":"「MLIR-0」从零开始","path":"/tech/MLIR_src/MLIR_0/","content":"LLVM、MLIR的编译先行编译基础的工具，后期用到别的工具再进行编译 git clone https://github.com/llvm/llvm-project.gitcd llvm-projectmkdir build cd build# CMake配置（仅编译LLVM和MLIR， clang也可以加上）cmake -G Ninja ../llvm \\ -DLLVM_ENABLE_PROJECTS=mlir;llvm;clang;lldb;lld; \\ -DLLVM_TARGETS_TO_BUILD=host \\ -DCMAKE_BUILD_TYPE=Release \\ -DLLVM_ENABLE_ASSERTIONS=ON \\ -DCMAKE_INSTALL_PREFIX=./installninja -j$(nproc)ninja install 等待编译完成后，检查一下install目录下都生成了哪些工具，依次来学习一下。以下内容都需要在后续学习中用到后，再次进行更新，确认当前查询到的信息是正确的。 一、MLIR核心工具 ​mlir-opt​ ​用途：MLIR优化与转换的核心工具 ​功能： 执行自定义Pass管道（如 --pass-pipeline=builtin.module(my-pass)） 验证IR合法性（-verify） 调试方言降级（如从 tensor 到 memref 的转换） ​示例： bash mlir-opt input.mlir --convert-linalg-to-loops -o output.mlir ​mlir-translate​ ​用途：MLIR与其他中间表示（LLVM IRSPIR-V）的互转 ​常用选项： --mlir-to-llvmir：将MLIR转换为LLVM IR --export-llvmir：导出为LLVM可读格式 ​示例： bash mlir-translate --mlir-to-llvmir input.mlir -o output.ll ​mlir-tblgen​ ​用途：生成MLIR方言（Dialect）的C++代码 ​场景：定义新方言时自动生成ODS（Operation Definition Specification）代码。 ​mlir-cpu-runner​ ​用途：在CPU上直接运行MLIR代码（支持JIT编译） ​示例： bash mlir-cpu-runner --entry-point-result=void input.mlir ​二、LLVM核心工具 ​llc​ ​用途：将LLVM IR编译为特定目标（如CPUGPU）的汇编代码 ​关键选项： -mtriple=amdgcn-amd-amdhsa：指定AMD GPU目标 -O3：启用优化级别3 ​示例： bash llc -O3 input.ll -o output.s ​opt​ ​用途：LLVM IR优化器，用于调试自定义Pass ​示例： bash opt -passes=loop-unroll input.ll -o output.ll ​llvm-dis llvm-as​ ​用途：LLVM IR二进制格式（bitcode）与文本格式互转 ​示例： bash llvm-dis input.bc -o output.ll # 反汇编llvm-as input.ll -o output.bc # 汇编 ​三、调试与分析工具 ​llvm-objdump​ ​用途：反汇编目标文件（如检查生成的GPU二进制） ​示例： bash llvm-objdump -d output.o ​llvm-dwarfdump​ ​用途：调试信息分析（DWARF格式），用于检查符号表与源码映射 ​示例： bash llvm-dwarfdump output.o ​FileCheck​ ​用途：LLVM测试框架核心工具，用于验证输出是否符合预期模式 ​示例： bash mlir-opt input.mlir | FileCheck check.txt ​四、硬件相关工具 ​amdgpu-arch nvptx-arch​ ​用途：检测当前系统的AMDNVIDIA GPU架构 ​示例： bash nvptx-arch # 输出如 sm_80（对应NVIDIA A100） ​llvm-mca​ ​用途：静态分析机器代码的流水线吞吐量（用于性能调优） ​示例： bash llc -O3 input.ll -o output.s llvm-mca output.s ​五、辅助开发工具 ​clang-format​ ​用途：格式化C++MLIR代码（维护代码风格一致性） ​示例： bash clang-format -i my_pass.cpp ​mlir-reduce​ ​用途：自动缩小触发错误的MLIR测试用例（类似Delta Debugging） ​示例： bash mlir-reduce crash.mlir --bugpoint=mlir-opt -o reduced.mlir ​mlir-lsp-server​ ​用途：MLIR语言服务器，支持IDE语法高亮与自动补全（如VSCode插件）。 编译其他库时遇到的llvm编译问题 ld64.lld: error: undefined symbol: divdc3场景：在编译torch-mlir的时候会依赖三方库stablehlo，在链接StableHLO 的 C++ 引用实现（libStablehloReferenceElement.a）时找不到复数除法符号 divdc3，这是compiler-rtlibgcc 提供的运行时辅助函数。 原因：可以从最前面的编译选项中看到我们在编译llvm的时候，并没有编译compiler-rt，加上重新编译就可以了-DLLVM_ENABLE_PROJECTS=mlir;llvm;clang;lldb;lld;compiler-rt \\-DLLVM_ENABLE_RUNTIMES=compiler-rt。（直接在llvm_enable_projects 中添加compiler-rt未来会被废弃） 找不到Python对应的库场景：在编译torch-mlir的时候发现开启-DMLIR_ENABLE_BINDINGS_PYTHON=ON之后会出现找不到mlir python相关的库。 原因：编译llvmMLIR的时候也需要开启-DMLIR_ENABLE_BINDINGS_PYTHON=ON nanobind找不到头文件场景：开启python binding之后找不到nanobind的头文件 原因： 没有安装nanobind brew install nanobind或者pip install nanobind 找不到路径 在编译选项中增加-Dnanobind_DIR$(python -c “import nanobind, pathlib, sys; print(pathlib.Path(nanobind.file).parent ‘cmake’)”)。 The dependency target FileCheck/not/count of xxx does not exist.原因：这些都是llvm的测试工具，某些测试套件依赖，需要增加编译选项-DLLVM_INSTALL_UTILS=ON"},{"title":"音频相关的处理函数","path":"/tech/audio function/","content":"hann_windowsstd::vectordouble hann_windows(int window_length = N_FFT,bool periodic = true) int N = window_length; if (periodic) N = window_length + 1; // 如果是周期性窗口，窗口长度加1 auto M = N - 1; auto double_PI = 2 * M_PI; std::vectordouble window(window_length); for (int n = 0; n window_length; ++n) double normalized_n = static_castdouble(n) / M; window[n] = 0.5 - 0.5 * cos( double_PI * normalized_n); // 或者使用等效的公式： // window[n] = pow(sin(M_PI * normalized_n), 2); return window; $$X[\\omega, m] \\sum_{k 0}^{\\text{win_length-1}}% \\text{window}[k]\\ \\text{input}[m \\times \\text{hop_length} + k]\\ % \\exp\\left(- j \\frac{2 \\pi \\cdot \\omega k}{\\text{n_fft}}\\right)$$"},{"title":"端侧芯片K230上利用llama.cpp部署大模型(简易教程)","path":"/tech/llama.cpp_on_K230/","content":"目前大模型研究非常广泛，但是对于个人用户如何部署一个大模型通常是比较麻烦的，而llama.cpp项目作为一个使用cpp来完成大模型推理的开源工具，对于没有丰富硬件资源的用户来说是一个很好用的工具，可以将中意的大模型十分简单的部署在自己的设备上。以下内容为使用llama.cpp将大模型部署在端侧设备K230上。 1. llama.cpp 下载源码 git clone https://github.com/ggerganov/llama.cpp.git 准备K230编译工具链 首先说明一点，llama.cpp是一个纯cc++的项目，虽然有相应的python文件作为模型转换工作，但是对于直接部署模型来说并不是很重要(毕竟个人用户也没有什么机会自己训练一个大模型，而预训练好的大模型在huggingface是很多的，同时llama.cpp需要的gguf格式的模型也可以在fuggingface上找到)。既然是c项目，那么很容易就可以通过交叉编译来生成K230上可运行的程序。 也可以使用带有rvv intrinsic的工具链进行编译，例如gcc14，这个自行尝试即可 K230的编译工具链很容易获取，编译过K230_sdk镜像的话，是可以在其toolchain目录下找到对应的工具链，拿来直接用就好。(现在我假设你已经有了噢) drwxr-xr-x 10 curio curio 4.0K 11月 20 12:24 Xuantie-900-gcc-linux-6.6.0-glibc-x86_64-V3.0.1 现在还缺少K230相关的编译选项，可以通过K230的编译器nncase repo来获取，保留以下内容即可： set(CMAKE_SYSTEM_NAME Linux)set(CMAKE_SYSTEM_PROCESSOR riscv64)if(DEFINED ENVRISCV_ROOT_PATH) file(TO_CMAKE_PATH $ENVRISCV_ROOT_PATH RISCV_ROOT_PATH)endif()if(NOT RISCV_ROOT_PATH) message(FATAL_ERROR RISCV_ROOT_PATH env must be defined)endif()set(RISCV_ROOT_PATH $RISCV_ROOT_PATH CACHE STRING root path to riscv toolchain)set(CMAKE_C_COMPILER $RISCV_ROOT_PATH/bin/riscv64-unknown-linux-gnu-gcc)set(CMAKE_CXX_COMPILER $RISCV_ROOT_PATH/bin/riscv64-unknown-linux-gnu-g++)set(CMAKE_FIND_ROOT_PATH $RISCV_ROOT_PATH/riscv64-unknown-linux-gnu)set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)set(ENABLE_VULKAN_RUNTIME OFF)set(CMAKE_C_FLAGS $CMAKE_C_FLAGS -march=rv64imafdcv -mabi=lp64d -mcmodel=medany -fstack-protector-strong -fPIE -pie -Wl,-z,now -Wl,-z,relro)set(CMAKE_CXX_FLAGS $CMAKE_CXX_FLAGS -march=rv64imafdcv -mabi=lp64d -mcmodel=medany -fstack-protector-strong -fPIE -pie -Wl,-z,now -Wl,-z,relro)// 开启rvv， K230的工具链没有添加这个宏定义，如果使用gcc14的话不需要定义。// add_definitions(-D__riscv_v_intrinsic)// 当前不推荐开启，llama.cpp 的rvv实现有问题 将以上内容写入到.cmake 文件中，在后面编译中会用到 编译 export RISCV_ROOT_PATH=/PATH/TO/YOUR/TOOLCHAIN/Xuantie-900-gcc-linux-6.6.0-glibc-x86_64-V3.0.1/cd llama.cppcmake . -B build -DCMAKE_TOOLCHAIN_FILE=/PATH/TO/YOUR/TOOLCHAIN/k230.cmake -DCMAKE_BUILD_TYPE=Releasecmake --build build --config Release -j 成功编译后可以在buildbin目录下找到llama-cli 下载模型直接在huggingface上下载 准备上板执行将llama.cpp 和 qwen2.5-0.5b-instruct-q2_k.gguf拷贝到K230上 执行命令 ./llama-cli -m qwen2.5-0.5b-instruct-q2_k.gguf -p system name whatever -cnv 这里的-cnv是用于开启对话模式，稍等几分钟就可以进行对话(虽然现在推理还很慢，如果开启rvv优化，这个2bit 量化的模型在K230上性能可以达到0.8ts)"},{"title":"通过cmake依赖分析项目结构","path":"/tech/cmake_dependence/","content":"拿到一个大的项目是否无从下手，不知道从哪里开始阅读代码，分析逻辑，最简单的办法应该就是通过cmake来分析库之间的依赖，然后逐个模块进行代码阅读。那么如果对cmake代码不是非常熟悉，直接看CMakeLists是很痛苦的，今天就就少一个简单的工具来进行cmake依赖分析 Graphviz安装在Linux平台直接通过apt安装 sudo apt-get install graphviz 使用在cmake config命令中加入–graphvizdep.dot例如： cmake --preset conan-runtime-release --graphviz=dep.dot 之后会在当前目录下生成dep.dot 以及各个可执行程序单独的模块依赖"},{"title":"windows-dll-missing-debug","path":"/tech/DLL missing debug/","content":"当程序出现类似于下面的错误时： # python import nncaseTraceback (most recent call last): File stdin, line 1, in module File D:\\a ncase ncase\\install\\python ncase\\__init__.py, line 34, in module import _nncaseImportError: DLL load failed while importing _nncase: The specified module could not be found. 这意味着系统中存在某个dll确实的情况，一种简单的排查手段是使用Dependency Walker来进行定位，github链接为：Dependency Walker github Dependency Walker可以直接使用wget下载： wget https://github.com/lucasg/Dependencies/releases/download/v1.11.1/Dependencies_x64_Release.zipunzip xxx.zip 命令行模式进入对应的目录下，找到Dependencies.exe（命令行执行） ./Dependencies.exe -depth 2 -chain _nncase.cp38-win_amd64.pyd log如下： # ./../../../Dependencies.exe -depth 2 -chain _nncase.cp38-win_amd64.pyd ├ _nncase.cp38-win_amd64.pyd (ROOT) : _nncase.cp38-win_amd64.pyd | ├ Nncase.Runtime.Native.dll (Environment) : D:\\a ncase ncase\\install\\bin\\Nncase.Runtime.Native.dll | | ├ libomp140.x86_64.dll (WindowsFolder) : C:\\Windows\\system32\\libomp140.x86_64.dll | | ├ KERNEL32.dll (WellKnownDlls) : C:\\Windows\\system32\\kernel32.dll | | | ├ api-ms-win-core-rtlsupport-l1-1-0.dll (ApiSetSchema) : C:\\Windows\\system32 tdll.dll | | | ├ api-ms-win-core-rtlsupport-l1-2-2.dll (ApiSetSchema) : C:\\Windows\\system32 tdll.dll | | | ├ ntdll.dll (WellKnownDlls) : C:\\Windows\\system32 tdll.dll | | | ├ api-ms-win-core-appcompat-l1-1-1.dll (ApiSetSchema) : C:\\Windows\\system32\\kernelbase.dll| | | ├ ext-ms-win-oobe-query-l1-1-0.dll (NOT_FOUND) :| | | ├ RPCRT4.dll (WellKnownDlls) : C:\\Windows\\system32\\rpcrt4.dll| | ├ ADVAPI32.dll (WellKnownDlls) : C:\\Windows\\system32\\advapi32.dll| ├ python38.dll (Environment) : C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\python38.dll| | ├ VERSION.dll (WindowsFolder) : C:\\Windows\\system32\\VERSION.dll| | ├ api-ms-win-crt-filesystem-l1-1-0.dll (ApiSetSchema) : C:\\Windows\\system32\\ucrtbase.dll| ├ KERNEL32.dll (WellKnownDlls) : C:\\Windows\\system32\\kernel32.dll 可以很容易发现ext-ms-win-oobe-query-l1-1-0.dll这个dll不存在，然后google对应的问题解决 目前该问题没有合适的解决方法，只是操作系统版本不匹配导致 gflags该方法目前没有找到非常合适的使用案例，tmate中无法使用event viewer工具，无法查看系统日志gflags.exe 的命令使用如下e /c/Program\\ Files\\ \\(x86\\)/Windows\\ Kits/10/Debuggers/x64/gflags.exe -i c:/hostedtoolcache/windows/Python/3.8.10/x64/python.exe +sls GUI界面如果要使用GUI，则执行DependenciesGUI.exe Dumpbin如何在tmate中使用dumpbin首先需要激活vscmd环境，不能直接使用windows server中的命令行，否则无法找到dumpbin工具$ cd cProgram FilesMicrosoft Visual Studio2022EnterpriseCommon7Tools$ .LaunchDevCmd.bat之后就激活了vscmd命令行环境，vs相关的工具都可以使用 dumpbin使用首先使用DEPENDENTS 来查找当前库所依赖的dll，之后逐级查找dumpbin DEPENDENTS _nncase.cp38-win_amd64.pyd Microsoft (R) COFF/PE Dumper Version 14.41.34120.0Copyright (C) Microsoft Corporation. All rights reserved.Dump of file _nncase.cp38-win_amd64.pydFile Type: DLL Image has the following dependencies: Nncase.Runtime.Native.dllpython38.dllKERNEL32.dll dumpbin DEPENDENTS D:\\a ncase ncase\\install\\bin\\Nncase.Runtime.Native.dll Dump of file D:\\a ncase ncase\\install\\bin\\Nncase.Runtime.Native.dllFile Type: DLL Image has the following dependencies: libomp140.x86_64.dllKERNEL32.dllADVAPI32.dll dumpbin DEPENDENTS C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\python38.dll Dump of file C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\python38.dllFile Type: DLL Image has the following dependencies: VERSION.dllSHLWAPI.dllWS2_32.dllADVAPI32.dllKERNEL32.dllVCRUNTIME140.dllapi-ms-win-crt-math-l1-1-0.dllapi-ms-win-crt-locale-l1-1-0.dllapi-ms-win-crt-string-l1-1-0.dllapi-ms-win-crt-runtime-l1-1-0.dllapi-ms-win-crt-stdio-l1-1-0.dllapi-ms-win-crt-convert-l1-1-0.dllapi-ms-win-crt-time-l1-1-0.dllapi-ms-win-crt-environment-l1-1-0.dllapi-ms-win-crt-process-l1-1-0.dllapi-ms-win-crt-heap-l1-1-0.dllapi-ms-win-crt-conio-l1-1-0.dllapi-ms-win-crt-filesystem-l1-1-0.dll dumpbin DEPENDENTS C:\\Windows\\System32\\kernel32.dll Dump of file C:\\Windows\\System32\\kernel32.dllFile Type: DLL Image has the following dependencies: api-ms-win-core-rtlsupport-l1-1-0.dllapi-ms-win-core-rtlsupport-l1-2-2.dllntdll.dllKERNELBASE.dllapi-ms-win-core-processthreads-l1-1-0.dllapi-ms-win-core-processthreads-l1-1-3.dllapi-ms-win-core-processthreads-l1-1-2.dllapi-ms-win-core-processthreads-l1-1-1.dllapi-ms-win-core-registry-l1-1-0.dllapi-ms-win-core-heap-l1-1-0.dllapi-ms-win-core-heap-l2-1-0.dllapi-ms-win-core-memory-l1-1-1.dllapi-ms-win-core-memory-l1-1-0.dllapi-ms-win-core-memory-l1-1-2.dllapi-ms-win-core-handle-l1-1-0.dllapi-ms-win-core-synch-l1-1-0.dllapi-ms-win-core-synch-l1-2-1.dllapi-ms-win-core-synch-l1-2-0.dllapi-ms-win-core-file-l1-1-0.dllapi-ms-win-core-file-l1-2-0.dllapi-ms-win-core-file-l1-2-2.dllapi-ms-win-core-file-l1-2-4.dllapi-ms-win-core-file-l1-2-1.dllapi-ms-win-core-delayload-l1-1-0.dllapi-ms-win-core-io-l1-1-0.dllapi-ms-win-core-io-l1-1-1.dllapi-ms-win-core-job-l1-1-0.dllapi-ms-win-core-threadpool-legacy-l1-1-0.dllapi-ms-win-core-threadpool-private-l1-1-0.dllapi-ms-win-core-largeinteger-l1-1-0.dllapi-ms-win-core-libraryloader-l1-2-3.dllapi-ms-win-core-libraryloader-l1-2-2.dllapi-ms-win-core-libraryloader-l1-2-0.dllapi-ms-win-core-libraryloader-l1-2-1.dllapi-ms-win-core-libraryloader-l2-1-0.dllapi-ms-win-core-namedpipe-l1-2-2.dllapi-ms-win-core-namedpipe-l1-1-0.dllapi-ms-win-core-namedpipe-l1-2-1.dllapi-ms-win-core-datetime-l1-1-1.dllapi-ms-win-core-datetime-l1-1-0.dllapi-ms-win-core-datetime-l1-1-2.dllapi-ms-win-core-sysinfo-l1-2-0.dllapi-ms-win-core-sysinfo-l1-2-1.dllapi-ms-win-core-sysinfo-l1-1-0.dllapi-ms-win-core-sysinfo-l1-2-3.dllapi-ms-win-core-timezone-l1-1-0.dllapi-ms-win-core-localization-l1-2-0.dllapi-ms-win-core-processsnapshot-l1-1-0.dllapi-ms-win-core-processenvironment-l1-1-0.dllapi-ms-win-core-processenvironment-l1-2-0.dllapi-ms-win-core-string-l1-1-0.dllapi-ms-win-core-debug-l1-1-0.dllapi-ms-win-core-debug-l1-1-1.dllapi-ms-win-core-errorhandling-l1-1-0.dllapi-ms-win-core-errorhandling-l1-1-3.dllapi-ms-win-core-fibers-l1-1-0.dllapi-ms-win-core-util-l1-1-0.dllapi-ms-win-core-profile-l1-1-0.dllapi-ms-win-security-base-l1-1-0.dllapi-ms-win-security-base-l1-2-0.dllapi-ms-win-security-appcontainer-l1-1-0.dllapi-ms-win-core-comm-l1-1-0.dllapi-ms-win-core-realtime-l1-1-0.dllapi-ms-win-core-wow64-l1-1-1.dllapi-ms-win-core-wow64-l1-1-0.dllapi-ms-win-core-wow64-l1-1-3.dllapi-ms-win-core-systemtopology-l1-1-1.dllapi-ms-win-core-systemtopology-l1-1-0.dllapi-ms-win-core-processtopology-l1-1-0.dllapi-ms-win-core-namespace-l1-1-0.dllapi-ms-win-core-file-l2-1-2.dllapi-ms-win-core-file-l2-1-0.dllapi-ms-win-core-file-l2-1-1.dllapi-ms-win-core-file-l2-1-3.dllapi-ms-win-core-xstate-l2-1-0.dllapi-ms-win-core-xstate-l2-1-1.dllapi-ms-win-core-xstate-l2-1-2.dllapi-ms-win-core-localization-l2-1-0.dllapi-ms-win-core-normalization-l1-1-0.dllapi-ms-win-core-fibers-l2-1-0.dllapi-ms-win-core-fibers-l2-1-1.dllapi-ms-win-core-localization-private-l1-1-0.dllapi-ms-win-core-sidebyside-l1-1-0.dllapi-ms-win-core-appcompat-l1-1-0.dllapi-ms-win-core-windowserrorreporting-l1-1-1.dllapi-ms-win-core-windowserrorreporting-l1-1-2.dllapi-ms-win-core-windowserrorreporting-l1-1-0.dllapi-ms-win-core-windowserrorreporting-l1-1-3.dllapi-ms-win-core-console-l1-1-0.dllapi-ms-win-core-console-l1-2-0.dllapi-ms-win-core-console-l1-2-1.dllapi-ms-win-core-console-l2-1-0.dllapi-ms-win-core-console-l2-2-0.dllapi-ms-win-core-console-l3-2-0.dllapi-ms-win-core-psapi-l1-1-0.dllapi-ms-win-core-psapi-ansi-l1-1-0.dllapi-ms-win-eventing-provider-l1-1-0.dllapi-ms-win-core-apiquery-l1-1-0.dllapi-ms-win-core-delayload-l1-1-1.dllapi-ms-win-core-appcompat-l1-1-1.dllImage has the following delay load dependencies:ext-ms-win-oobe-query-l1-1-0.dllRPCRT4.dll","tags":["windows"],"categories":["Debug"]},{"title":"tmate ssh github CI","path":"/tech/ssh调试github CI/","content":"CI workflow中增加远程配置- name: Start tmate session for debugging uses: mxschmitt/action-tmate@v3 continue-on-error: true# 例如，如果编译失败，可以通过SSH进行调试- run: echo Run after tmate session 这一步添加到你想要开始调试的位置,CI运行后会出现如下log: ssh nM72R3SZDXgCtPqB4DGha8yax@nyc1.tmate.ioSSH: ssh nM72R3SZDXgCtPqB4DGha8yax@nyc1.tmate.io 在命令行运行即可:q 退出当前界面即可登入github CI环境","tags":["debug"],"categories":["program"]},{"title":"北京半日游攻略","path":"/life/Beijing_half_day_travel/","content":"好玩的线路1天安门广场 – 毛主席纪念堂– 故宫 国家博物馆 该路线可以在周六早上进行(故宫国博二选一能一上午结束) 地点 预约时间 预约方式(公众号或者小程序) 8月30日 8月31日(周六) 9月1日 备注(很多地方周一是闭馆的) 预约日期 升旗 00:00 天安门印象 2105:39 2205:40 2305:40 天安门广场禁止打火机升旗时间 纪念堂 12:00 毛主席纪念堂预约瞻仰 24 25 26 大约不到1小时(前提是能约到最早的那个时间段) ,不能穿的太随意 故宫 20:00 故宫博物院 23 24 25 国博 17:00 国家博物馆 23 24 25 线路2天坛(提前7天 00:00放票 天坛公园官网购票平台 上次去门口买票就行) – 前门大街天坛: 可以悠闲溜达溜达, 人比颐和园少前门: 各种小吃, 至于它地不地地道道(不知道没去过), 人多警告 线路3颐和园(提前7天 21:00放票) “颐和园”公众号人多, 但是逛起来还是可以的, 推荐坐个船啥的(接近中午的时候体验比较好), 体验下皇帝同款? 简单的游玩，而且比较有北京特色的，大概就这些个了 Tips:长城至少得一天时间玩，远！时间少的就不推荐了 好吃的 烤鸭，炸酱面，小吊梨汤： 四季民福 (故宫里也有)，有钱可以去吃大董(没吃过，吃不起)，没必要专门去方砖厂吃炸酱面，烤鸭店基本都有 涮羊肉，爆肚： 个人觉得哪都差不太多，反正麻酱沾一沾都挺好吃 卤煮：比较重口味，味道独特可以尝试， 大众点评找吧，没吃过几次 炒肝： 早餐店基本都会有，配着包子吃 稻香村：建议去店里各样买一块常常，再决定要不要给家里带大盒的，不要买直接装好的盒装产品，里面的真不一定是最好吃的！","tags":["life","wedding"],"categories":["life","wedding"]},{"title":"cross-compile-debug","path":"/tech/cross-compile-dbg/","content":"交叉编译的程序如何调试1. 调试工具工具链gdb+gdbserver 2. 调试方法 需要使用交叉编译工具链先完成gdbserver的编译，然后放到target上，之后使用如下命令启动 gdbserver host_ip:port /path/to/program args..../gdbserver 192.168.1.2:1234 ./nncase_test_v2.elf $model/test.kmodel $model/input_0_0.bin $model/nncase_result_0.bin 在host上使用gdb连接 gdbtarget remote target_ip:porttarget remote 192.168.1.22:1234 之后在host上正常使用gdb调试命令进行调试 b rvv_memcpy # 设置断点run # 开始执行stepi # step ininfo registers # 查看寄存器 3. host配置问题 安装nfs sudo apt-get install nfs-kernel-server 配置共享文件夹 vim /etc/exports#添加下面内容/home/curio/project/k230/rebuild-ir/k230_linux 192.168.1.22(rw,sync,no_root_squash,no_all_squash) #192.168.1.22也可以替换成 * 表示任意地址可以访问 target上配置 ifconfig eth0 192.168.1.22 # 配置target ipmkdir /modules # mount -t nfs -o nolock 192.168.1.2:/home/curio/project/k230/rebuild-ir/k230_linux/ /modules #nfs挂载host目录","tags":["cross compile"],"categories":["program"]},{"title":"Latex语法归纳","path":"/tech/latex/","content":"数学公式等式关系 小于等于 1. a\\le b2. a\\leq b $a \\le b$ $a\\leq b$ 大于等于 1. a\\ge b2. a\\geq b $a\\ge b$ $a\\geq b$ 不等于 1. a eq b $a eq b$ 行内换行(多行表达式在一行显示)\\atop1. aa \\atop bb + cc2. \\min_aa \\atop bb $aa\\atop bb + cc$ $$\\min_{aa\\atop bb}$$ 上下标1. a_aa2. b^bb $a_{aa}$ $b^{bb}$ 公式编号1. \\tag1 $$yk*x+b \\tag{1}$$","tags":["Latex"],"categories":["Utils"]},{"title":"分布式计算和并行编程中常用的通信原语","path":"/tech/Communication_primitives/","content":"1. broadcast目的：将一个进程（通常称为根进程）持有的数据复制到所有其他参与通信的进程中。操作： 根进程发送其数据至所有其他进程。 其他进程接收并存储根进程发送的数据。结果：所有进程中都有一份与根进程相同的数据副本。应用场景： 初始化阶段，将初始参数或配置信息分发到所有计算节点。 从主节点向工作节点广播最新的模型权重或状态信息。 2. reduce目的：将所有进程各自拥有的相同类型和形状的数据进行某种全局运算（如求和、最大值、最小值、乘积等），并将运算结果保存在指定的一个进程中（通常称为根进程）。操作： 各进程通过高效通信算法逐级或逐环地局部聚合数据，每次聚合后将结果传递到下一个阶段。 最终，根进程保留全局运算的结果。结果：只有根进程拥有经过全局运算后得到的单一结果数据，其他进程不保留结果。应用场景： 计算全局统计指标，如总损失、平均精度等，只需一个进程知道全局结果即可。 并行计算中的收敛检测，根进程收集各进程的局部收敛信息以判断全局收敛状态。 3. scatter目的：将根进程持有的数据分割成多个片段，并将这些片段均匀地分发给所有参与通信的进程。操作： 根进程将数据按照一定的规则（如均等分割、按需分配等）拆分成多个部分。 根进程将每个数据片段发送给相应的目标进程。 目标进程接收并存储分配给自己的数据片段。结果：每个进程最终拥有整个数据集中的一部分，这些部分合起来构成原始数据。应用场景： 分布式训练中，将大型数据集均匀划分给各个计算节点进行并行处理。 分配任务参数或初始化数据到各个工作节点。 4. gather目的：将所有进程各自拥有的数据片段收集到一个指定进程（通常称为根进程）。操作： 各进程将其本地数据发送给根进程。 根进程接收到所有进程发送过来的数据后，将其拼接整合成完整的数据集。结果：只有根进程拥有所有进程发送过来的数据片段组成的完整数据集，其他进程不保留结果。应用场景： 收集各个计算节点的中间结果或局部统计信息，汇总到主节点进行后续处理或决策。 训练完成后，从各个工作节点收集模型权重以进行模型融合或持久化存储。 5. reduce-scatter目的：将所有进程各自拥有的相同类型和形状的数据进行某种全局运算（如求和、最大值、最小值、乘积等），并将运算后的结果分散到各个进程中，每个进程获得一部分结果。操作： 各进程先进行类似reduce的操作，逐步聚合数据并产生全局运算结果。 然后将全局结果按照特定规则分割，并将分割后的片段分散到各个进程。结果：每个进程最终拥有全局运算结果的一部分，这些部分合起来构成全局运算结果。应用场景： 分布式矩阵乘法中，计算矩阵乘积后将其分割回各个节点，用于后续计算。 高效实现某些特定算法，如分布式K-means聚类中的中心点更新。 6. all-to-all目的：每个进程拥有一个数据向量，通信完成后，每个进程获得一个新向量，其中的每个元素来自原来不同进程对应位置的元素。操作： 各进程按照通信协议，交换各自向量中的元素。 每个进程接收到来自其他进程对应位置的元素，并将其放置在新向量的相应位置。结果：所有进程都得到了一个新的数据向量，其中的每个元素来源于参与通信的其他进程。应用场景： 实现数据块的全排列或重新分布，如在神经网络训练中进行层间数据交换。 并行FFT（快速傅里叶变换）等算法中的数据重组。 7. all-gather目的：将参与通信的所有进程各自拥有的不同数据片段合并成一份完整的数据集，并确保这份完整的数据集被所有进程所拥有。操作： 每个进程拥有一个本地的数据片段。 通过通信协议（如MPI、NCCL、Gloo等），每个进程将其本地数据发送给所有其他进程。 所有进程接收到其他进程发送过来的数据后，将其拼接到本地已有的数据片段上，形成完整的数据集。结果：每个进程最终都拥有了一份完整的数据集，包含了所有参与通信进程最初各自拥有的数据片段。应用场景： 在分布式训练中，可能需要将各个设备上的梯度片段收集起来，以便在全局视角下进行参数更新。 在并行计算中，可能需要汇总各个进程计算得到的部分结果，形成全局结果。 8. all-reduce目的：将参与通信的所有进程各自拥有的相同类型和形状的数据进行某种全局运算（如求和、最大值、最小值、乘积等），并将运算结果广播回所有进程。操作： 每个进程拥有一个相同类型和形状的数据元素。 通过高效的通信算法（如树形Reduce、环形Reduce、层级Reduce等），进程间按照指定的运算规则逐步聚合数据。 经过多轮通信和局部运算，每个进程最终得到全局运算的结果。结果：所有进程都拥有经过全局运算后得到的同一份结果数据。应用场景： 在分布式训练中，计算全局的梯度平均值，用于参数更新。 在并行计算中，求解全局的最大值、最小值、累计值等统计量。 总结： all-gather 的目的是将所有进程的独立数据片段合并成一份完整的数据集，每个进程最终拥有全部数据。 all-reduce 的目的是对所有进程的相同数据进行全局运算，每个进程最终拥有运算后的同一份结果数据。 两者都是为了实现分布式环境下的数据聚合，但前者侧重于数据的收集与复制，后者侧重于数据的并行运算与结果广播。在实际应用中，根据任务需求选择合适的通信原语可以有效提升并行计算的效率和可扩展性。这些通信原语构成了分布式计算中基本的数据交换机制，根据实际应用需求的不同，可以单独或组合使用这些原语来构建高效的并行算法和分布式系统。","tags":["Distribution"],"categories":["常识"]},{"title":"全局函数","path":"/tech/after-main-function/","content":"main函数结束后还能执行其他函数？在 main 函数结束后执行的函数或代码通常被称为”终止处理程序”（termination handler）或”退出处理程序”（exit handler）。这个概念在不同的上下文中可能有不同的具体名称： 退出函数（Exit function）当使用 std::atexit() 注册时，这个函数通常被称为”退出函数”。 终止处理程序（Termination handler）特别是在使用 std::set_terminate() 设置处理未捕获异常的函数时。 析构函数（Destructor）当使用全局对象的析构函数来执行清理操作时。 清理函数（Cleanup function）这是一个通用术语，经常用来描述这类函数的功能。 信号处理函数（Signal handler）当使用信号处理机制（如处理 SIGINT）来执行程序结束时的操作。 终结器（Finalizer）这个术语有时用于描述在程序结束时执行最终清理的函数。 钩子函数（Hook function）在某些上下文中，这种在特定时刻（如程序退出时）自动执行的函数被称为”钩子”。 atexit 处理程序（atexit handler）特指通过 std::atexit() 注册的函数。 程序终止函数（Program termination function）一个更正式的术语，描述在程序终止时执行的函数。 延迟加载函数（Deferred function）在一些编程语言和环境中，这种在程序结束时执行的函数被称为”延迟”执行的函数。 尽管没有一个统一的官方术语，但在 C++ 编程中，”退出函数”（exit function）或”清理函数”（cleanup function）是最常用的术语。选择使用哪个术语通常取决于具体的上下文和个人或团队的偏好。在技术文档或代码注释中，使用这些术语中的任何一个都是可以的，只要能清楚地表达其功能即可。 示例以下为几种实现不同的方法介绍。 析构函数 destructor attribute这是 GCC 的一个扩展语法，用于向编译器传达特殊的信息或指令。 这个属性告诉编译器，被标记的函数应该在程序结束时自动调用。它的行为类似于 C++ 全局对象的析构函数。 最简单易用的方式，但是不属于标准特性，并非所有编译器都支持，代码移植性可能会降低。 当程序正常终止（main 函数返回或调用 exit()）时，标记为 destructor 的函数会被自动调用，不需要在main函数中做任何修改。 这些函数的调用顺序与它们的定义顺序相反。 多个 destructor 函数可以共存。 #include iostreamvoid __attribute__((destructor)) cleanup1() std::cout Cleanup 1 executed std::endl;void __attribute__((destructor)) cleanup2() std::cout Cleanup 2 executed std::endl;int main() std::cout Main function executing std::endl; return 0; 输出 Main function executingCleanup 2 executedCleanup 1 executed 扩展 constructor 该语法与destructor相反，用于main函数执行之前调用。 信号处理函数用于处理程序信号异常时的善后工作，或者其他处理。 #include csignalvoid signalHandler(int signum) std::cout Interrupt signal ( signum ) received. ; // your code exit(signum);int main(int argc, char *argv[]) signal(SIGINT, signalHandler); //... return 0; 退出函数（Exit function）使用 std::atexit() 注册时，这个函数通常被称为”退出函数”，也是比较简单易用的方式。 #include iostream#include cstdlibvoid cleanup() std::cout Performing cleanup operations... std::endl; // clean up.int main() // register exit func std::atexit(cleanup); // main code return 0; 钩子函数（Hook Function） TODO","tags":["cpp"],"categories":["Code"]},{"title":"Alpa论文阅读","path":"/paper/Alpa/","content":"论文链接：[https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin] 内容简介Alpa通过生成统一数据、操作符和管道并行性的执行计划，自动化大型深度学习(DL)模型的模型并行训练。现有的模型并行训练系统要么要求用户手动创建并行化计划，要么从有限的模型并行配置空间中自动生成并行化计划。它们不足以在分布式计算设备上扩展复杂的深度学习模型。Alpa通过将并行性视为两个层次:操作符间并行性和操作符内并行性来分配大型DL模型的训练。在此基础上，Alpa为大规模模型并行执行计划构建了一个新的分层空间。Alpa设计了许多编译通道，以便在每个并行级别自动派生有效的并行执行计划。Alpa实现了一个高效的运行时来协调分布式计算设备上的两级并行执行。我们的评估表明，Alpa生成的并行化计划可以匹配或优于手动调整的模型并行训练系统，甚至在它们设计的模型上也是如此。与专门的系统不同，Alpa还泛化到具有异构体系结构的模型和没有手动设计计划的模型。 通过inter-op pass将计算图分割为子图，设备集群分割为device mesh，并寻找将子图分配给device mesh的最佳方式。 通过intra-op pass为inter-op pass找到每个流水线阶段的子图-device mesh的最佳并行方案 运行时编排 生成静态指令，按照顺序排列计算和通信操作，并启动设备集群上的分布式计算图推理。 出发点 创新点在两个层级上进行分布式并行执行策略：intra-op 和 inter-op ，并且这两种策略可以混合使用。 intra-op的并行个人理解为算子内的并行，通过整数线性规划(ILP)来最小化执行成本，将其算子内的并行策略分配到分布式设备中的不同网格(mesh)，并返回其最小的执行成本给inter-op并行过程。 inter-op的并行个人理解为算子间的并行，将模型分割放在分布式设备中的不同网格(mesh)中执行，在该过程中通过DP来最小化不同设备通信的latency成本，最终确定最佳的分布式策略。 以下为这两点的基础示意图 device mesh概念是将一组物理设备视作2D的逻辑设备。每个设备(x,y)都可以沿着这两个维度以不同的带宽进行通信。假设不同组的设备沿着同一个维度进行通信时具备同样的性能。device mesh可以由节点和GPU灵活组成，例如2个节点，每个节点8个GPU，通过inter-op pass可以将其组成2x8，1x16，4x4，8x2的device mesh，而不会局限于单个节点，这些不同的组合方式会导致不同的设备通信代价意味着同一个mesh中可能存在跨节点的设备，这就相比与同一个节点设备有更高的通信成本，后续也会通过inter-op pass 来优化物理设备和逻辑device mesh的映射。 4. Intra-op 并行在所给的device mesh上，最小化执行计算子图的算子内并行成本。根据工作负载的分配，不同的mesh可能会有不同数量的计算设备。采用SPMD风格的算子内并行，认为单个device mesh中所有的设备都有等效的计算能力，将所有计算平均分配到所有的设备上，执行相同的指令。 平均分配的方式减少了分布策略的搜索空间，更加方便的表示数据并行，算子并行等策略。这种方式与Tofu和FlexFlow(自动算子并行)不同。与FlexFlow采用的随机搜索和Tofu采用的假设线性不同的是，Alpa采用ILP来求解具有数万个运算符的模型。 算子内并行空间对于计算图上的一个算子，在mesh上可能有多种并行算法。 $$C_{ij} \\sum_kA_{ik}*B_{kj}$$ 对于矩阵乘来说，并行可以在ijk任意维度，或者[跨设备并行化ijk的组合?]，不同的方式就会有不同的计算和通信成本，不同的方式也都要求输入tensor有不同的layout，也导致输出tensor有不同的layout。如果输入的layout和并行方式需求的layout存在差异，那么久需要额外的转换，这也进一步增加通信开销。intra-op pass的目的就是给每个算子选择一个并行策略，来让整个图的执行时间最小。 Sharding Spec分片规范上面这个表格是一组Sharding Spec，用于描述一个tensor的不同layout，对于一个N-D的tensor来说，他的Sharding Spec会被定义为 $X_0X_1…X_{n-1}，X\\in {S,R}$ , S表示切分，R表示复制。Table 1.中最左侧就是可选择的layout，$S^0$ 表示在mesh的第一个维度进行切分，$S^{01}$表示在mesh的两个维度都进行切分，Spec中的两个字母表示tensor的两个维度的sharding策略。 Resharding当一个算子的输入tensor layout不满足当前为该算子选择的并行策略，且这个并行策略可能会有跨设备的通信，就需要加入 resharding这个layout转换。表格2. 就是几个resharding的例子。例如，a)要将一个全复制的tensor转换成其他的sharding spec时(case 1) ，可以在不通信的情况下局部切分tensor；b)交换切分轴(case 4)，需要使用all-to-all的通信源语了，详见Communication_primitives。 算子的并行算法结合以上内容，考虑在2D的mesh上将一个3D矩阵乘$C_{b,i,j}\\Sigma_kA_{b,i,k}B_{b,k,j}$ 并行化。表3. 列出了几种算子内并行策略最左侧一列表示将哪个维度进行并行，0和1 表示并行于mesh哪个维度。此外例如conv和reduction也可以通过分析其数学表达式来得到类似的所有可并行策略。Alpa中使用XLA的HLO作为基础算子，(为啥是XLA呢，因为算子少，工作量少 :-) ) ILP 公式一个计算图$G(V,E)$全部的执行开销包含：所有算子$v\\in V$的计算开销、通信开销以及所有路径$e\\in E$的resharding开销。这里将成本最小化表述为ILP，并通过一个现成的求解其进行最优求解。以下是论文，后续再看。 http://www.decom.ufop.br/haroldo/proglinear/files/cbcUserGuide.pdf 对于一个算子$v$, 可能存在的并行策略数量是$k_v$ ，有一个通信开销向量$c_v$ 长度为$k_v$ ，或者可以描述为 $c_{v}\\in \\Bbb{R}^{k_v}$ 长度为$k_v$的实数向量空间，$c_{vi}$表示第i个策略的通信开销。类似的，计算开销向量表示为$d_{v}\\in \\Bbb{R}^{k_v}$ 。同时有一个决策向量$s_{v\\in}{0,1}^{k_v}$ 来表示那个策略被使用， $s_{vi}1$ 表示我们给算子$v$选择了第i个并行策略。对于算子$v,u$之间的Resharding开销来说，定义了一个矩阵 $R_{vu} \\in \\Bbb{R}^{k_v×k_u}$ ，$R_{vuij}$表示算子$v$的第i个并行策略的输出到算子$u$的第$j$个并行策略的输入的Resharding开销目标函数如下: $$\\min_s \\sum_{v \\in V} s_v^\\top (c_v + d_v) + \\sum_{(v, u) \\in E} s_v^\\top R_{vu} s_u\\tag{1}$$ 前一部分是算子$v$的计算开销和通信开销，后面一部分是从算子$v$到算子$u$ 的resharding开销。在上面这个公式中$s$是变量，其余都是常量值。第二部分是二次方程式，所以无法使用ILP求解器。这里通过引入一个新的决策向量$e_{vu}{0,1}^{k_v·k_u}$将其线性化，这个向量表示算子$v,u$之间的resharding决策。新的公式如下： $$\\min_s \\sum_{v \\in V} s_v^\\top (c_v + d_v) + \\sum_{(v, u) \\in E} R_{vu}e_{vu}\\tag{1}$$ 虽然可以通过分析来获得上面三个常量的准确成本，但是为了简单起见，采用了以下方法来评估它们。 用通信字节数并除以mesh维度带宽来得到通信开销$d_{v}、R_{vu}$ 将所有的计算开销设置为0，动机与https://arxiv.org/abs/1807.08887 一致， 具体原因需要阅读该论文 之所以进行这样的简化是因为： 对于计算量很高的算子，例如matmul，不允许在重复计算。所有的并行策略总是会把所有的工作都分配到所有的设备上，所以一个算子所有的并行策略都会具有相同的算数复杂度。 对于计算量不高的算子，例如element-wise，允许复制操作，但是它们的计算开销可以忽略不计。为了简化计算图，将计算量极少的算子进行融合，比如element-wise、transpose、reduction等，将这些算子融合到计算图的其他算子中，并将目标算子的sharding spec进行传播，从而使这些计算量很少的算子尽可能的隐藏到其他算子庞大的计算开销、通信开销中。这种方式极大的减少了计算图中算子的数量，从而减少了ILP问题的大小。通过广度优先遍历的方式来计算每个算子的深度，然后将这些计算量少的算子尽可能的融合到最深的算子里。一旦并行策略被ILP决定，就会应用一系列post-ILP通信优化，就比如在合适的情况下将all-reduce操作用reduce-scatter和all-gather取代，因为后者减少了复制tensor的数量和相应的计算量，同时保持通信量不变，这也达到了与权重sharding或者ZrRO优化其一样的效果。 5. Inter-op 并行在最小化算子间并行的延迟问题上，主要考虑如何切分模型和device cluster到每个阶段和mesh，以及如何将他们映射成stage-mesh对。优化的目标是最小化整个计算图端到端pipeline的执行延迟，[17，33] 论文中已经考虑了一些简单的问题，例如假设每个阶段的设备是预先分配好的，并且所有阶段都是有固定的数据或者算子并行策略。Apla中通过联合考虑device mesh的分配和每个阶段存在不同的算子内并行策略舍弃了这些假设。 算子间并行策略的搜索空间假设计算图中包含了一个算子序列，并且具备图一样的拓扑顺序，标记为$o_1,o_2…o_k$ ，其中操作$o_k$的输入来自$o_1,…,o_{k-1}$。将其切分为S个阶段$s_1,s_2,…,s_S$ ，每个阶段都有一些算子组成$O_{l_i},…,O_{r_i}$ ,并且将每个阶段$S_i$分配到一个大小的$n_i×m_i$的子mesh上，子mesh是从包含deivce的计算机集群中切分出来的，这个计算机集群标记为带有形状为$N×M$的cluster mesh。让$t_{i} t_{intra}(s_i,Mesh(n_i,m_i))$ 作为执行$s_i$ 阶段在大小为$n_i*m_i$的子mesh上执行的延迟，这是通过ILP得出的最小延迟，由intra-op pass中返回给inter-op pass。 Curio：上图中abcd..表示一个输入的不同batch，stage表示子图和device mesh的组合，时间表示每个stage执行一个batch的时间开销。总体的延迟就是最耗时的子图的所有batch的计算时间加上一个batch的其他子图的执行时间(其他batch的其他子图执行的时间就被掩盖到最耗时的子图执行时间中，这部分的计算中主要是取决于子图-device mesh的划分，尽可能将数据依赖和子图的执行时间开销平衡) 如图所示，假设有B个不同的输入微batch，对于整个计算图的最小延迟可以表示为如下公式。 $$T^{*} \\min_{\\substack{s_1,…,s_S \\ (n_1,m_1),…,(n_S,m_S)}}\\left { \\sum^S_{i1} t_i + (B-1) \\cdot \\max_{1\\leq j\\leq S}{t_j} \\right } \\tag{2}$$ 总延迟包含两个部分：第一部分是所有阶段的总延迟，解释为第一个微batch通过整个流程的延迟；第二部分是剩余$B-1$个微batch的执行时间，该时间由最慢的阶段所限制，如上图中的第三阶段。目标是解决上述带有两个约束的公式：1）对于计算图前向传播的算子，我们想要将其与对应的反向算子放在同一个子mesh上。因为反向传播通常使用一系列与正向传播相似的tensor，这有效地减少从前向传播中获取所需的tensor到反向传播阶段的通信量。使用前向和后向的延迟之和作为$t_{intra}$ ，所以公式2反映了总体的延迟，包括了前向和后向。2）需要已经切分的子mesh能够完全地覆盖$N×M$的集群mesh——不浪费任何计算设备资源。接下来详细说明DP公式。 DP 公式为了确保所有的子mesh能够完整的覆盖$N×M$的mesh集群，将可用的子mesh的形状转化为两种类型：1）1D的子mesh，形状为$(1,1),(1,2),(1,4),…(1,2^m)$ ；2）2D的子mesh，形状为$(2,M),(3,M),…,(N,M)$，这种方式充分地利用了mesh集群的第二个维度（例如，在一个GPU集群中，这意味着每一个物理机器充分使用所有的计算设备）；具体的定理证明见附录A，表明了这些特定的子mesh能够始终完全覆盖mesh集群。为了将集群中的物理设备分配给DP算子找到的结果子mesh，分配设备时按照从大尺寸mesh到小尺寸的策略，即优先为较大的子mesh分配设备，然后再分配给小的子mesh。 这样的分配逻辑有助于充分利用高性能的通信链路，因为在大型子网格中，设备间的通信往往更为高效。这是因为大型子网格可能横跨多台物理机器，而这些机器之间的数据传输速度通常高于单台机器内部设备间的传输速度。？？？？源自LLM的解释 当有多个相同大小子mesh的pipeline阶段时，倾向于将相邻的pipeline阶段放的更近，从而减少通信延迟。 –剪枝的策略–对于一系列子mesh，$n1 and mM$ 的组合配剔除集合中，这些组合有着较差的表现，因为作为可选的子mesh$(n^{’},M)$具有更多可以用高带宽进行通信的设备，这种简化中只需要确保$\\sum_{i1}^{S} {n_i·m_iN·M}$ 为了求解公式2的$T^*$ ，这里开发了一个动态规划算法。首先枚举了第二项$t_{max}max_{1\\le j\\le S}t_j$ 并且对于每个不同的$t_{max}$最小化第一项$t_{total}(t_{max})\\sum_{1iS} {t_i}$ ，具体来说，当将算子$o_k$使用函数$F(s,k,d;t_{max})$ 来表示把从第$k$个算子到最后的算子序列切分为$s$个阶段后，分配到$d$个设备上的最小总延迟，任何一个阶段的延迟都不超过$t_{max}$ 。开始时设置$F(0,K+1,0;t_{max})0$ ，推导出F的最优子结构为： $$F(s,k,d; t_{\\max}) \\min_{\\substack{k \\leq i \\leq K \\ n_s \\cdot m_s \\leq d}}\\begin{cases}t_{\\text{intra}}((o_k, …, o_i), \\text{Mesh}(n_s, m_s), s) \\ F(s-1, i+1, d-n_s \\cdot m_s; t_{\\max}) \\| t_{\\text{intra}}((o_k, …, o_i), \\text{Mesh}(n_s, m_s), s) \\leq t_{\\max}\\end{cases} \\tag{3}$$ 并推导出最佳总延迟为: $$T^*(t_{max})\\min_{s}{F(s,0,N \\cdot M; t_{max})}+(B-1)\\cdot t_{max} \\tag{4}$$ 其中$t_{intra}((o_k,…,o_i),Mesh(n_s,m_s),s)$是由intra-op pass来决定的，这是$s$子序列阶段在$Mesh(n_s,m_s)$上执行子图$(o_k,…o_i)$的最低延迟。这个Mesh是一系列物理设备，因此枚举出所有逻辑mesh潜在的shape可能性，并通过intra-op pass查询子图、当前mesh和子图的输入的来自其他intra-op 的设置，并得到一个intra-op的计划。之后按照这个计划和其他的一些low-level编译器优化(fusion,mem)来编译这个子图，得到一个可执行文件来进行精确的分析。这个可执行程序是为了获取这个stage的延迟和每个设备上执行这个阶段所需要的mem，以及保存中间结果所需要的mem。之后根据所选择的pipeline执行计划来检查这个需要的mem是否与设备的mem合适，这里存在一个约束： $$mem_{stage}+s \\cdot mem_{act} \\leq mem_{device} \\tag{5}$$ 也就是说执行这个stage需要的内存加上保存中间结果的内存必须小于设备内存(显而易见的约束，不然多出来的放到设备外？增加通信代价，得不偿失)，选择一个满足最小延迟同时满足设备mem的逻辑mesh，如果都不满足，就把$t_{intra}$置0。这个DP算法是基于TeraPipe的，然而TeraPipe中假设所有pipeline stage都是相同的，并且目标是找到一个最优的方式将带Batch的输入token组织成大小不同的微batch。alpa的目标是将计算图的算子划分成不同的组，放到不同的pipeline stages，是在假设所有微batch都有相同的大小。此外alpa用DP算法在inter-op pass中优化每个stage pipeline 的mesh shape。 TeraPipe论文阅读 复杂度在$O(K^3NM(X+log(M)))$ 的时间内计算固定$t_{max}$的切分。$t_{max}$最多有$O(K^2(N+log(M)))$ 种选择:$t_{intra}((o_i,…,o_j),Mesh(n_s,m_s))for\\ i,j1,…,k$ 和所有子mesh选择。因此总的复杂度是$O(K^5NM(N+log(M))^2)$ 。这个时间复杂度对于大的计算图(超过1W个算子)来说并不可行。为了加速DP，做以下实用的优化 early pruning提前剪枝，1）递增枚举：方法类似于TeraPipe，将$t_{max}$从小到大列举，2）当$B·t_{max}$大于当前最优的$T^$，立刻停止枚举。这是因为较大的$t_{max}$无法提供一个更好的解决方案。3）同样，在枚举$t_{max}$的时候，每次只评估比上一个$t_{max}$大至少ε的. 虽然找到的解决方案不一定是最优的，但是全局最优条件的差距最大为$Bε$ 。 按照经验设定$ε10^{-6}s$ ，实验中该算法找到的方案基本上和最优解一致。 运算符聚类计算图上的很多算子并不是计算密集的，这些算子的确切位置对总体执行的时间影响很小。这里开发了另一个DP算法来聚类相邻的算子，从而减少计算图的大小。将K个算子聚类成L层，L是远小于K的。 这个算法的目的是将两种类型的算子合并：1）不会调用大量的计算，但是会增加计算图的长度；2）相邻的算子如果放在不同的设备上可能会产生实质性的通信。定义函数$G(k,r)$ 作为聚类算子$(o_1,…o_k)$时划分到聚类层r时接收的最大数据量的最小值。最优子结构如下 $$G(k,r) \\min_{1ik}\\begin{cases}max{G(i-1,r-1),C(i,k)}\\atop|FLOP(o_i,…,o_k)\\frac{(1+\\delta)FLOP_{total}}{L}\\end{cases}$$ 其中$C(i，k)$ 表示从$(o_1,…o_{i-1})$到$(o_i,…o_k)$的输入数据总量，$FLOP_{total}$表示整个计算图的FLOP。确保每个聚类层的FLOP都是在 1 + δ乘以每层平均FLOP之内的同时，也最小化了个层间FLOP的方差，从而达到更加均匀的结构分布。对于通信成本相同的方案，我们选择结构最均匀的一个，同事最小化每层FLOP的方差。通过这个DP算法，我们可以在$O(K^2L)$的时间内算出最优的层聚类。 $L$是算法的超参数，根据经验我们基于设备数量和计算图中计算量大的算子数量选择一个小的$L$。其实这个选择对于最终性能并不会有跟明显的影响。 算法描述Inter-op pass总结输入：计算图G , 集群C {with shape (N,M)}输出：最小的执行延迟 $T^*$ # 计算图前处理1. 序列化模型 (o_1,...,o_k) = Flatten(G)2. 算子层聚类 (l_1,...,l_L) = OperatorClustering(o_1,...,o_K)# 执行intra-op pass，获取不同stage-mesh组合submesh_shapes = (1,1),(1,2),(1,4),...,(1,M)∪(2,M),(3,M),...,(N,M)for 1=i=j=L :\tstage = (l_i,...l_L)\tfor (n,m) in submesh_shapes: # 初始化所有intra-op阶段的 for s in range(1,L): t_intra(stage, Mesh(n, m), s) = INF for (n_l,m_l),opt in LogicalMeshShapeAndIntraOpOptions(n,m): plan = IntraOpPass(stage, Mesh(n_l,m_l),opt) t_l,mem_stage,mem_act = Profile(plan) for s 满足公式5 # mem_stage + s · mem_act ≤ mem_device . if t_l = t_intra(stage, Mesh(n, m), s): t_intra(stage, Mesh(n, m), s) = t_lreturn t_l# inter-op pass （DP）T* = INFfor t_max in SortedAndFilter(t_intra, ε):\tif B·t_max = T*: break\tF(0,L+1,0; t_max) = 0\tfor s in range(1,L): for l in range(L,1): for d in range (1,N*M): Compute F(s,l,d; t_max) 按照公式3\tT*(t_max) = min_sF(s,0,N·M;t_max)+(B−1)·t_max\tif T*(t_max) T*: T* = T*(t_max)return T* 6. Parallelism Orchestration并行编排当Stages，device mesh的分配决定以后，在intra-op pass阶段，Alpa遵循ILP求解器得出的intra-op并行计划，根据分配给它的device mesh编译每个阶段。编译依赖于XLA和GSPMD，并生成每个stage-mesh的并行可执行程序。当需要时，编译会自动插入第四节中提到的通信源语来处理算子内并行所需要的resharding。在inter-op pass阶段，Alpa实现了一个额外的并行编排pass来处理跨mesh的stage间通信，并为算子间并行执行生成静态指令。 跨mesh的Resharding现存的手动系统，就像Megatron-LM，约束所有的pipeline stage都有相同的数据和tensor模型并行度，因此pipeline stage之间的通信通常通过两个等效设备网格的对应设备之间的P2P发送接收来实现(图6a)。在Alpa中包含两个相邻stage的device mesh可能有不同的mesh shape，并且tensor在两个stage之间的通信可能有着不同的sharding spec(图6bc)，这里称这种通信模式为cross-mesh resharding，这是一个多对多的多点传送问题。给定tensor在发送和接收方的mesh上的sharding spec， Alpa在两次迭代中生成一个用于解决 corss-mesh sharding 的通信方案。第一次迭代中，计算在源mesh和目标mesh之间tensor tile的对应关系。基于此，生成源设备和目标设备之间的P2P发送接收源语来实现通信。然后，第二次迭代中来识别目标tensor在其sharding spec中存在复制的机会。在这种情况下tensor只需要在mesh之间传输一次，之后利用目标mesh上利用更高的带宽通过 all-gather来完成交换–这会重写第一次迭代中生成的发送接收，避免重复通信。称这种方法为 local all-gather跨设备resharding。因为在Alpa的设计中stage之间的通信通常都很小，实验符合设计的期望，后续还会开发更优的跨设备resharding策略。 生成可执行指令最后一步，Alpa生成静态执行指令在集群上启动训练，因为每个stage都有不同的算子集合，并且在放在有不同shape 的mesh上 （与很多APMD 并行训练系统不同），Alpa采用MPMD风格的runtime来安排算子间并行执行–Alpa生成确定的静态执行指令给每一个device mesh。Alpa给算子间并行执行开发了一系列指令，包括stage内tensor的内存分配和释放指令，按照cross-mesh resharding方案在stage间的tensor通信指令，同步指令和计算指令等。。。根据用户选择的pipeline schedule，Alpa使用一个驱动进程提前生成指令，并在执行之前整个指令列表分发给每个worker，避免了运行时的驱动和worker之间协调的开销。 7. 限制和讨论相比于现有的一些手动组合数据、算子、和pipeline并行(比如3D-parallelism [45]和PTD-P[40])，Alpa对inter-op、intra-op并行性的分层视图显著的提高了他们的三个主要灵活性：1）stage可以包含不均匀数量的算子和层；2）stages可能被映射到不同shape的device mesh上；3）在每个stage中，数据和算子并行结构配置对于每个算子来说都是单独定制的。综上，是的Alpa能够整合所有现存的模型并行方法并推广到具有更多异构性的模型结构和集群设置。抛开这些改进，Alpa的优化算法目前还是存在一些限制： 并没有对不同stage的通信成本进行建模，因为cross-stage的通信成本本来就很小。但是实际上在DP或者ILP中对cost进行建模都是可行的，但是会需要枚举更多矢量的intra-op pass和DP状态。 inter-op pass目前有一个超参数：微batch $B$，目前的公式中并没有被优化，但是可以通过枚举的方式进行搜索。 inter-op pass用静态线性schedule建模pipeline的并行结构，而没有考虑更多动态schedule，例如在不同的deivce上并行计算图中的不同分支 对于overlap计算和通信并没有优化一个最好的方案，Alpa值能处理静态计算图，所有tensor的shape编译时已知。尽管如此，Alpa都是可以将现在一些常见的模型生成出接近最优的执行方案。 附录关于算子聚类的疑惑解释在Alpa论文中的动态规划（DP）公式里，递推关系中取两部分的最大值的原因是为了确保负载均衡和计算效率。这种方法有助于找到一个平衡点，使得每个阶段（或者子任务）的最大开销最小化，从而实现整体优化。 公式回顾公式如下： $$G(k,r) \\min_{1 \\leq i \\leq k}\\begin{cases}\\max{G(i-1,r-1), C(i,k)} \\text{| } FLOP(o_i, \\ldots, o_k) \\leq \\frac{(1+\\delta)FLOP_{total}}{L}\\end{cases}$$ 解释 ( G(k, r) )：表示将前 (k) 个算子分成 (r) 个阶段时的最小成本。 递推关系：$\\min_{1 \\leq i \\leq k} { \\max{G(i-1, r-1), C(i, k)} }$ 为什么要取最大值 最大值的含义：在每个划分点 (i)，我们计算两部分的最大值： G(i-1, r-1) ：前 (i-1) 个算子分成 (r-1) 个阶段的最小成本。 ( C(i, k) )：从第 (i) 个算子到第 (k) 个算子的聚类成本。 最大值的目的是为了确保最坏情况下的开销最小化： 在分布式计算中，如果某一个阶段的计算开销特别大，它将成为整个系统的瓶颈，导致整体性能下降。 通过取两部分中的最大值，公式确保即使在最坏情况下（即某一阶段的开销最大），整体方案依然是最优的。这种策略有效地平衡了各个阶段的负载，避免单点过载。 为什么要取最小值 最小值的含义：在所有可能的划分点 (i) 中，选择使得最大值最小的那个划分点。 最小值的目的是找到全局最优解： 动态规划通过递推关系逐步寻找全局最优解。在每一步中，选择最优的划分点，以确保整体开销最小。 通过在所有可能的 (i) 中取最小值，公式确保选择的划分点能够使整体计算开销最小化。 负载均衡与效率优化 负载均衡： 分布式系统中，负载均衡至关重要。均衡的负载分布可以避免某些设备过载，从而提高整体系统的效率。 通过限制每个阶段的FLOP数，并取最大值来衡量最坏情况下的开销，公式确保每个设备的计算任务不会超过其承受能力。 效率优化： 通过动态规划逐步优化，从而找到整体计算开销最小的方案。 在每一步中选择使得当前阶段最优的划分点，确保最终得到的划分方案是全局最优的。 总结在Alpa论文中的DP公式里，取两部分的最大值是为了确保分布式计算的负载均衡和效率优化。这种方法通过考虑最坏情况下的开销，避免了单点过载，从而提升了整体系统的性能和稳定性。最终通过在所有可能的划分点中取最小值，公式实现了全局最优的算子聚类和任务分配方案。","tags":["Distribution"],"categories":["Paper"]},{"title":"gperf 使用","path":"/tech/perftools/","content":"安装# 克隆代码git clone https://github.com/gperftools/gperftools.git# 安装依赖库sudo apt-get install libunwind8-dev# 程序编译cd gperftools./autogen.sh./configuremake -j 8# 安装到系统文件夹sudo make install# 刷新动态库文件ldconfig 使用非侵入式顾名思义，不需要在程序中加入任何代码，直接运行即可。需要在CmakeList中加入相关的编译选项 -Wl,--no-as-needed,-lprofiler,--as-needed。 target_link_libraries(ncnn_test ncnn -Wl,--no-as-needed,-lprofiler,--as-needed) 然后在命令行中使用CPUPROFILE进行执行前的配置，同时执行需要分析的程序。 CPUPROFILE=./ncnn_test.prof ./ncnn_test 之后，即可生成当前程序的性能分析文件。 侵入式需要在程序中加入相关的代码，然后运行程序。这种方式需要在每个需要分析的函数中加入相关的代码。但是CmakeList中仅需要加入-lprofiler即可。 target_link_libraries(ncnn_test ncnn -lprofiler) 代码中的相关代码如下： #include gperftools/profiler.hint main() ProfilerStart(./ncnn_test.prof); // ... ProfilerStop(); return 0; 之后，执行程序即可得到性能分析文件。 性能分析需要使用pprof进行文件信息解析 pprof --text ./ncnn_test ncnn_test.profUsing local file ./ncnn_test.Using local file test.prof.addr2line: DWARF error: section .debug_info is larger than its filesize! (0x93f189 vs 0x530f70)Total: 8 samples 8 100.0% 100.0% 8 100.0% omp_get_num_procs 0 0.0% 100.0% 8 100.0% clone 0 0.0% 100.0% 8 100.0% omp_in_final 0 0.0% 100.0% 8 100.0% start_thread 其输出形式有--text控制，还可以使用其他形式进行展示，如--pdf生成pdf文件，--web生成网页。 注意事项当你的程序很短，或者运行速度很快时，使用--text形式进行性能分析时，可能无法得到有效的性能分析信息。只会得到如下信息 Using local file ./ncnn_test.Using local file test.prof. 此时，可以通过设置CPUPROFILE_FREQUENCY为更大的数值，来增加性能分析的频率。从而避免出现上述情况。采样频率越高，性能分析的准确度越高。 export CPUPROFILE_FREQUENCY=9999999 或者 # RECOMMENDCPUPROFILE_FREQUENCY=9999999 ./ncnn_test","tags":["性能优化","Utils"],"categories":["性能优化"]},{"title":"shell脚本总结","path":"/tech/shell/","content":"1. 获取特定的目录提取特定目录现有一些文件路径，例如： “homeuserprojectsrcmainjavacomexampledemoDemoApplication.java” “homeuserprojectsrcmaincppcomexampledemoDemoApplicationTests.cpp” “homeuserprojectsrcmainccomexampledemocontrollerDemoController.c” 需要提取出java,cpp,c这个子目录。 #!/bin/bashpath=/home/user/project/src/main/java/com/example/demo/DemoApplication.javatarget_dir=$path#*/main/ # 移除“main/”及之前的部分desired_dir=$target_dir%%/* # 取得剩余字符串中的第一个目录名echo $desired_dir java 可以用于处理测试报告，提取出java,cpp,c这三个子目录。 2. 定制log颜色编译脚本执行完毕以后，输出的log信息默认是和终端颜色一致的，无法快速帮助我们获取特定信息，那么就可以通过颜色来区分不同类型的log信息。 #!/bin/bash# ANSI转义码颜色定义RED=\\033[0;31mGREEN=\\033[0;32mNC=\\033[0m # No Colorecho -e $RED not convert success !$NC","tags":["Utils"],"categories":["Utils"]},{"title":"一些好用的工具","path":"/tech/utils/","content":"命令行 hyperfine 命令行基准测试工具 VSCode 插件 crayons 高亮选中word Markdown借助html标签来分栏 1. 对计算图进行拓扑排序 2. 找出最小的volum 1. asd 2. 啊试点单位 3. aa 梯子https://www.racknerd.com。每年只要10美元，3T的流量，地点选在洛杉矶，200ms时延，性价比超高.","tags":["Utils"]},{"title":"loop optimize","path":"/tech/loop_opt/","content":"背景问题如果要进行1-10000的累乘，该如何实现？最常见的做法就是使用for循环，那么for循环是否能进行优化呢，提升到极致的性能会是什么样子呢。 实现为了避免编译器过度，我们使用Debug模式进行性能分析。 方法一 int sum = 1;for (int i = 1; i = 10000; i++)\tsum *= i; times 0 1 2 3 4 5 6 7 8 9 duration (ms) 17.929 17.434 17.804 17.816 16.706 17.201 18.129 17.049 17.368 17.823 这是最基础的for循环执行10次的性能。接下来我们开始对其进行优化。 方法二 方法一中每次计算需要进行一次循环判断，总的指令数量为10000次循环控制指令+10000次计算指令。通常对for循环进行性能优化的办法是循环展开，即在一次循环中进行多次计算，减少循环控制相关指令的数量，使有效的计算指令的占比提升。因此在 方法二中我们减少循环指令的占比，在一次循环中进行4次计算。 int sum = 1;for(int i = 1; i = 10000; i+=4)\tsum *= i;\tsum *= i+1;\tsum *= i+2;\tsum *= i+3; times 0 1 2 3 4 5 6 7 8 9 duration (ms) 16.979 17.056 16.99 17.047 17.025 17.04 16.979 17.032 17.017 17.013 这次的循环指令数量为2500次，10000次计算。可以看到，这次的for循环性能相比于 方法一有了一定的提升，但是总体来说提升并不大，我们先不进行过度深入的理解，就当作循环指令的开销比较小，所以单纯减少循环指令对性能的提升并不明显。 这里首先理解一下循环展开是为了什么，单纯的减少本身就开销比较小的控制指令，以此来优化性能？也许并不是！ 这里就需要了解下现在处理器的一个特性，就是能够在一个始终周期内执行多条指令，本文中不做详细展开，可以先通过这篇文章了解一下这一特性 指令流水线. 方法三 现在我们来实现一个可以利用处理器指令流水线特性的for循环代码。 int sum0=1, sum1=1, sum2=1, sum3=1;for (int i = 1; i = 10000; i+=4)\tsum0 *= i;\tsum1 *= i+1;\tsum2 *= i+2;\tsum3 *= i+3;return sum0*sum1*sum2*sum3; times 0 1 2 3 4 5 6 7 8 9 duration (ms) 5.038 5.073 5.755 4.924 5.231 5.56 5.59 4.717 5.782 5.017 可以看到，利用了指令流水线后，程序的性能是大幅度提升的，并且这是没有进行编译器优化的代码。 总结所有软件都是在硬件的基础上运行的，所以，如果想要写出非常高效的代码，那么就必须得熟悉或者说对计算机体系结构有所了解，否则就没法使代码达到极致的性能。 同样利用硬件特性的程序，在多线程程序中也有体现，可以看看下一篇 多线程程序性能优化.","tags":["loop"],"categories":["optimize"]},{"title":"生活点滴","path":"/life/index.html","content":"叽叽喳喳、闲言碎语 .timeline{position:relative;margin:1rem 0;padding-left:1.25rem} .timeline::before{content:\"\";position:absolute;left:.45rem;top:.25rem;bottom:.25rem;width:2px;background:rgba(127,127,127,.25)} .timeline-item{position:relative;margin:.85rem 0;padding-left:.75rem} .timeline-item::before{content:\"\";position:absolute;left:-.05rem;top:.35rem;width:.55rem;height:.55rem;border-radius:999px;background:var(--theme-color,#3b82f6);box-shadow:0 0 0 3px rgba(59,130,246,.18)} .timeline-time{font-size:.9rem;opacity:.85;font-variant-numeric:tabular-nums} .timeline-card{margin-top:.25rem;padding:.6rem .75rem;border:1px solid rgba(127,127,127,.18);border-radius:.6rem;background:rgba(127,127,127,.06)} .timeline-card p{margin:.25rem 0} .timeline-more{text-align:right;margin:1rem 0} .timeline-more-btn{display:inline-flex;align-items:center;gap:.5rem;padding:.45rem 1rem;background:linear-gradient(135deg,#818cf8,#a78bfa);color:white;border-radius:.5rem;cursor:pointer;text-decoration:none;font-weight:500;font-size:.85rem;transition:all 0.3s ease;box-shadow:0 2px 6px rgba(129,140,248,.25);border:2px solid transparent} .timeline-more-btn:hover{opacity:1;transform:translateY(-1px);box-shadow:0 3px 8px rgba(129,140,248,.35)} .timeline-more-btn:active{transform:translateY(0);box-shadow:0 2px 4px rgba(129,140,248,.25)} .timeline-collapse{display:none} 2026-01-05 00:38 完成了山东威海的旅游 2025-12-24 17:14 终于忙完了搬家的一系列事情，可以开始考虑找工作的事情了。 2025-11-22 09:22 今天开始健身，记录一些关于健身的想法。 2025-11-21 17:00 嘉楠LastDay咯，这五年多收获很多，认识很多很好的朋友，但是也该离开了。 2025-11-15 12:45 +2领导居然让我留下，算咯算咯，好不容易下定决心呢，还是走吧，虽然不舍的紧。 function toggleTimeline(btn) { const collapseDiv = btn.closest('.timeline').querySelector('.timeline-collapse'); const isHidden = collapseDiv.style.display === 'none' || collapseDiv.style.display === ''; const itemCount = collapseDiv.querySelectorAll('.timeline-item').length; if (isHidden) { collapseDiv.style.display = 'block'; btn.innerHTML = ' 收起'; } else { collapseDiv.style.display = 'none'; btn.innerHTML = ' 显示更多 (' + itemCount + ')'; } return false; }"},{"title":"About Me","path":"/about/index.html","content":"我年近三十岁，是一个普通的程序员，有着普通的生活，有着普通的梦想。"},{"title":"Wiki使用指南","path":"/wiki/index.html","content":"Wiki界面说明首先，这里会作为个人知识的记录，内容不一定准确，阅读需要有一定辨识度。"},{"title":"技术问题","path":"/tech/index.html","content":"近期目标最近待在家里好好研究MLIR。 .timeline{position:relative;margin:1rem 0;padding-left:1.25rem} .timeline::before{content:\"\";position:absolute;left:.45rem;top:.25rem;bottom:.25rem;width:2px;background:rgba(127,127,127,.25)} .timeline-item{position:relative;margin:.85rem 0;padding-left:.75rem} .timeline-item::before{content:\"\";position:absolute;left:-.05rem;top:.35rem;width:.55rem;height:.55rem;border-radius:999px;background:var(--theme-color,#3b82f6);box-shadow:0 0 0 3px rgba(59,130,246,.18)} .timeline-time{font-size:.9rem;opacity:.85;font-variant-numeric:tabular-nums} .timeline-card{margin-top:.25rem;padding:.6rem .75rem;border:1px solid rgba(127,127,127,.18);border-radius:.6rem;background:rgba(127,127,127,.06)} .timeline-card p{margin:.25rem 0} .timeline-more{text-align:right;margin:1rem 0} .timeline-more-btn{display:inline-flex;align-items:center;gap:.5rem;padding:.45rem 1rem;background:linear-gradient(135deg,#818cf8,#a78bfa);color:white;border-radius:.5rem;cursor:pointer;text-decoration:none;font-weight:500;font-size:.85rem;transition:all 0.3s ease;box-shadow:0 2px 6px rgba(129,140,248,.25);border:2px solid transparent} .timeline-more-btn:hover{opacity:1;transform:translateY(-1px);box-shadow:0 3px 8px rgba(129,140,248,.35)} .timeline-more-btn:active{transform:translateY(0);box-shadow:0 2px 4px rgba(129,140,248,.25)} .timeline-collapse{display:none} 2026-01-09 13:25 TritonNext 不如说是智源的宣传会了，FlagOS (Open Software)，本质上是一个不错的想法，构建一个通用的AI软件栈，本身也是基于llvm和Triton的，但是生态构建不会太容易了。 2025-12-24 22:14 今天正式完善博客的各个功能，目前还遗留一个项目模块，用于后续添加个人项目。 function toggleTimeline(btn) { const collapseDiv = btn.closest('.timeline').querySelector('.timeline-collapse'); const isHidden = collapseDiv.style.display === 'none' || collapseDiv.style.display === ''; const itemCount = collapseDiv.querySelectorAll('.timeline-item').length; if (isHidden) { collapseDiv.style.display = 'block'; btn.innerHTML = ' 收起'; } else { collapseDiv.style.display = 'none'; btn.innerHTML = ' 显示更多 (' + itemCount + ')'; } return false; }"}]